{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try with given scripts (BAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = np.load('embeddings.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.pkl\", \"rb\") as f:\n",
    "        vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines_pos = sum(1 for line in open('Datasets/twitter-datasets/train_pos_full_cleaned.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = np.zeros((num_lines_pos,emb.shape[1]))\n",
    "with open('Datasets/twitter-datasets/train_pos_full_cleaned.txt') as f:\n",
    "    for line_index, line in enumerate(f):\n",
    "        words = line.split()\n",
    "        index = [vocab[word] for word in words if word in vocab.keys()]\n",
    "        line_fet = np.mean(np.array([emb[i] for i in index]),axis = 0)\n",
    "        train_pos[line_index] = line_fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove_pos = np.unique([x for x,y in np.argwhere(np.isnan(train_pos))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_pos_2 = np.delete(train_pos,index_to_remove_pos,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines_neg = sum(1 for line in open('Datasets/twitter-datasets/train_neg_full_cleaned.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = np.zeros((num_lines_neg,emb.shape[1]))\n",
    "with open('Datasets/twitter-datasets/train_neg_full_cleaned.txt') as f:\n",
    "    for line_index, line in enumerate(f):\n",
    "        words = line.split()\n",
    "        index = [vocab[word] for word in words if word in vocab.keys()]\n",
    "        line_fet = np.mean(np.array([emb[i] for i in index]),axis = 0)\n",
    "        train_neg[line_index] = line_fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove_neg = np.unique([x for x,y in np.argwhere(np.isnan(train_neg))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_neg_2 = np.delete(train_neg,index_to_remove_neg,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get total training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((train_pos_2,train_neg_2))\n",
    "y_pos = np.ones(train_pos_2.shape[0])\n",
    "y_neg = np.repeat(-1,train_neg_2.shape[0])\n",
    "Y = np.hstack((y_pos,y_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X',X)\n",
    "np.save('Y',Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X.npy')\n",
    "Y = np.load('Y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = build_poly(X,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=True, order='C')\n",
    "X = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = std.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train',X_train)\n",
    "np.save('X_test',X_test)\n",
    "np.save('Y_train',Y_train)\n",
    "np.save('Y_test',Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "Y_train = np.load('Y_train.npy')\n",
    "Y_test = np.load('Y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi = LogisticRegression(penalty='l2', dual=False, tol=10e-10, C=0.5, fit_intercept=True, intercept_scaling=1, \n",
    "                          class_weight=None, random_state=None, solver='warn', max_iter=100, multi_class='warn', \n",
    "                          verbose=1, warm_start=False, n_jobs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=1e-09, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5966421276625049"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62238"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#old\n",
    "logi.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, tol=10e-10, C=0.5, multi_class='ovr', \n",
    "                fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=1, random_state=None, \n",
    "                max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=1e-09,\n",
       "     verbose=1)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.595322209290848"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5041910083820168"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5041910083820168"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5041910083820168"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize test tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines_test = sum(1 for line in open('Datasets/twitter-datasets/test_data.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.zeros((num_lines_test,emb.shape[1]))\n",
    "with open('Datasets/twitter-datasets/test_data.txt') as f:\n",
    "    for line_index, line in enumerate(f):\n",
    "        line = line.split(',',1)[1]\n",
    "        words = line.split()\n",
    "        index = [vocab[word] for word in words if word in vocab.keys()]\n",
    "        line_fet = np.mean(np.array([emb[i] for i in index]),axis = 0)\n",
    "        test[line_index] = line_fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove_test = np.unique([x for x,y in np.argwhere(np.isnan(test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = np.delete(test,index_to_remove_test,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = std.fit_transform(test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = build_poly(test_2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(test_2)\n",
    "prediction_2 = np.insert(prediction, index_to_remove_test -1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "## Vocabulary vectorizing\n",
    "Read words in positive and neg tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "import logging\n",
    "import tempfile\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Datasets/twitter-datasets/train_pos_full_cleaned.txt\")\n",
    "tweets_pos = [line.split() for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Datasets/twitter-datasets/train_neg_full_cleaned.txt\")\n",
    "tweets_neg = [line.split() for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Word2vec\n",
    "size = 300\n",
    "min_count = 5\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:10:31,545 : INFO : collecting all words and their counts\n",
      "2019-12-11 15:10:31,548 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-12-11 15:10:31,594 : INFO : PROGRESS: at sentence #10000, processed 127472 words, keeping 12836 word types\n",
      "2019-12-11 15:10:31,666 : INFO : PROGRESS: at sentence #20000, processed 256855 words, keeping 20142 word types\n",
      "2019-12-11 15:10:31,711 : INFO : PROGRESS: at sentence #30000, processed 385671 words, keeping 26057 word types\n",
      "2019-12-11 15:10:31,765 : INFO : PROGRESS: at sentence #40000, processed 514151 words, keeping 31480 word types\n",
      "2019-12-11 15:10:31,831 : INFO : PROGRESS: at sentence #50000, processed 643058 words, keeping 36364 word types\n",
      "2019-12-11 15:10:31,892 : INFO : PROGRESS: at sentence #60000, processed 772564 words, keeping 40846 word types\n",
      "2019-12-11 15:10:31,943 : INFO : PROGRESS: at sentence #70000, processed 901334 words, keeping 45115 word types\n",
      "2019-12-11 15:10:31,989 : INFO : PROGRESS: at sentence #80000, processed 1030026 words, keeping 49114 word types\n",
      "2019-12-11 15:10:32,054 : INFO : PROGRESS: at sentence #90000, processed 1158523 words, keeping 53037 word types\n",
      "2019-12-11 15:10:32,100 : INFO : PROGRESS: at sentence #100000, processed 1287613 words, keeping 56800 word types\n",
      "2019-12-11 15:10:32,147 : INFO : PROGRESS: at sentence #110000, processed 1415424 words, keeping 60309 word types\n",
      "2019-12-11 15:10:32,235 : INFO : PROGRESS: at sentence #120000, processed 1543958 words, keeping 63695 word types\n",
      "2019-12-11 15:10:32,297 : INFO : PROGRESS: at sentence #130000, processed 1672993 words, keeping 67042 word types\n",
      "2019-12-11 15:10:32,374 : INFO : PROGRESS: at sentence #140000, processed 1802888 words, keeping 70285 word types\n",
      "2019-12-11 15:10:32,417 : INFO : PROGRESS: at sentence #150000, processed 1933257 words, keeping 73411 word types\n",
      "2019-12-11 15:10:32,501 : INFO : PROGRESS: at sentence #160000, processed 2061798 words, keeping 76351 word types\n",
      "2019-12-11 15:10:32,628 : INFO : PROGRESS: at sentence #170000, processed 2190742 words, keeping 79443 word types\n",
      "2019-12-11 15:10:32,707 : INFO : PROGRESS: at sentence #180000, processed 2320310 words, keeping 82348 word types\n",
      "2019-12-11 15:10:32,789 : INFO : PROGRESS: at sentence #190000, processed 2448875 words, keeping 85321 word types\n",
      "2019-12-11 15:10:32,870 : INFO : PROGRESS: at sentence #200000, processed 2577583 words, keeping 88257 word types\n",
      "2019-12-11 15:10:32,904 : INFO : PROGRESS: at sentence #210000, processed 2706577 words, keeping 91319 word types\n",
      "2019-12-11 15:10:32,951 : INFO : PROGRESS: at sentence #220000, processed 2836716 words, keeping 94122 word types\n",
      "2019-12-11 15:10:33,012 : INFO : PROGRESS: at sentence #230000, processed 2965344 words, keeping 96902 word types\n",
      "2019-12-11 15:10:33,058 : INFO : PROGRESS: at sentence #240000, processed 3093702 words, keeping 99602 word types\n",
      "2019-12-11 15:10:33,142 : INFO : PROGRESS: at sentence #250000, processed 3221116 words, keeping 102114 word types\n",
      "2019-12-11 15:10:33,234 : INFO : PROGRESS: at sentence #260000, processed 3350133 words, keeping 104728 word types\n",
      "2019-12-11 15:10:33,285 : INFO : PROGRESS: at sentence #270000, processed 3479354 words, keeping 107356 word types\n",
      "2019-12-11 15:10:33,329 : INFO : PROGRESS: at sentence #280000, processed 3607997 words, keeping 109888 word types\n",
      "2019-12-11 15:10:33,373 : INFO : PROGRESS: at sentence #290000, processed 3736297 words, keeping 112407 word types\n",
      "2019-12-11 15:10:33,431 : INFO : PROGRESS: at sentence #300000, processed 3865243 words, keeping 115111 word types\n",
      "2019-12-11 15:10:33,502 : INFO : PROGRESS: at sentence #310000, processed 3994171 words, keeping 117611 word types\n",
      "2019-12-11 15:10:33,540 : INFO : PROGRESS: at sentence #320000, processed 4123782 words, keeping 120088 word types\n",
      "2019-12-11 15:10:33,620 : INFO : PROGRESS: at sentence #330000, processed 4252329 words, keeping 122595 word types\n",
      "2019-12-11 15:10:33,663 : INFO : PROGRESS: at sentence #340000, processed 4382120 words, keeping 125046 word types\n",
      "2019-12-11 15:10:33,696 : INFO : PROGRESS: at sentence #350000, processed 4511595 words, keeping 127314 word types\n",
      "2019-12-11 15:10:33,750 : INFO : PROGRESS: at sentence #360000, processed 4640072 words, keeping 129718 word types\n",
      "2019-12-11 15:10:33,822 : INFO : PROGRESS: at sentence #370000, processed 4769703 words, keeping 131999 word types\n",
      "2019-12-11 15:10:33,880 : INFO : PROGRESS: at sentence #380000, processed 4898686 words, keeping 134386 word types\n",
      "2019-12-11 15:10:33,915 : INFO : PROGRESS: at sentence #390000, processed 5027170 words, keeping 136713 word types\n",
      "2019-12-11 15:10:33,980 : INFO : PROGRESS: at sentence #400000, processed 5157086 words, keeping 139066 word types\n",
      "2019-12-11 15:10:34,021 : INFO : PROGRESS: at sentence #410000, processed 5286271 words, keeping 141265 word types\n",
      "2019-12-11 15:10:34,051 : INFO : PROGRESS: at sentence #420000, processed 5414952 words, keeping 143497 word types\n",
      "2019-12-11 15:10:34,077 : INFO : PROGRESS: at sentence #430000, processed 5544491 words, keeping 145685 word types\n",
      "2019-12-11 15:10:34,108 : INFO : PROGRESS: at sentence #440000, processed 5673426 words, keeping 147901 word types\n",
      "2019-12-11 15:10:34,141 : INFO : PROGRESS: at sentence #450000, processed 5802636 words, keeping 150133 word types\n",
      "2019-12-11 15:10:34,170 : INFO : PROGRESS: at sentence #460000, processed 5932419 words, keeping 152269 word types\n",
      "2019-12-11 15:10:34,200 : INFO : PROGRESS: at sentence #470000, processed 6060877 words, keeping 154513 word types\n",
      "2019-12-11 15:10:34,300 : INFO : PROGRESS: at sentence #480000, processed 6190093 words, keeping 156678 word types\n",
      "2019-12-11 15:10:34,370 : INFO : PROGRESS: at sentence #490000, processed 6319623 words, keeping 158820 word types\n",
      "2019-12-11 15:10:34,417 : INFO : PROGRESS: at sentence #500000, processed 6448734 words, keeping 160935 word types\n",
      "2019-12-11 15:10:34,463 : INFO : PROGRESS: at sentence #510000, processed 6579524 words, keeping 163076 word types\n",
      "2019-12-11 15:10:34,551 : INFO : PROGRESS: at sentence #520000, processed 6708548 words, keeping 165172 word types\n",
      "2019-12-11 15:10:34,582 : INFO : PROGRESS: at sentence #530000, processed 6837068 words, keeping 167234 word types\n",
      "2019-12-11 15:10:34,629 : INFO : PROGRESS: at sentence #540000, processed 6966529 words, keeping 169296 word types\n",
      "2019-12-11 15:10:34,745 : INFO : PROGRESS: at sentence #550000, processed 7095790 words, keeping 171324 word types\n",
      "2019-12-11 15:10:34,884 : INFO : PROGRESS: at sentence #560000, processed 7226696 words, keeping 173403 word types\n",
      "2019-12-11 15:10:34,960 : INFO : PROGRESS: at sentence #570000, processed 7355922 words, keeping 175492 word types\n",
      "2019-12-11 15:10:35,043 : INFO : PROGRESS: at sentence #580000, processed 7485418 words, keeping 177489 word types\n",
      "2019-12-11 15:10:35,136 : INFO : PROGRESS: at sentence #590000, processed 7615281 words, keeping 179528 word types\n",
      "2019-12-11 15:10:35,194 : INFO : PROGRESS: at sentence #600000, processed 7745069 words, keeping 181444 word types\n",
      "2019-12-11 15:10:35,279 : INFO : PROGRESS: at sentence #610000, processed 7874610 words, keeping 183533 word types\n",
      "2019-12-11 15:10:35,316 : INFO : PROGRESS: at sentence #620000, processed 8004074 words, keeping 185494 word types\n",
      "2019-12-11 15:10:35,354 : INFO : PROGRESS: at sentence #630000, processed 8133824 words, keeping 187562 word types\n",
      "2019-12-11 15:10:35,396 : INFO : PROGRESS: at sentence #640000, processed 8263848 words, keeping 189611 word types\n",
      "2019-12-11 15:10:35,455 : INFO : PROGRESS: at sentence #650000, processed 8393705 words, keeping 191548 word types\n",
      "2019-12-11 15:10:35,516 : INFO : PROGRESS: at sentence #660000, processed 8522653 words, keeping 193556 word types\n",
      "2019-12-11 15:10:35,573 : INFO : PROGRESS: at sentence #670000, processed 8651844 words, keeping 195488 word types\n",
      "2019-12-11 15:10:35,624 : INFO : PROGRESS: at sentence #680000, processed 8781618 words, keeping 197503 word types\n",
      "2019-12-11 15:10:35,680 : INFO : PROGRESS: at sentence #690000, processed 8911890 words, keeping 199420 word types\n",
      "2019-12-11 15:10:35,794 : INFO : PROGRESS: at sentence #700000, processed 9041314 words, keeping 201354 word types\n",
      "2019-12-11 15:10:35,893 : INFO : PROGRESS: at sentence #710000, processed 9171425 words, keeping 203187 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:10:35,996 : INFO : PROGRESS: at sentence #720000, processed 9301095 words, keeping 205074 word types\n",
      "2019-12-11 15:10:36,069 : INFO : PROGRESS: at sentence #730000, processed 9430444 words, keeping 207037 word types\n",
      "2019-12-11 15:10:36,126 : INFO : PROGRESS: at sentence #740000, processed 9558939 words, keeping 208953 word types\n",
      "2019-12-11 15:10:36,199 : INFO : PROGRESS: at sentence #750000, processed 9689112 words, keeping 210850 word types\n",
      "2019-12-11 15:10:36,228 : INFO : PROGRESS: at sentence #760000, processed 9819464 words, keeping 212716 word types\n",
      "2019-12-11 15:10:36,289 : INFO : PROGRESS: at sentence #770000, processed 9949877 words, keeping 214578 word types\n",
      "2019-12-11 15:10:36,354 : INFO : PROGRESS: at sentence #780000, processed 10079045 words, keeping 216375 word types\n",
      "2019-12-11 15:10:36,387 : INFO : PROGRESS: at sentence #790000, processed 10209081 words, keeping 218180 word types\n",
      "2019-12-11 15:10:36,433 : INFO : PROGRESS: at sentence #800000, processed 10339341 words, keeping 220058 word types\n",
      "2019-12-11 15:10:36,495 : INFO : PROGRESS: at sentence #810000, processed 10469567 words, keeping 221930 word types\n",
      "2019-12-11 15:10:36,551 : INFO : PROGRESS: at sentence #820000, processed 10598822 words, keeping 223651 word types\n",
      "2019-12-11 15:10:36,626 : INFO : PROGRESS: at sentence #830000, processed 10728630 words, keeping 225448 word types\n",
      "2019-12-11 15:10:36,691 : INFO : PROGRESS: at sentence #840000, processed 10858692 words, keeping 227266 word types\n",
      "2019-12-11 15:10:36,742 : INFO : PROGRESS: at sentence #850000, processed 10988262 words, keeping 229048 word types\n",
      "2019-12-11 15:10:36,809 : INFO : PROGRESS: at sentence #860000, processed 11118756 words, keeping 230908 word types\n",
      "2019-12-11 15:10:36,867 : INFO : PROGRESS: at sentence #870000, processed 11248537 words, keeping 232750 word types\n",
      "2019-12-11 15:10:36,921 : INFO : PROGRESS: at sentence #880000, processed 11378710 words, keeping 234564 word types\n",
      "2019-12-11 15:10:36,973 : INFO : PROGRESS: at sentence #890000, processed 11509036 words, keeping 236356 word types\n",
      "2019-12-11 15:10:37,030 : INFO : PROGRESS: at sentence #900000, processed 11638771 words, keeping 238110 word types\n",
      "2019-12-11 15:10:37,087 : INFO : PROGRESS: at sentence #910000, processed 11768573 words, keeping 239874 word types\n",
      "2019-12-11 15:10:37,123 : INFO : PROGRESS: at sentence #920000, processed 11899050 words, keeping 241716 word types\n",
      "2019-12-11 15:10:37,237 : INFO : PROGRESS: at sentence #930000, processed 12029799 words, keeping 243419 word types\n",
      "2019-12-11 15:10:37,501 : INFO : PROGRESS: at sentence #940000, processed 12158725 words, keeping 245141 word types\n",
      "2019-12-11 15:10:37,612 : INFO : PROGRESS: at sentence #950000, processed 12288268 words, keeping 246896 word types\n",
      "2019-12-11 15:10:37,677 : INFO : PROGRESS: at sentence #960000, processed 12419202 words, keeping 248697 word types\n",
      "2019-12-11 15:10:37,743 : INFO : PROGRESS: at sentence #970000, processed 12550050 words, keeping 250361 word types\n",
      "2019-12-11 15:10:37,778 : INFO : PROGRESS: at sentence #980000, processed 12679799 words, keeping 252119 word types\n",
      "2019-12-11 15:10:37,834 : INFO : PROGRESS: at sentence #990000, processed 12808511 words, keeping 253872 word types\n",
      "2019-12-11 15:10:37,876 : INFO : PROGRESS: at sentence #1000000, processed 12938495 words, keeping 255537 word types\n",
      "2019-12-11 15:10:37,915 : INFO : PROGRESS: at sentence #1010000, processed 13067180 words, keeping 257213 word types\n",
      "2019-12-11 15:10:37,964 : INFO : PROGRESS: at sentence #1020000, processed 13196942 words, keeping 258942 word types\n",
      "2019-12-11 15:10:38,007 : INFO : PROGRESS: at sentence #1030000, processed 13327666 words, keeping 260671 word types\n",
      "2019-12-11 15:10:38,066 : INFO : PROGRESS: at sentence #1040000, processed 13458448 words, keeping 262361 word types\n",
      "2019-12-11 15:10:38,105 : INFO : PROGRESS: at sentence #1050000, processed 13588201 words, keeping 264041 word types\n",
      "2019-12-11 15:10:38,169 : INFO : PROGRESS: at sentence #1060000, processed 13719041 words, keeping 265755 word types\n",
      "2019-12-11 15:10:38,200 : INFO : PROGRESS: at sentence #1070000, processed 13849872 words, keeping 267506 word types\n",
      "2019-12-11 15:10:38,243 : INFO : PROGRESS: at sentence #1080000, processed 13980611 words, keeping 269214 word types\n",
      "2019-12-11 15:10:38,284 : INFO : PROGRESS: at sentence #1090000, processed 14110723 words, keeping 270964 word types\n",
      "2019-12-11 15:10:38,316 : INFO : PROGRESS: at sentence #1100000, processed 14248470 words, keeping 273727 word types\n",
      "2019-12-11 15:10:38,361 : INFO : PROGRESS: at sentence #1110000, processed 14395236 words, keeping 277359 word types\n",
      "2019-12-11 15:10:38,404 : INFO : PROGRESS: at sentence #1120000, processed 14542237 words, keeping 280745 word types\n",
      "2019-12-11 15:10:38,452 : INFO : PROGRESS: at sentence #1130000, processed 14688409 words, keeping 284062 word types\n",
      "2019-12-11 15:10:38,498 : INFO : PROGRESS: at sentence #1140000, processed 14834829 words, keeping 287027 word types\n",
      "2019-12-11 15:10:38,534 : INFO : PROGRESS: at sentence #1150000, processed 14980614 words, keeping 290087 word types\n",
      "2019-12-11 15:10:38,580 : INFO : PROGRESS: at sentence #1160000, processed 15126516 words, keeping 293075 word types\n",
      "2019-12-11 15:10:38,629 : INFO : PROGRESS: at sentence #1170000, processed 15273132 words, keeping 295963 word types\n",
      "2019-12-11 15:10:38,684 : INFO : PROGRESS: at sentence #1180000, processed 15419004 words, keeping 298710 word types\n",
      "2019-12-11 15:10:38,726 : INFO : PROGRESS: at sentence #1190000, processed 15564890 words, keeping 301516 word types\n",
      "2019-12-11 15:10:38,791 : INFO : PROGRESS: at sentence #1200000, processed 15710658 words, keeping 304146 word types\n",
      "2019-12-11 15:10:38,828 : INFO : PROGRESS: at sentence #1210000, processed 15857802 words, keeping 306928 word types\n",
      "2019-12-11 15:10:38,875 : INFO : PROGRESS: at sentence #1220000, processed 16004560 words, keeping 309628 word types\n",
      "2019-12-11 15:10:38,910 : INFO : PROGRESS: at sentence #1230000, processed 16150775 words, keeping 312244 word types\n",
      "2019-12-11 15:10:38,954 : INFO : PROGRESS: at sentence #1240000, processed 16298196 words, keeping 314901 word types\n",
      "2019-12-11 15:10:38,989 : INFO : PROGRESS: at sentence #1250000, processed 16444834 words, keeping 317529 word types\n",
      "2019-12-11 15:10:39,056 : INFO : PROGRESS: at sentence #1260000, processed 16591767 words, keeping 320098 word types\n",
      "2019-12-11 15:10:39,092 : INFO : PROGRESS: at sentence #1270000, processed 16738150 words, keeping 322556 word types\n",
      "2019-12-11 15:10:39,134 : INFO : PROGRESS: at sentence #1280000, processed 16883466 words, keeping 325080 word types\n",
      "2019-12-11 15:10:39,179 : INFO : PROGRESS: at sentence #1290000, processed 17030375 words, keeping 327465 word types\n",
      "2019-12-11 15:10:39,217 : INFO : PROGRESS: at sentence #1300000, processed 17177123 words, keeping 329881 word types\n",
      "2019-12-11 15:10:39,256 : INFO : PROGRESS: at sentence #1310000, processed 17322957 words, keeping 332154 word types\n",
      "2019-12-11 15:10:39,294 : INFO : PROGRESS: at sentence #1320000, processed 17470081 words, keeping 334556 word types\n",
      "2019-12-11 15:10:39,333 : INFO : PROGRESS: at sentence #1330000, processed 17616244 words, keeping 336853 word types\n",
      "2019-12-11 15:10:39,375 : INFO : PROGRESS: at sentence #1340000, processed 17761863 words, keeping 339145 word types\n",
      "2019-12-11 15:10:39,416 : INFO : PROGRESS: at sentence #1350000, processed 17908543 words, keeping 341365 word types\n",
      "2019-12-11 15:10:39,458 : INFO : PROGRESS: at sentence #1360000, processed 18055291 words, keeping 343712 word types\n",
      "2019-12-11 15:10:39,500 : INFO : PROGRESS: at sentence #1370000, processed 18202064 words, keeping 345916 word types\n",
      "2019-12-11 15:10:39,538 : INFO : PROGRESS: at sentence #1380000, processed 18347651 words, keeping 348229 word types\n",
      "2019-12-11 15:10:39,594 : INFO : PROGRESS: at sentence #1390000, processed 18493979 words, keeping 350438 word types\n",
      "2019-12-11 15:10:39,631 : INFO : PROGRESS: at sentence #1400000, processed 18640395 words, keeping 352662 word types\n",
      "2019-12-11 15:10:39,670 : INFO : PROGRESS: at sentence #1410000, processed 18786650 words, keeping 354906 word types\n",
      "2019-12-11 15:10:39,710 : INFO : PROGRESS: at sentence #1420000, processed 18932995 words, keeping 357151 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:10:39,745 : INFO : PROGRESS: at sentence #1430000, processed 19078868 words, keeping 359312 word types\n",
      "2019-12-11 15:10:39,785 : INFO : PROGRESS: at sentence #1440000, processed 19225539 words, keeping 361430 word types\n",
      "2019-12-11 15:10:39,825 : INFO : PROGRESS: at sentence #1450000, processed 19372052 words, keeping 363486 word types\n",
      "2019-12-11 15:10:39,860 : INFO : PROGRESS: at sentence #1460000, processed 19518827 words, keeping 365528 word types\n",
      "2019-12-11 15:10:39,899 : INFO : PROGRESS: at sentence #1470000, processed 19665445 words, keeping 367671 word types\n",
      "2019-12-11 15:10:39,937 : INFO : PROGRESS: at sentence #1480000, processed 19811735 words, keeping 369824 word types\n",
      "2019-12-11 15:10:39,976 : INFO : PROGRESS: at sentence #1490000, processed 19958006 words, keeping 372046 word types\n",
      "2019-12-11 15:10:40,021 : INFO : PROGRESS: at sentence #1500000, processed 20105476 words, keeping 374098 word types\n",
      "2019-12-11 15:10:40,079 : INFO : PROGRESS: at sentence #1510000, processed 20252058 words, keeping 376140 word types\n",
      "2019-12-11 15:10:40,121 : INFO : PROGRESS: at sentence #1520000, processed 20399259 words, keeping 378252 word types\n",
      "2019-12-11 15:10:40,155 : INFO : PROGRESS: at sentence #1530000, processed 20545693 words, keeping 380295 word types\n",
      "2019-12-11 15:10:40,201 : INFO : PROGRESS: at sentence #1540000, processed 20691825 words, keeping 382282 word types\n",
      "2019-12-11 15:10:40,241 : INFO : PROGRESS: at sentence #1550000, processed 20838051 words, keeping 384282 word types\n",
      "2019-12-11 15:10:40,287 : INFO : PROGRESS: at sentence #1560000, processed 20984856 words, keeping 386189 word types\n",
      "2019-12-11 15:10:40,328 : INFO : PROGRESS: at sentence #1570000, processed 21132016 words, keeping 388243 word types\n",
      "2019-12-11 15:10:40,372 : INFO : PROGRESS: at sentence #1580000, processed 21278942 words, keeping 390181 word types\n",
      "2019-12-11 15:10:40,416 : INFO : PROGRESS: at sentence #1590000, processed 21426215 words, keeping 392152 word types\n",
      "2019-12-11 15:10:40,465 : INFO : PROGRESS: at sentence #1600000, processed 21573621 words, keeping 394107 word types\n",
      "2019-12-11 15:10:40,505 : INFO : PROGRESS: at sentence #1610000, processed 21719522 words, keeping 396042 word types\n",
      "2019-12-11 15:10:40,558 : INFO : PROGRESS: at sentence #1620000, processed 21866949 words, keeping 398070 word types\n",
      "2019-12-11 15:10:40,600 : INFO : PROGRESS: at sentence #1630000, processed 22013417 words, keeping 399914 word types\n",
      "2019-12-11 15:10:40,654 : INFO : PROGRESS: at sentence #1640000, processed 22160093 words, keeping 401748 word types\n",
      "2019-12-11 15:10:40,686 : INFO : PROGRESS: at sentence #1650000, processed 22306855 words, keeping 403673 word types\n",
      "2019-12-11 15:10:40,744 : INFO : PROGRESS: at sentence #1660000, processed 22453420 words, keeping 405605 word types\n",
      "2019-12-11 15:10:40,781 : INFO : PROGRESS: at sentence #1670000, processed 22600156 words, keeping 407495 word types\n",
      "2019-12-11 15:10:40,828 : INFO : PROGRESS: at sentence #1680000, processed 22745556 words, keeping 409358 word types\n",
      "2019-12-11 15:10:40,871 : INFO : PROGRESS: at sentence #1690000, processed 22891674 words, keeping 411259 word types\n",
      "2019-12-11 15:10:40,916 : INFO : PROGRESS: at sentence #1700000, processed 23038796 words, keeping 413162 word types\n",
      "2019-12-11 15:10:40,951 : INFO : PROGRESS: at sentence #1710000, processed 23184850 words, keeping 415043 word types\n",
      "2019-12-11 15:10:40,993 : INFO : PROGRESS: at sentence #1720000, processed 23332044 words, keeping 416930 word types\n",
      "2019-12-11 15:10:41,037 : INFO : PROGRESS: at sentence #1730000, processed 23479547 words, keeping 418871 word types\n",
      "2019-12-11 15:10:41,081 : INFO : PROGRESS: at sentence #1740000, processed 23625386 words, keeping 420753 word types\n",
      "2019-12-11 15:10:41,132 : INFO : PROGRESS: at sentence #1750000, processed 23772296 words, keeping 422563 word types\n",
      "2019-12-11 15:10:41,169 : INFO : PROGRESS: at sentence #1760000, processed 23917784 words, keeping 424374 word types\n",
      "2019-12-11 15:10:41,224 : INFO : PROGRESS: at sentence #1770000, processed 24064827 words, keeping 426302 word types\n",
      "2019-12-11 15:10:41,262 : INFO : PROGRESS: at sentence #1780000, processed 24211385 words, keeping 428131 word types\n",
      "2019-12-11 15:10:41,298 : INFO : PROGRESS: at sentence #1790000, processed 24357788 words, keeping 429924 word types\n",
      "2019-12-11 15:10:41,337 : INFO : PROGRESS: at sentence #1800000, processed 24505215 words, keeping 431717 word types\n",
      "2019-12-11 15:10:41,380 : INFO : PROGRESS: at sentence #1810000, processed 24652439 words, keeping 433521 word types\n",
      "2019-12-11 15:10:41,424 : INFO : PROGRESS: at sentence #1820000, processed 24799050 words, keeping 435352 word types\n",
      "2019-12-11 15:10:41,464 : INFO : PROGRESS: at sentence #1830000, processed 24945668 words, keeping 437126 word types\n",
      "2019-12-11 15:10:41,519 : INFO : PROGRESS: at sentence #1840000, processed 25091813 words, keeping 438945 word types\n",
      "2019-12-11 15:10:41,552 : INFO : PROGRESS: at sentence #1850000, processed 25237786 words, keeping 440698 word types\n",
      "2019-12-11 15:10:41,591 : INFO : PROGRESS: at sentence #1860000, processed 25384661 words, keeping 442498 word types\n",
      "2019-12-11 15:10:41,632 : INFO : PROGRESS: at sentence #1870000, processed 25530850 words, keeping 444230 word types\n",
      "2019-12-11 15:10:41,674 : INFO : PROGRESS: at sentence #1880000, processed 25676625 words, keeping 445984 word types\n",
      "2019-12-11 15:10:41,718 : INFO : PROGRESS: at sentence #1890000, processed 25822301 words, keeping 447762 word types\n",
      "2019-12-11 15:10:41,758 : INFO : PROGRESS: at sentence #1900000, processed 25969581 words, keeping 449486 word types\n",
      "2019-12-11 15:10:41,808 : INFO : PROGRESS: at sentence #1910000, processed 26116368 words, keeping 451280 word types\n",
      "2019-12-11 15:10:41,874 : INFO : PROGRESS: at sentence #1920000, processed 26262874 words, keeping 452954 word types\n",
      "2019-12-11 15:10:41,914 : INFO : PROGRESS: at sentence #1930000, processed 26409976 words, keeping 454644 word types\n",
      "2019-12-11 15:10:41,975 : INFO : PROGRESS: at sentence #1940000, processed 26557482 words, keeping 456418 word types\n",
      "2019-12-11 15:10:42,037 : INFO : PROGRESS: at sentence #1950000, processed 26703612 words, keeping 458110 word types\n",
      "2019-12-11 15:10:42,112 : INFO : PROGRESS: at sentence #1960000, processed 26849957 words, keeping 459798 word types\n",
      "2019-12-11 15:10:42,286 : INFO : PROGRESS: at sentence #1970000, processed 26996077 words, keeping 461466 word types\n",
      "2019-12-11 15:10:42,340 : INFO : PROGRESS: at sentence #1980000, processed 27143024 words, keeping 463122 word types\n",
      "2019-12-11 15:10:42,412 : INFO : PROGRESS: at sentence #1990000, processed 27290311 words, keeping 464902 word types\n",
      "2019-12-11 15:10:42,466 : INFO : PROGRESS: at sentence #2000000, processed 27438029 words, keeping 466621 word types\n",
      "2019-12-11 15:10:42,517 : INFO : PROGRESS: at sentence #2010000, processed 27585464 words, keeping 468288 word types\n",
      "2019-12-11 15:10:42,609 : INFO : PROGRESS: at sentence #2020000, processed 27733156 words, keeping 469930 word types\n",
      "2019-12-11 15:10:42,694 : INFO : PROGRESS: at sentence #2030000, processed 27879218 words, keeping 471589 word types\n",
      "2019-12-11 15:10:42,764 : INFO : PROGRESS: at sentence #2040000, processed 28026601 words, keeping 473342 word types\n",
      "2019-12-11 15:10:42,800 : INFO : PROGRESS: at sentence #2050000, processed 28173029 words, keeping 474971 word types\n",
      "2019-12-11 15:10:42,873 : INFO : PROGRESS: at sentence #2060000, processed 28319123 words, keeping 476627 word types\n",
      "2019-12-11 15:10:42,911 : INFO : PROGRESS: at sentence #2070000, processed 28465511 words, keeping 478258 word types\n",
      "2019-12-11 15:10:42,958 : INFO : PROGRESS: at sentence #2080000, processed 28612954 words, keeping 479918 word types\n",
      "2019-12-11 15:10:43,039 : INFO : PROGRESS: at sentence #2090000, processed 28760245 words, keeping 481507 word types\n",
      "2019-12-11 15:10:43,090 : INFO : PROGRESS: at sentence #2100000, processed 28907013 words, keeping 483152 word types\n",
      "2019-12-11 15:10:43,163 : INFO : PROGRESS: at sentence #2110000, processed 29054879 words, keeping 484939 word types\n",
      "2019-12-11 15:10:43,321 : INFO : PROGRESS: at sentence #2120000, processed 29202170 words, keeping 486622 word types\n",
      "2019-12-11 15:10:43,365 : INFO : PROGRESS: at sentence #2130000, processed 29349398 words, keeping 488277 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:10:43,407 : INFO : PROGRESS: at sentence #2140000, processed 29495123 words, keeping 489887 word types\n",
      "2019-12-11 15:10:43,455 : INFO : PROGRESS: at sentence #2150000, processed 29641809 words, keeping 491495 word types\n",
      "2019-12-11 15:10:43,510 : INFO : PROGRESS: at sentence #2160000, processed 29787640 words, keeping 493054 word types\n",
      "2019-12-11 15:10:43,561 : INFO : PROGRESS: at sentence #2170000, processed 29934864 words, keeping 494729 word types\n",
      "2019-12-11 15:10:43,631 : INFO : PROGRESS: at sentence #2180000, processed 30081595 words, keeping 496425 word types\n",
      "2019-12-11 15:10:43,655 : INFO : collected 496960 word types from a corpus of 30133876 raw words and 2183552 sentences\n",
      "2019-12-11 15:10:43,656 : INFO : Loading a fresh vocabulary\n",
      "2019-12-11 15:10:44,082 : INFO : effective_min_count=5 retains 83846 unique words (16% of original 496960, drops 413114)\n",
      "2019-12-11 15:10:44,083 : INFO : effective_min_count=5 leaves 29564409 word corpus (98% of original 30133876, drops 569467)\n",
      "2019-12-11 15:10:44,357 : INFO : deleting the raw counts dictionary of 496960 items\n",
      "2019-12-11 15:10:44,368 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2019-12-11 15:10:44,369 : INFO : downsampling leaves estimated 22520341 word corpus (76.2% of prior 29564409)\n",
      "2019-12-11 15:10:44,633 : INFO : estimated required memory for 83846 words and 300 dimensions: 243153400 bytes\n",
      "2019-12-11 15:10:44,634 : INFO : resetting layer weights\n",
      "2019-12-11 15:10:45,821 : INFO : training model with 1 workers on 83846 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-12-11 15:10:46,843 : INFO : EPOCH 1 - PROGRESS: at 1.21% examples, 246147 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:10:47,857 : INFO : EPOCH 1 - PROGRESS: at 2.84% examples, 289763 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:10:48,877 : INFO : EPOCH 1 - PROGRESS: at 4.55% examples, 308547 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:10:49,891 : INFO : EPOCH 1 - PROGRESS: at 6.25% examples, 318506 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:10:50,913 : INFO : EPOCH 1 - PROGRESS: at 7.99% examples, 325461 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:10:51,922 : INFO : EPOCH 1 - PROGRESS: at 9.66% examples, 328283 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:10:52,931 : INFO : EPOCH 1 - PROGRESS: at 11.40% examples, 332409 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:10:53,944 : INFO : EPOCH 1 - PROGRESS: at 13.11% examples, 334472 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:10:54,959 : INFO : EPOCH 1 - PROGRESS: at 14.74% examples, 334311 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:10:55,975 : INFO : EPOCH 1 - PROGRESS: at 16.37% examples, 334234 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:10:56,977 : INFO : EPOCH 1 - PROGRESS: at 17.96% examples, 333898 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:10:57,984 : INFO : EPOCH 1 - PROGRESS: at 19.45% examples, 331652 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:10:58,988 : INFO : EPOCH 1 - PROGRESS: at 21.15% examples, 333239 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:00,007 : INFO : EPOCH 1 - PROGRESS: at 22.89% examples, 334741 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:01,023 : INFO : EPOCH 1 - PROGRESS: at 24.62% examples, 336094 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:02,025 : INFO : EPOCH 1 - PROGRESS: at 26.17% examples, 335295 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:03,046 : INFO : EPOCH 1 - PROGRESS: at 27.65% examples, 333377 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:04,053 : INFO : EPOCH 1 - PROGRESS: at 29.03% examples, 330696 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:05,070 : INFO : EPOCH 1 - PROGRESS: at 30.66% examples, 330789 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:06,085 : INFO : EPOCH 1 - PROGRESS: at 32.28% examples, 330939 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:07,105 : INFO : EPOCH 1 - PROGRESS: at 33.97% examples, 331703 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:08,105 : INFO : EPOCH 1 - PROGRESS: at 35.59% examples, 332004 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:09,117 : INFO : EPOCH 1 - PROGRESS: at 37.28% examples, 332736 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:10,133 : INFO : EPOCH 1 - PROGRESS: at 38.97% examples, 333358 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:11,136 : INFO : EPOCH 1 - PROGRESS: at 40.63% examples, 333823 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:11:12,155 : INFO : EPOCH 1 - PROGRESS: at 42.42% examples, 335174 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:13,157 : INFO : EPOCH 1 - PROGRESS: at 44.21% examples, 336619 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:14,170 : INFO : EPOCH 1 - PROGRESS: at 46.05% examples, 338082 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:15,173 : INFO : EPOCH 1 - PROGRESS: at 47.85% examples, 339314 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:16,192 : INFO : EPOCH 1 - PROGRESS: at 49.46% examples, 339073 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:17,197 : INFO : EPOCH 1 - PROGRESS: at 50.82% examples, 337986 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:18,214 : INFO : EPOCH 1 - PROGRESS: at 51.97% examples, 336032 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:19,249 : INFO : EPOCH 1 - PROGRESS: at 53.16% examples, 334227 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:20,266 : INFO : EPOCH 1 - PROGRESS: at 54.32% examples, 332505 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:21,287 : INFO : EPOCH 1 - PROGRESS: at 55.51% examples, 331041 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:22,301 : INFO : EPOCH 1 - PROGRESS: at 56.76% examples, 330146 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:23,320 : INFO : EPOCH 1 - PROGRESS: at 58.07% examples, 329643 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:24,330 : INFO : EPOCH 1 - PROGRESS: at 59.41% examples, 329453 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:25,354 : INFO : EPOCH 1 - PROGRESS: at 60.72% examples, 328962 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:26,362 : INFO : EPOCH 1 - PROGRESS: at 61.82% examples, 327312 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:27,384 : INFO : EPOCH 1 - PROGRESS: at 63.01% examples, 326195 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:28,388 : INFO : EPOCH 1 - PROGRESS: at 64.04% examples, 324366 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:29,393 : INFO : EPOCH 1 - PROGRESS: at 65.35% examples, 324161 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:30,402 : INFO : EPOCH 1 - PROGRESS: at 66.76% examples, 324467 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:31,409 : INFO : EPOCH 1 - PROGRESS: at 68.10% examples, 324443 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:32,410 : INFO : EPOCH 1 - PROGRESS: at 69.41% examples, 324288 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:33,437 : INFO : EPOCH 1 - PROGRESS: at 70.75% examples, 324120 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:34,454 : INFO : EPOCH 1 - PROGRESS: at 72.06% examples, 323880 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:35,460 : INFO : EPOCH 1 - PROGRESS: at 73.43% examples, 324018 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:36,481 : INFO : EPOCH 1 - PROGRESS: at 74.80% examples, 324063 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:37,495 : INFO : EPOCH 1 - PROGRESS: at 76.18% examples, 324145 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:38,514 : INFO : EPOCH 1 - PROGRESS: at 77.52% examples, 324047 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:39,527 : INFO : EPOCH 1 - PROGRESS: at 78.86% examples, 323983 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:40,538 : INFO : EPOCH 1 - PROGRESS: at 80.24% examples, 324079 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:41,552 : INFO : EPOCH 1 - PROGRESS: at 81.74% examples, 324694 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:42,567 : INFO : EPOCH 1 - PROGRESS: at 83.23% examples, 325284 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:43,584 : INFO : EPOCH 1 - PROGRESS: at 84.58% examples, 325188 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:44,595 : INFO : EPOCH 1 - PROGRESS: at 86.05% examples, 325643 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:45,604 : INFO : EPOCH 1 - PROGRESS: at 87.54% examples, 326212 words/s, in_qsize 1, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:11:46,608 : INFO : EPOCH 1 - PROGRESS: at 88.94% examples, 326419 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:47,618 : INFO : EPOCH 1 - PROGRESS: at 90.45% examples, 326956 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:48,621 : INFO : EPOCH 1 - PROGRESS: at 91.87% examples, 327280 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:11:49,633 : INFO : EPOCH 1 - PROGRESS: at 93.37% examples, 327779 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:50,634 : INFO : EPOCH 1 - PROGRESS: at 94.81% examples, 328090 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:51,635 : INFO : EPOCH 1 - PROGRESS: at 96.24% examples, 328381 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:52,643 : INFO : EPOCH 1 - PROGRESS: at 97.67% examples, 328627 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:53,657 : INFO : EPOCH 1 - PROGRESS: at 99.10% examples, 328832 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:54,284 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 15:11:54,286 : INFO : EPOCH - 1 : training on 30133876 raw words (22521780 effective words) took 68.5s, 328990 effective words/s\n",
      "2019-12-11 15:11:55,309 : INFO : EPOCH 2 - PROGRESS: at 1.85% examples, 376364 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:11:56,321 : INFO : EPOCH 2 - PROGRESS: at 3.59% examples, 366231 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:57,328 : INFO : EPOCH 2 - PROGRESS: at 5.23% examples, 356334 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:58,405 : INFO : EPOCH 2 - PROGRESS: at 6.18% examples, 311341 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:11:59,425 : INFO : EPOCH 2 - PROGRESS: at 7.39% examples, 298229 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:00,442 : INFO : EPOCH 2 - PROGRESS: at 9.02% examples, 303966 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:01,451 : INFO : EPOCH 2 - PROGRESS: at 10.65% examples, 308384 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:02,461 : INFO : EPOCH 2 - PROGRESS: at 12.18% examples, 308977 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:03,480 : INFO : EPOCH 2 - PROGRESS: at 13.89% examples, 313143 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:04,499 : INFO : EPOCH 2 - PROGRESS: at 15.55% examples, 315752 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:05,505 : INFO : EPOCH 2 - PROGRESS: at 17.22% examples, 318290 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:06,518 : INFO : EPOCH 2 - PROGRESS: at 18.85% examples, 319604 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:07,518 : INFO : EPOCH 2 - PROGRESS: at 20.41% examples, 319914 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:08,527 : INFO : EPOCH 2 - PROGRESS: at 22.07% examples, 321556 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:09,538 : INFO : EPOCH 2 - PROGRESS: at 23.91% examples, 325344 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:10,542 : INFO : EPOCH 2 - PROGRESS: at 25.75% examples, 328775 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:11,553 : INFO : EPOCH 2 - PROGRESS: at 27.58% examples, 331713 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:12,558 : INFO : EPOCH 2 - PROGRESS: at 29.38% examples, 333977 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:13,561 : INFO : EPOCH 2 - PROGRESS: at 31.11% examples, 335295 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:14,580 : INFO : EPOCH 2 - PROGRESS: at 32.81% examples, 335870 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:15,581 : INFO : EPOCH 2 - PROGRESS: at 34.60% examples, 337701 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:16,647 : INFO : EPOCH 2 - PROGRESS: at 35.73% examples, 332124 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:17,667 : INFO : EPOCH 2 - PROGRESS: at 37.03% examples, 329263 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:18,668 : INFO : EPOCH 2 - PROGRESS: at 38.55% examples, 328724 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:19,680 : INFO : EPOCH 2 - PROGRESS: at 40.31% examples, 330118 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:20,697 : INFO : EPOCH 2 - PROGRESS: at 42.14% examples, 331904 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:21,698 : INFO : EPOCH 2 - PROGRESS: at 43.90% examples, 333215 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:22,702 : INFO : EPOCH 2 - PROGRESS: at 45.66% examples, 334402 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:23,712 : INFO : EPOCH 2 - PROGRESS: at 47.49% examples, 335913 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:24,720 : INFO : EPOCH 2 - PROGRESS: at 49.28% examples, 337136 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:25,741 : INFO : EPOCH 2 - PROGRESS: at 50.94% examples, 338077 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:26,760 : INFO : EPOCH 2 - PROGRESS: at 52.38% examples, 338198 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:27,762 : INFO : EPOCH 2 - PROGRESS: at 53.82% examples, 338471 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:28,779 : INFO : EPOCH 2 - PROGRESS: at 55.32% examples, 339025 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:29,802 : INFO : EPOCH 2 - PROGRESS: at 56.76% examples, 339063 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:30,811 : INFO : EPOCH 2 - PROGRESS: at 58.22% examples, 339442 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:31,825 : INFO : EPOCH 2 - PROGRESS: at 59.76% examples, 340163 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:32,845 : INFO : EPOCH 2 - PROGRESS: at 61.29% examples, 340783 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:33,854 : INFO : EPOCH 2 - PROGRESS: at 62.75% examples, 341088 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:34,857 : INFO : EPOCH 2 - PROGRESS: at 64.26% examples, 341603 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:35,876 : INFO : EPOCH 2 - PROGRESS: at 65.57% examples, 340864 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:36,886 : INFO : EPOCH 2 - PROGRESS: at 67.10% examples, 341490 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:37,887 : INFO : EPOCH 2 - PROGRESS: at 68.63% examples, 342153 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:38,904 : INFO : EPOCH 2 - PROGRESS: at 70.10% examples, 342304 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:39,907 : INFO : EPOCH 2 - PROGRESS: at 71.63% examples, 342906 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:12:40,933 : INFO : EPOCH 2 - PROGRESS: at 72.93% examples, 342176 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:12:41,945 : INFO : EPOCH 2 - PROGRESS: at 74.37% examples, 342215 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:42,952 : INFO : EPOCH 2 - PROGRESS: at 75.68% examples, 341669 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:44,021 : INFO : EPOCH 2 - PROGRESS: at 76.80% examples, 339793 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:12:45,046 : INFO : EPOCH 2 - PROGRESS: at 77.49% examples, 336203 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:46,066 : INFO : EPOCH 2 - PROGRESS: at 78.68% examples, 335118 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:47,080 : INFO : EPOCH 2 - PROGRESS: at 79.92% examples, 334419 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:48,096 : INFO : EPOCH 2 - PROGRESS: at 81.21% examples, 333863 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:49,119 : INFO : EPOCH 2 - PROGRESS: at 82.33% examples, 332607 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:50,128 : INFO : EPOCH 2 - PROGRESS: at 83.64% examples, 332283 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:51,153 : INFO : EPOCH 2 - PROGRESS: at 84.80% examples, 331213 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:52,170 : INFO : EPOCH 2 - PROGRESS: at 86.08% examples, 330745 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:53,189 : INFO : EPOCH 2 - PROGRESS: at 87.36% examples, 330287 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:54,192 : INFO : EPOCH 2 - PROGRESS: at 88.67% examples, 330050 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:55,223 : INFO : EPOCH 2 - PROGRESS: at 89.95% examples, 329549 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:12:56,226 : INFO : EPOCH 2 - PROGRESS: at 91.26% examples, 329347 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:57,245 : INFO : EPOCH 2 - PROGRESS: at 92.56% examples, 329060 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:12:58,250 : INFO : EPOCH 2 - PROGRESS: at 93.87% examples, 328860 words/s, in_qsize 1, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:12:59,268 : INFO : EPOCH 2 - PROGRESS: at 95.27% examples, 328948 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:00,281 : INFO : EPOCH 2 - PROGRESS: at 96.67% examples, 329052 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:01,294 : INFO : EPOCH 2 - PROGRESS: at 98.07% examples, 329148 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:02,301 : INFO : EPOCH 2 - PROGRESS: at 99.48% examples, 329271 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:02,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 15:13:02,672 : INFO : EPOCH - 2 : training on 30133876 raw words (22519505 effective words) took 68.4s, 329339 effective words/s\n",
      "2019-12-11 15:13:03,711 : INFO : EPOCH 3 - PROGRESS: at 1.57% examples, 314161 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:04,730 : INFO : EPOCH 3 - PROGRESS: at 3.20% examples, 322938 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:05,743 : INFO : EPOCH 3 - PROGRESS: at 4.80% examples, 324230 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:06,761 : INFO : EPOCH 3 - PROGRESS: at 6.15% examples, 311876 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:07,764 : INFO : EPOCH 3 - PROGRESS: at 7.71% examples, 314208 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:08,772 : INFO : EPOCH 3 - PROGRESS: at 9.41% examples, 320158 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:09,786 : INFO : EPOCH 3 - PROGRESS: at 11.08% examples, 323086 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:10,816 : INFO : EPOCH 3 - PROGRESS: at 12.36% examples, 314813 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:11,822 : INFO : EPOCH 3 - PROGRESS: at 13.78% examples, 312358 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:12,842 : INFO : EPOCH 3 - PROGRESS: at 15.52% examples, 316477 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:13,846 : INFO : EPOCH 3 - PROGRESS: at 17.18% examples, 319028 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:14,862 : INFO : EPOCH 3 - PROGRESS: at 18.49% examples, 314747 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:15,881 : INFO : EPOCH 3 - PROGRESS: at 19.98% examples, 313867 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:16,899 : INFO : EPOCH 3 - PROGRESS: at 21.65% examples, 315725 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:17,903 : INFO : EPOCH 3 - PROGRESS: at 23.31% examples, 317652 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:18,923 : INFO : EPOCH 3 - PROGRESS: at 24.87% examples, 317624 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:19,971 : INFO : EPOCH 3 - PROGRESS: at 26.31% examples, 315850 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:20,983 : INFO : EPOCH 3 - PROGRESS: at 27.48% examples, 311667 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:22,004 : INFO : EPOCH 3 - PROGRESS: at 29.06% examples, 312311 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:13:23,023 : INFO : EPOCH 3 - PROGRESS: at 30.73% examples, 313651 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:24,040 : INFO : EPOCH 3 - PROGRESS: at 32.42% examples, 315259 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:25,059 : INFO : EPOCH 3 - PROGRESS: at 34.11% examples, 316689 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:26,073 : INFO : EPOCH 3 - PROGRESS: at 35.70% examples, 317134 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:13:27,079 : INFO : EPOCH 3 - PROGRESS: at 37.35% examples, 318229 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:28,163 : INFO : EPOCH 3 - PROGRESS: at 37.64% examples, 306989 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:13:29,212 : INFO : EPOCH 3 - PROGRESS: at 38.48% examples, 301516 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:30,216 : INFO : EPOCH 3 - PROGRESS: at 38.73% examples, 292398 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:13:31,263 : INFO : EPOCH 3 - PROGRESS: at 39.25% examples, 285544 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:13:32,285 : INFO : EPOCH 3 - PROGRESS: at 39.61% examples, 278170 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:33,290 : INFO : EPOCH 3 - PROGRESS: at 40.10% examples, 272408 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:34,412 : INFO : EPOCH 3 - PROGRESS: at 40.63% examples, 266247 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:35,483 : INFO : EPOCH 3 - PROGRESS: at 40.98% examples, 259805 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:13:36,509 : INFO : EPOCH 3 - PROGRESS: at 41.33% examples, 254105 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:37,548 : INFO : EPOCH 3 - PROGRESS: at 41.89% examples, 249919 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:13:38,574 : INFO : EPOCH 3 - PROGRESS: at 42.35% examples, 245441 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:39,641 : INFO : EPOCH 3 - PROGRESS: at 42.70% examples, 240352 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:13:40,703 : INFO : EPOCH 3 - PROGRESS: at 43.13% examples, 235954 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:13:42,015 : INFO : EPOCH 3 - PROGRESS: at 43.52% examples, 230138 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:43,070 : INFO : EPOCH 3 - PROGRESS: at 43.90% examples, 226133 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:44,112 : INFO : EPOCH 3 - PROGRESS: at 44.53% examples, 223647 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:13:45,141 : INFO : EPOCH 3 - PROGRESS: at 45.52% examples, 223080 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:46,142 : INFO : EPOCH 3 - PROGRESS: at 47.00% examples, 225056 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:47,145 : INFO : EPOCH 3 - PROGRESS: at 48.06% examples, 224952 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:48,150 : INFO : EPOCH 3 - PROGRESS: at 49.25% examples, 225480 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:49,192 : INFO : EPOCH 3 - PROGRESS: at 50.35% examples, 225525 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:50,237 : INFO : EPOCH 3 - PROGRESS: at 51.03% examples, 224081 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:51,243 : INFO : EPOCH 3 - PROGRESS: at 52.00% examples, 224272 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:52,254 : INFO : EPOCH 3 - PROGRESS: at 53.29% examples, 225958 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:53,258 : INFO : EPOCH 3 - PROGRESS: at 54.57% examples, 227614 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:13:54,261 : INFO : EPOCH 3 - PROGRESS: at 55.79% examples, 228915 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:55,270 : INFO : EPOCH 3 - PROGRESS: at 57.01% examples, 230139 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:56,293 : INFO : EPOCH 3 - PROGRESS: at 58.29% examples, 231535 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:57,308 : INFO : EPOCH 3 - PROGRESS: at 59.57% examples, 232918 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:58,340 : INFO : EPOCH 3 - PROGRESS: at 60.82% examples, 234037 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:13:59,355 : INFO : EPOCH 3 - PROGRESS: at 61.63% examples, 233310 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:00,368 : INFO : EPOCH 3 - PROGRESS: at 62.32% examples, 232108 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:01,388 : INFO : EPOCH 3 - PROGRESS: at 62.91% examples, 230531 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:02,399 : INFO : EPOCH 3 - PROGRESS: at 63.54% examples, 229158 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:03,400 : INFO : EPOCH 3 - PROGRESS: at 64.66% examples, 229864 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:04,402 : INFO : EPOCH 3 - PROGRESS: at 65.98% examples, 231277 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:05,411 : INFO : EPOCH 3 - PROGRESS: at 67.32% examples, 232745 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:06,417 : INFO : EPOCH 3 - PROGRESS: at 68.66% examples, 234169 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:07,431 : INFO : EPOCH 3 - PROGRESS: at 69.94% examples, 235288 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:08,566 : INFO : EPOCH 3 - PROGRESS: at 70.94% examples, 234905 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:09,567 : INFO : EPOCH 3 - PROGRESS: at 71.60% examples, 233765 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:10,568 : INFO : EPOCH 3 - PROGRESS: at 72.53% examples, 233664 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:11,572 : INFO : EPOCH 3 - PROGRESS: at 73.71% examples, 234430 words/s, in_qsize 1, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:14:12,598 : INFO : EPOCH 3 - PROGRESS: at 74.56% examples, 233908 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:13,606 : INFO : EPOCH 3 - PROGRESS: at 75.58% examples, 234106 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:14,625 : INFO : EPOCH 3 - PROGRESS: at 76.83% examples, 234995 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:15,629 : INFO : EPOCH 3 - PROGRESS: at 78.06% examples, 235798 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:16,637 : INFO : EPOCH 3 - PROGRESS: at 79.33% examples, 236776 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:17,639 : INFO : EPOCH 3 - PROGRESS: at 80.65% examples, 237846 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:18,646 : INFO : EPOCH 3 - PROGRESS: at 81.77% examples, 238273 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:19,683 : INFO : EPOCH 3 - PROGRESS: at 82.45% examples, 237226 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:20,739 : INFO : EPOCH 3 - PROGRESS: at 82.89% examples, 235373 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:22,225 : INFO : EPOCH 3 - PROGRESS: at 83.08% examples, 231544 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:23,252 : INFO : EPOCH 3 - PROGRESS: at 83.70% examples, 230472 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:24,272 : INFO : EPOCH 3 - PROGRESS: at 84.64% examples, 230373 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:25,295 : INFO : EPOCH 3 - PROGRESS: at 85.48% examples, 229990 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:26,329 : INFO : EPOCH 3 - PROGRESS: at 86.30% examples, 229497 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:27,345 : INFO : EPOCH 3 - PROGRESS: at 87.11% examples, 229063 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:28,358 : INFO : EPOCH 3 - PROGRESS: at 87.92% examples, 228650 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:29,390 : INFO : EPOCH 3 - PROGRESS: at 88.73% examples, 228193 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:30,395 : INFO : EPOCH 3 - PROGRESS: at 89.51% examples, 227732 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:31,420 : INFO : EPOCH 3 - PROGRESS: at 90.45% examples, 227658 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:32,449 : INFO : EPOCH 3 - PROGRESS: at 91.53% examples, 227993 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:33,459 : INFO : EPOCH 3 - PROGRESS: at 92.56% examples, 228208 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:34,473 : INFO : EPOCH 3 - PROGRESS: at 93.40% examples, 227912 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:35,477 : INFO : EPOCH 3 - PROGRESS: at 94.40% examples, 228057 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:36,482 : INFO : EPOCH 3 - PROGRESS: at 95.49% examples, 228436 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:37,485 : INFO : EPOCH 3 - PROGRESS: at 96.64% examples, 228965 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:38,491 : INFO : EPOCH 3 - PROGRESS: at 97.85% examples, 229636 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:39,511 : INFO : EPOCH 3 - PROGRESS: at 99.13% examples, 230412 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:40,240 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 15:14:40,244 : INFO : EPOCH - 3 : training on 30133876 raw words (22520689 effective words) took 97.6s, 230833 effective words/s\n",
      "2019-12-11 15:14:41,266 : INFO : EPOCH 4 - PROGRESS: at 1.53% examples, 312407 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:42,276 : INFO : EPOCH 4 - PROGRESS: at 2.84% examples, 291035 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:43,283 : INFO : EPOCH 4 - PROGRESS: at 4.05% examples, 276790 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:44,301 : INFO : EPOCH 4 - PROGRESS: at 5.37% examples, 274442 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:45,320 : INFO : EPOCH 4 - PROGRESS: at 6.71% examples, 274422 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:46,332 : INFO : EPOCH 4 - PROGRESS: at 8.06% examples, 274794 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:47,346 : INFO : EPOCH 4 - PROGRESS: at 9.45% examples, 276011 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:48,359 : INFO : EPOCH 4 - PROGRESS: at 10.61% examples, 271491 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:49,380 : INFO : EPOCH 4 - PROGRESS: at 12.00% examples, 272579 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:14:50,384 : INFO : EPOCH 4 - PROGRESS: at 13.07% examples, 267338 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:51,407 : INFO : EPOCH 4 - PROGRESS: at 14.14% examples, 262631 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:52,451 : INFO : EPOCH 4 - PROGRESS: at 14.95% examples, 254030 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:53,462 : INFO : EPOCH 4 - PROGRESS: at 15.98% examples, 250766 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:54,470 : INFO : EPOCH 4 - PROGRESS: at 17.36% examples, 253169 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:55,481 : INFO : EPOCH 4 - PROGRESS: at 18.74% examples, 255200 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:56,488 : INFO : EPOCH 4 - PROGRESS: at 20.12% examples, 257057 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:57,503 : INFO : EPOCH 4 - PROGRESS: at 21.68% examples, 260709 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:58,527 : INFO : EPOCH 4 - PROGRESS: at 23.20% examples, 263437 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:14:59,547 : INFO : EPOCH 4 - PROGRESS: at 24.62% examples, 264764 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:00,570 : INFO : EPOCH 4 - PROGRESS: at 26.06% examples, 266284 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:01,570 : INFO : EPOCH 4 - PROGRESS: at 27.44% examples, 267255 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:15:02,593 : INFO : EPOCH 4 - PROGRESS: at 28.75% examples, 267208 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:03,596 : INFO : EPOCH 4 - PROGRESS: at 30.02% examples, 267057 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:04,620 : INFO : EPOCH 4 - PROGRESS: at 30.90% examples, 263391 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:15:05,635 : INFO : EPOCH 4 - PROGRESS: at 31.75% examples, 259824 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:15:06,654 : INFO : EPOCH 4 - PROGRESS: at 33.12% examples, 260658 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:15:07,675 : INFO : EPOCH 4 - PROGRESS: at 34.43% examples, 260880 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:08,676 : INFO : EPOCH 4 - PROGRESS: at 35.84% examples, 262057 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:09,697 : INFO : EPOCH 4 - PROGRESS: at 37.21% examples, 262721 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:10,704 : INFO : EPOCH 4 - PROGRESS: at 38.59% examples, 263454 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:11,719 : INFO : EPOCH 4 - PROGRESS: at 39.85% examples, 263378 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:12,736 : INFO : EPOCH 4 - PROGRESS: at 41.12% examples, 263286 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:13,755 : INFO : EPOCH 4 - PROGRESS: at 42.53% examples, 264071 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:14,762 : INFO : EPOCH 4 - PROGRESS: at 43.90% examples, 264685 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:15,786 : INFO : EPOCH 4 - PROGRESS: at 45.38% examples, 265758 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:15:16,805 : INFO : EPOCH 4 - PROGRESS: at 46.93% examples, 267217 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:17,825 : INFO : EPOCH 4 - PROGRESS: at 48.44% examples, 268392 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:18,826 : INFO : EPOCH 4 - PROGRESS: at 49.91% examples, 269432 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:19,836 : INFO : EPOCH 4 - PROGRESS: at 51.13% examples, 269805 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:20,850 : INFO : EPOCH 4 - PROGRESS: at 52.32% examples, 270147 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:21,871 : INFO : EPOCH 4 - PROGRESS: at 53.54% examples, 270614 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:15:22,882 : INFO : EPOCH 4 - PROGRESS: at 54.73% examples, 270946 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:23,903 : INFO : EPOCH 4 - PROGRESS: at 55.95% examples, 271382 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:24,925 : INFO : EPOCH 4 - PROGRESS: at 57.19% examples, 271950 words/s, in_qsize 1, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:15:25,944 : INFO : EPOCH 4 - PROGRESS: at 58.44% examples, 272511 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:26,959 : INFO : EPOCH 4 - PROGRESS: at 59.69% examples, 273074 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:27,969 : INFO : EPOCH 4 - PROGRESS: at 61.01% examples, 273954 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:28,977 : INFO : EPOCH 4 - PROGRESS: at 62.35% examples, 274971 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:29,997 : INFO : EPOCH 4 - PROGRESS: at 63.51% examples, 274964 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:15:31,008 : INFO : EPOCH 4 - PROGRESS: at 64.60% examples, 274692 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:32,029 : INFO : EPOCH 4 - PROGRESS: at 65.92% examples, 275404 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:33,052 : INFO : EPOCH 4 - PROGRESS: at 67.26% examples, 276234 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:34,059 : INFO : EPOCH 4 - PROGRESS: at 68.57% examples, 276969 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:35,072 : INFO : EPOCH 4 - PROGRESS: at 69.69% examples, 276815 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:36,096 : INFO : EPOCH 4 - PROGRESS: at 70.94% examples, 277153 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:37,103 : INFO : EPOCH 4 - PROGRESS: at 72.09% examples, 277171 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:38,116 : INFO : EPOCH 4 - PROGRESS: at 73.27% examples, 277285 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:39,118 : INFO : EPOCH 4 - PROGRESS: at 74.62% examples, 278094 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:40,129 : INFO : EPOCH 4 - PROGRESS: at 75.96% examples, 278827 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:41,130 : INFO : EPOCH 4 - PROGRESS: at 77.27% examples, 279457 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:42,143 : INFO : EPOCH 4 - PROGRESS: at 78.65% examples, 280255 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:43,160 : INFO : EPOCH 4 - PROGRESS: at 79.99% examples, 280898 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:44,175 : INFO : EPOCH 4 - PROGRESS: at 81.27% examples, 281288 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:45,188 : INFO : EPOCH 4 - PROGRESS: at 82.42% examples, 281207 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:46,209 : INFO : EPOCH 4 - PROGRESS: at 83.89% examples, 282245 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:47,214 : INFO : EPOCH 4 - PROGRESS: at 85.39% examples, 283426 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:48,221 : INFO : EPOCH 4 - PROGRESS: at 86.95% examples, 284781 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:49,236 : INFO : EPOCH 4 - PROGRESS: at 88.54% examples, 286182 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:50,245 : INFO : EPOCH 4 - PROGRESS: at 90.14% examples, 287562 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:51,266 : INFO : EPOCH 4 - PROGRESS: at 91.69% examples, 288750 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:52,276 : INFO : EPOCH 4 - PROGRESS: at 93.15% examples, 289634 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:53,278 : INFO : EPOCH 4 - PROGRESS: at 94.71% examples, 290840 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:54,291 : INFO : EPOCH 4 - PROGRESS: at 96.30% examples, 292064 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:55,301 : INFO : EPOCH 4 - PROGRESS: at 97.88% examples, 293268 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:56,314 : INFO : EPOCH 4 - PROGRESS: at 99.48% examples, 294430 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:56,668 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 15:15:56,669 : INFO : EPOCH - 4 : training on 30133876 raw words (22521145 effective words) took 76.4s, 294723 effective words/s\n",
      "2019-12-11 15:15:57,690 : INFO : EPOCH 5 - PROGRESS: at 1.64% examples, 333162 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:58,708 : INFO : EPOCH 5 - PROGRESS: at 3.55% examples, 361706 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:15:59,714 : INFO : EPOCH 5 - PROGRESS: at 5.48% examples, 372810 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:00,716 : INFO : EPOCH 5 - PROGRESS: at 7.39% examples, 378688 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:01,726 : INFO : EPOCH 5 - PROGRESS: at 9.30% examples, 381560 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:02,738 : INFO : EPOCH 5 - PROGRESS: at 11.22% examples, 383337 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:03,746 : INFO : EPOCH 5 - PROGRESS: at 13.14% examples, 384922 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:04,755 : INFO : EPOCH 5 - PROGRESS: at 15.06% examples, 385978 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:05,765 : INFO : EPOCH 5 - PROGRESS: at 16.97% examples, 386844 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:06,776 : INFO : EPOCH 5 - PROGRESS: at 18.88% examples, 387444 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:07,780 : INFO : EPOCH 5 - PROGRESS: at 20.79% examples, 388207 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:08,782 : INFO : EPOCH 5 - PROGRESS: at 22.71% examples, 388891 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:09,787 : INFO : EPOCH 5 - PROGRESS: at 24.58% examples, 388869 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:10,795 : INFO : EPOCH 5 - PROGRESS: at 26.42% examples, 388233 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:11,798 : INFO : EPOCH 5 - PROGRESS: at 28.29% examples, 388240 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:12,812 : INFO : EPOCH 5 - PROGRESS: at 29.98% examples, 385708 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:13,818 : INFO : EPOCH 5 - PROGRESS: at 31.82% examples, 385418 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:14,829 : INFO : EPOCH 5 - PROGRESS: at 33.72% examples, 385843 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:15,832 : INFO : EPOCH 5 - PROGRESS: at 35.59% examples, 386013 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:16,842 : INFO : EPOCH 5 - PROGRESS: at 37.50% examples, 386365 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:17,850 : INFO : EPOCH 5 - PROGRESS: at 39.36% examples, 386387 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:18,857 : INFO : EPOCH 5 - PROGRESS: at 41.26% examples, 386756 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:19,864 : INFO : EPOCH 5 - PROGRESS: at 43.16% examples, 387094 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:20,875 : INFO : EPOCH 5 - PROGRESS: at 45.03% examples, 387049 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:21,880 : INFO : EPOCH 5 - PROGRESS: at 46.86% examples, 386822 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:22,883 : INFO : EPOCH 5 - PROGRESS: at 48.72% examples, 386910 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:23,885 : INFO : EPOCH 5 - PROGRESS: at 50.53% examples, 387112 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:24,896 : INFO : EPOCH 5 - PROGRESS: at 52.13% examples, 386934 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:25,901 : INFO : EPOCH 5 - PROGRESS: at 53.69% examples, 386584 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:26,906 : INFO : EPOCH 5 - PROGRESS: at 55.26% examples, 386265 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:27,909 : INFO : EPOCH 5 - PROGRESS: at 56.82% examples, 385985 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:16:28,925 : INFO : EPOCH 5 - PROGRESS: at 58.41% examples, 385795 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:29,939 : INFO : EPOCH 5 - PROGRESS: at 60.01% examples, 385624 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:30,948 : INFO : EPOCH 5 - PROGRESS: at 61.48% examples, 384640 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:31,970 : INFO : EPOCH 5 - PROGRESS: at 62.85% examples, 382956 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:32,987 : INFO : EPOCH 5 - PROGRESS: at 64.32% examples, 382024 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:33,993 : INFO : EPOCH 5 - PROGRESS: at 65.88% examples, 381851 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:35,006 : INFO : EPOCH 5 - PROGRESS: at 67.48% examples, 381833 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:36,012 : INFO : EPOCH 5 - PROGRESS: at 69.04% examples, 381675 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:37,029 : INFO : EPOCH 5 - PROGRESS: at 70.53% examples, 381054 words/s, in_qsize 1, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:16:38,044 : INFO : EPOCH 5 - PROGRESS: at 72.06% examples, 380674 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:39,055 : INFO : EPOCH 5 - PROGRESS: at 73.59% examples, 380346 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:40,061 : INFO : EPOCH 5 - PROGRESS: at 75.05% examples, 379727 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:41,076 : INFO : EPOCH 5 - PROGRESS: at 76.58% examples, 379419 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:42,083 : INFO : EPOCH 5 - PROGRESS: at 78.15% examples, 379321 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:43,098 : INFO : EPOCH 5 - PROGRESS: at 79.71% examples, 379186 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:44,106 : INFO : EPOCH 5 - PROGRESS: at 81.27% examples, 379088 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:45,110 : INFO : EPOCH 5 - PROGRESS: at 82.83% examples, 379044 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:46,121 : INFO : EPOCH 5 - PROGRESS: at 84.29% examples, 378478 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:47,138 : INFO : EPOCH 5 - PROGRESS: at 85.83% examples, 378183 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:48,144 : INFO : EPOCH 5 - PROGRESS: at 87.39% examples, 378134 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:49,165 : INFO : EPOCH 5 - PROGRESS: at 88.98% examples, 378126 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:50,168 : INFO : EPOCH 5 - PROGRESS: at 90.54% examples, 378095 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:51,171 : INFO : EPOCH 5 - PROGRESS: at 92.09% examples, 378073 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:52,178 : INFO : EPOCH 5 - PROGRESS: at 93.65% examples, 378026 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:53,195 : INFO : EPOCH 5 - PROGRESS: at 95.24% examples, 378041 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:16:54,198 : INFO : EPOCH 5 - PROGRESS: at 96.79% examples, 378018 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:55,201 : INFO : EPOCH 5 - PROGRESS: at 98.35% examples, 377992 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:56,216 : INFO : EPOCH 5 - PROGRESS: at 99.91% examples, 377890 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:56,268 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 15:16:56,268 : INFO : EPOCH - 5 : training on 30133876 raw words (22520479 effective words) took 59.6s, 377908 effective words/s\n",
      "2019-12-11 15:16:57,275 : INFO : EPOCH 6 - PROGRESS: at 1.92% examples, 395393 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:58,292 : INFO : EPOCH 6 - PROGRESS: at 3.87% examples, 396411 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:16:59,304 : INFO : EPOCH 6 - PROGRESS: at 5.83% examples, 397585 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:00,318 : INFO : EPOCH 6 - PROGRESS: at 7.74% examples, 396392 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:01,329 : INFO : EPOCH 6 - PROGRESS: at 9.09% examples, 372403 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:02,337 : INFO : EPOCH 6 - PROGRESS: at 10.94% examples, 373526 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:03,347 : INFO : EPOCH 6 - PROGRESS: at 12.86% examples, 376414 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:04,350 : INFO : EPOCH 6 - PROGRESS: at 14.77% examples, 378782 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:05,357 : INFO : EPOCH 6 - PROGRESS: at 16.61% examples, 378955 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:06,357 : INFO : EPOCH 6 - PROGRESS: at 18.49% examples, 380042 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:07,371 : INFO : EPOCH 6 - PROGRESS: at 20.41% examples, 381145 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:08,389 : INFO : EPOCH 6 - PROGRESS: at 22.32% examples, 381923 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:09,394 : INFO : EPOCH 6 - PROGRESS: at 24.19% examples, 382413 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:10,407 : INFO : EPOCH 6 - PROGRESS: at 26.06% examples, 382635 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:11,417 : INFO : EPOCH 6 - PROGRESS: at 27.97% examples, 383363 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:12,426 : INFO : EPOCH 6 - PROGRESS: at 29.87% examples, 383968 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:13,440 : INFO : EPOCH 6 - PROGRESS: at 31.78% examples, 384443 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:14,440 : INFO : EPOCH 6 - PROGRESS: at 32.91% examples, 376249 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:15,442 : INFO : EPOCH 6 - PROGRESS: at 34.64% examples, 375401 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:17:16,456 : INFO : EPOCH 6 - PROGRESS: at 36.54% examples, 376222 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:17,471 : INFO : EPOCH 6 - PROGRESS: at 38.45% examples, 376937 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:18,472 : INFO : EPOCH 6 - PROGRESS: at 40.31% examples, 377520 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:17:19,489 : INFO : EPOCH 6 - PROGRESS: at 42.21% examples, 378097 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:20,500 : INFO : EPOCH 6 - PROGRESS: at 44.04% examples, 378103 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:21,518 : INFO : EPOCH 6 - PROGRESS: at 45.91% examples, 378308 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:22,534 : INFO : EPOCH 6 - PROGRESS: at 47.81% examples, 378821 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:23,550 : INFO : EPOCH 6 - PROGRESS: at 49.70% examples, 379303 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:24,551 : INFO : EPOCH 6 - PROGRESS: at 51.35% examples, 379471 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:25,562 : INFO : EPOCH 6 - PROGRESS: at 52.94% examples, 379560 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:26,581 : INFO : EPOCH 6 - PROGRESS: at 54.51% examples, 379288 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:27,594 : INFO : EPOCH 6 - PROGRESS: at 56.07% examples, 379114 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:28,612 : INFO : EPOCH 6 - PROGRESS: at 57.66% examples, 379098 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:29,631 : INFO : EPOCH 6 - PROGRESS: at 59.22% examples, 378865 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:30,650 : INFO : EPOCH 6 - PROGRESS: at 60.75% examples, 378436 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:31,662 : INFO : EPOCH 6 - PROGRESS: at 62.32% examples, 378296 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:32,678 : INFO : EPOCH 6 - PROGRESS: at 63.91% examples, 378340 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:33,689 : INFO : EPOCH 6 - PROGRESS: at 65.35% examples, 377403 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:34,709 : INFO : EPOCH 6 - PROGRESS: at 66.95% examples, 377446 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:35,726 : INFO : EPOCH 6 - PROGRESS: at 68.54% examples, 377476 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:36,728 : INFO : EPOCH 6 - PROGRESS: at 70.10% examples, 377470 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:37,733 : INFO : EPOCH 6 - PROGRESS: at 71.66% examples, 377444 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:38,740 : INFO : EPOCH 6 - PROGRESS: at 73.18% examples, 377227 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:39,759 : INFO : EPOCH 6 - PROGRESS: at 74.77% examples, 377266 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:40,759 : INFO : EPOCH 6 - PROGRESS: at 76.33% examples, 377296 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:41,769 : INFO : EPOCH 6 - PROGRESS: at 77.93% examples, 377388 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:42,780 : INFO : EPOCH 6 - PROGRESS: at 79.52% examples, 377471 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:43,790 : INFO : EPOCH 6 - PROGRESS: at 81.11% examples, 377561 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:44,809 : INFO : EPOCH 6 - PROGRESS: at 82.70% examples, 377584 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:45,819 : INFO : EPOCH 6 - PROGRESS: at 84.29% examples, 377666 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:46,822 : INFO : EPOCH 6 - PROGRESS: at 85.86% examples, 377645 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:47,837 : INFO : EPOCH 6 - PROGRESS: at 87.45% examples, 377687 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:48,847 : INFO : EPOCH 6 - PROGRESS: at 89.04% examples, 377765 words/s, in_qsize 1, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:17:49,851 : INFO : EPOCH 6 - PROGRESS: at 90.60% examples, 377742 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:50,852 : INFO : EPOCH 6 - PROGRESS: at 92.15% examples, 377740 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:51,870 : INFO : EPOCH 6 - PROGRESS: at 93.71% examples, 377623 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:52,885 : INFO : EPOCH 6 - PROGRESS: at 95.30% examples, 377665 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:53,900 : INFO : EPOCH 6 - PROGRESS: at 96.89% examples, 377697 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:54,913 : INFO : EPOCH 6 - PROGRESS: at 98.48% examples, 377737 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:55,902 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 15:17:55,903 : INFO : EPOCH - 6 : training on 30133876 raw words (22519410 effective words) took 59.6s, 377646 effective words/s\n",
      "2019-12-11 15:17:56,919 : INFO : EPOCH 7 - PROGRESS: at 1.96% examples, 401005 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:57,936 : INFO : EPOCH 7 - PROGRESS: at 3.91% examples, 399297 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:58,954 : INFO : EPOCH 7 - PROGRESS: at 5.87% examples, 398776 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:17:59,955 : INFO : EPOCH 7 - PROGRESS: at 7.78% examples, 398407 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:00,956 : INFO : EPOCH 7 - PROGRESS: at 9.69% examples, 398076 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:01,958 : INFO : EPOCH 7 - PROGRESS: at 11.51% examples, 394163 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:02,974 : INFO : EPOCH 7 - PROGRESS: at 13.36% examples, 391630 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:03,979 : INFO : EPOCH 7 - PROGRESS: at 15.27% examples, 392119 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:04,980 : INFO : EPOCH 7 - PROGRESS: at 17.18% examples, 392650 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:05,996 : INFO : EPOCH 7 - PROGRESS: at 19.06% examples, 391730 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:06,996 : INFO : EPOCH 7 - PROGRESS: at 20.94% examples, 391596 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:07,999 : INFO : EPOCH 7 - PROGRESS: at 22.85% examples, 392019 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:09,001 : INFO : EPOCH 7 - PROGRESS: at 24.76% examples, 392363 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:18:10,012 : INFO : EPOCH 7 - PROGRESS: at 26.49% examples, 389836 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:11,017 : INFO : EPOCH 7 - PROGRESS: at 28.36% examples, 389734 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:18:12,024 : INFO : EPOCH 7 - PROGRESS: at 30.23% examples, 389551 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:13,030 : INFO : EPOCH 7 - PROGRESS: at 32.14% examples, 389867 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:14,031 : INFO : EPOCH 7 - PROGRESS: at 34.04% examples, 390250 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:15,041 : INFO : EPOCH 7 - PROGRESS: at 35.94% examples, 390441 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:16,044 : INFO : EPOCH 7 - PROGRESS: at 37.81% examples, 390337 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:17,050 : INFO : EPOCH 7 - PROGRESS: at 39.71% examples, 390543 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:18,063 : INFO : EPOCH 7 - PROGRESS: at 41.61% examples, 390619 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:19,068 : INFO : EPOCH 7 - PROGRESS: at 43.48% examples, 390509 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:20,074 : INFO : EPOCH 7 - PROGRESS: at 45.38% examples, 390704 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:21,087 : INFO : EPOCH 7 - PROGRESS: at 47.28% examples, 390771 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:22,094 : INFO : EPOCH 7 - PROGRESS: at 49.14% examples, 390651 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:18:23,110 : INFO : EPOCH 7 - PROGRESS: at 50.91% examples, 390616 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:24,116 : INFO : EPOCH 7 - PROGRESS: at 52.47% examples, 390122 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:25,117 : INFO : EPOCH 7 - PROGRESS: at 53.95% examples, 388928 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:26,120 : INFO : EPOCH 7 - PROGRESS: at 55.48% examples, 388318 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:27,121 : INFO : EPOCH 7 - PROGRESS: at 57.04% examples, 387988 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:28,133 : INFO : EPOCH 7 - PROGRESS: at 58.63% examples, 387782 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:29,139 : INFO : EPOCH 7 - PROGRESS: at 60.22% examples, 387648 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:30,139 : INFO : EPOCH 7 - PROGRESS: at 61.79% examples, 387363 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:18:31,149 : INFO : EPOCH 7 - PROGRESS: at 63.38% examples, 387217 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:32,156 : INFO : EPOCH 7 - PROGRESS: at 64.98% examples, 387084 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:33,167 : INFO : EPOCH 7 - PROGRESS: at 66.57% examples, 386935 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:34,178 : INFO : EPOCH 7 - PROGRESS: at 68.16% examples, 386789 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:35,185 : INFO : EPOCH 7 - PROGRESS: at 69.75% examples, 386689 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:36,197 : INFO : EPOCH 7 - PROGRESS: at 71.16% examples, 385424 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:37,216 : INFO : EPOCH 7 - PROGRESS: at 72.62% examples, 384525 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:38,227 : INFO : EPOCH 7 - PROGRESS: at 74.18% examples, 384291 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:39,230 : INFO : EPOCH 7 - PROGRESS: at 75.74% examples, 384128 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:40,237 : INFO : EPOCH 7 - PROGRESS: at 77.34% examples, 384104 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:41,256 : INFO : EPOCH 7 - PROGRESS: at 78.93% examples, 383972 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:42,264 : INFO : EPOCH 7 - PROGRESS: at 80.52% examples, 383946 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:43,280 : INFO : EPOCH 7 - PROGRESS: at 82.08% examples, 383700 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:44,287 : INFO : EPOCH 7 - PROGRESS: at 83.67% examples, 383685 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:18:45,304 : INFO : EPOCH 7 - PROGRESS: at 85.26% examples, 383589 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:46,313 : INFO : EPOCH 7 - PROGRESS: at 86.83% examples, 383413 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:47,326 : INFO : EPOCH 7 - PROGRESS: at 88.42% examples, 383354 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:48,338 : INFO : EPOCH 7 - PROGRESS: at 90.01% examples, 383307 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:49,348 : INFO : EPOCH 7 - PROGRESS: at 91.60% examples, 383267 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:50,367 : INFO : EPOCH 7 - PROGRESS: at 93.00% examples, 382347 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:51,377 : INFO : EPOCH 7 - PROGRESS: at 94.52% examples, 382069 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:18:52,395 : INFO : EPOCH 7 - PROGRESS: at 96.11% examples, 382006 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:53,397 : INFO : EPOCH 7 - PROGRESS: at 97.67% examples, 381921 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:54,403 : INFO : EPOCH 7 - PROGRESS: at 99.23% examples, 381800 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:54,892 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 15:18:54,893 : INFO : EPOCH - 7 : training on 30133876 raw words (22519541 effective words) took 59.0s, 381808 effective words/s\n",
      "2019-12-11 15:18:55,905 : INFO : EPOCH 8 - PROGRESS: at 1.96% examples, 402192 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:56,923 : INFO : EPOCH 8 - PROGRESS: at 3.55% examples, 363358 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:57,936 : INFO : EPOCH 8 - PROGRESS: at 5.48% examples, 373107 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:58,953 : INFO : EPOCH 8 - PROGRESS: at 7.42% examples, 379372 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:18:59,970 : INFO : EPOCH 8 - PROGRESS: at 9.34% examples, 381648 words/s, in_qsize 1, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:19:00,978 : INFO : EPOCH 8 - PROGRESS: at 11.26% examples, 383661 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:01,994 : INFO : EPOCH 8 - PROGRESS: at 13.18% examples, 384734 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:03,005 : INFO : EPOCH 8 - PROGRESS: at 15.09% examples, 385793 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:04,010 : INFO : EPOCH 8 - PROGRESS: at 17.01% examples, 386857 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:05,026 : INFO : EPOCH 8 - PROGRESS: at 18.92% examples, 387273 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:06,033 : INFO : EPOCH 8 - PROGRESS: at 20.79% examples, 387310 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:07,047 : INFO : EPOCH 8 - PROGRESS: at 22.71% examples, 387699 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:08,052 : INFO : EPOCH 8 - PROGRESS: at 24.62% examples, 388254 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:09,060 : INFO : EPOCH 8 - PROGRESS: at 26.52% examples, 388713 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:10,063 : INFO : EPOCH 8 - PROGRESS: at 28.43% examples, 389220 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:11,074 : INFO : EPOCH 8 - PROGRESS: at 30.34% examples, 389450 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:12,084 : INFO : EPOCH 8 - PROGRESS: at 32.24% examples, 389672 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:13,093 : INFO : EPOCH 8 - PROGRESS: at 34.15% examples, 389911 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:14,105 : INFO : EPOCH 8 - PROGRESS: at 36.05% examples, 390058 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:19:15,112 : INFO : EPOCH 8 - PROGRESS: at 37.95% examples, 390273 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:16,125 : INFO : EPOCH 8 - PROGRESS: at 39.85% examples, 390383 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:17,139 : INFO : EPOCH 8 - PROGRESS: at 41.75% examples, 390462 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:18,145 : INFO : EPOCH 8 - PROGRESS: at 43.65% examples, 390662 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:19,147 : INFO : EPOCH 8 - PROGRESS: at 45.38% examples, 389376 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:19:20,153 : INFO : EPOCH 8 - PROGRESS: at 47.28% examples, 389606 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:21,161 : INFO : EPOCH 8 - PROGRESS: at 49.14% examples, 389502 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:22,165 : INFO : EPOCH 8 - PROGRESS: at 50.85% examples, 389121 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:23,178 : INFO : EPOCH 8 - PROGRESS: at 52.44% examples, 388840 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:24,190 : INFO : EPOCH 8 - PROGRESS: at 54.04% examples, 388583 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:25,206 : INFO : EPOCH 8 - PROGRESS: at 55.63% examples, 388294 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:19:26,222 : INFO : EPOCH 8 - PROGRESS: at 57.22% examples, 388026 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:27,228 : INFO : EPOCH 8 - PROGRESS: at 58.76% examples, 387415 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:28,229 : INFO : EPOCH 8 - PROGRESS: at 60.32% examples, 387139 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:29,247 : INFO : EPOCH 8 - PROGRESS: at 61.91% examples, 386895 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:30,266 : INFO : EPOCH 8 - PROGRESS: at 63.51% examples, 386667 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:31,280 : INFO : EPOCH 8 - PROGRESS: at 65.10% examples, 386477 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:32,298 : INFO : EPOCH 8 - PROGRESS: at 66.70% examples, 386276 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:33,312 : INFO : EPOCH 8 - PROGRESS: at 68.29% examples, 386128 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:34,320 : INFO : EPOCH 8 - PROGRESS: at 69.88% examples, 386036 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:35,337 : INFO : EPOCH 8 - PROGRESS: at 71.47% examples, 385867 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:19:36,355 : INFO : EPOCH 8 - PROGRESS: at 73.06% examples, 385705 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:37,365 : INFO : EPOCH 8 - PROGRESS: at 74.65% examples, 385619 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:38,380 : INFO : EPOCH 8 - PROGRESS: at 76.21% examples, 385315 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:39,394 : INFO : EPOCH 8 - PROGRESS: at 77.80% examples, 385197 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:40,402 : INFO : EPOCH 8 - PROGRESS: at 79.39% examples, 385149 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:41,407 : INFO : EPOCH 8 - PROGRESS: at 80.99% examples, 385106 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:42,420 : INFO : EPOCH 8 - PROGRESS: at 82.58% examples, 385020 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:43,427 : INFO : EPOCH 8 - PROGRESS: at 84.14% examples, 384824 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:44,428 : INFO : EPOCH 8 - PROGRESS: at 85.70% examples, 384687 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:45,435 : INFO : EPOCH 8 - PROGRESS: at 87.30% examples, 384642 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:19:46,455 : INFO : EPOCH 8 - PROGRESS: at 88.76% examples, 383923 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:19:47,474 : INFO : EPOCH 8 - PROGRESS: at 90.17% examples, 382942 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:48,494 : INFO : EPOCH 8 - PROGRESS: at 91.47% examples, 381578 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:49,508 : INFO : EPOCH 8 - PROGRESS: at 92.84% examples, 380581 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:50,516 : INFO : EPOCH 8 - PROGRESS: at 94.40% examples, 380490 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:51,519 : INFO : EPOCH 8 - PROGRESS: at 95.96% examples, 380424 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:52,536 : INFO : EPOCH 8 - PROGRESS: at 97.51% examples, 380272 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:53,537 : INFO : EPOCH 8 - PROGRESS: at 99.07% examples, 380217 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:54,136 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 15:19:54,136 : INFO : EPOCH - 8 : training on 30133876 raw words (22519938 effective words) took 59.2s, 380172 effective words/s\n",
      "2019-12-11 15:19:55,146 : INFO : EPOCH 9 - PROGRESS: at 1.96% examples, 401741 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:56,164 : INFO : EPOCH 9 - PROGRESS: at 3.91% examples, 399435 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:57,180 : INFO : EPOCH 9 - PROGRESS: at 5.87% examples, 399050 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:58,184 : INFO : EPOCH 9 - PROGRESS: at 7.78% examples, 398246 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:19:59,185 : INFO : EPOCH 9 - PROGRESS: at 9.69% examples, 397988 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:00,191 : INFO : EPOCH 9 - PROGRESS: at 11.61% examples, 397484 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:01,192 : INFO : EPOCH 9 - PROGRESS: at 13.53% examples, 397384 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:20:02,196 : INFO : EPOCH 9 - PROGRESS: at 15.45% examples, 397236 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:03,200 : INFO : EPOCH 9 - PROGRESS: at 17.29% examples, 395497 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:04,212 : INFO : EPOCH 9 - PROGRESS: at 18.92% examples, 389351 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:05,222 : INFO : EPOCH 9 - PROGRESS: at 20.83% examples, 389729 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:06,238 : INFO : EPOCH 9 - PROGRESS: at 22.74% examples, 389866 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:07,252 : INFO : EPOCH 9 - PROGRESS: at 24.65% examples, 390019 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:08,260 : INFO : EPOCH 9 - PROGRESS: at 26.56% examples, 390351 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:09,278 : INFO : EPOCH 9 - PROGRESS: at 28.43% examples, 389858 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:10,288 : INFO : EPOCH 9 - PROGRESS: at 30.34% examples, 390063 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:11,288 : INFO : EPOCH 9 - PROGRESS: at 32.21% examples, 390047 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:12,294 : INFO : EPOCH 9 - PROGRESS: at 34.11% examples, 390328 words/s, in_qsize 1, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:20:13,298 : INFO : EPOCH 9 - PROGRESS: at 36.02% examples, 390610 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:14,314 : INFO : EPOCH 9 - PROGRESS: at 37.92% examples, 390638 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:15,317 : INFO : EPOCH 9 - PROGRESS: at 39.82% examples, 390891 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:16,322 : INFO : EPOCH 9 - PROGRESS: at 41.68% examples, 390772 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:17,324 : INFO : EPOCH 9 - PROGRESS: at 43.58% examples, 391040 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:18,337 : INFO : EPOCH 9 - PROGRESS: at 45.49% examples, 391090 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:19,344 : INFO : EPOCH 9 - PROGRESS: at 47.39% examples, 391237 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:20,355 : INFO : EPOCH 9 - PROGRESS: at 49.28% examples, 391315 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:21,358 : INFO : EPOCH 9 - PROGRESS: at 51.00% examples, 391198 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:22,371 : INFO : EPOCH 9 - PROGRESS: at 52.57% examples, 390572 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:23,384 : INFO : EPOCH 9 - PROGRESS: at 54.16% examples, 390256 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:20:24,389 : INFO : EPOCH 9 - PROGRESS: at 55.73% examples, 389801 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:25,403 : INFO : EPOCH 9 - PROGRESS: at 57.32% examples, 389518 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:26,422 : INFO : EPOCH 9 - PROGRESS: at 58.91% examples, 389177 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:27,436 : INFO : EPOCH 9 - PROGRESS: at 60.44% examples, 388468 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:28,454 : INFO : EPOCH 9 - PROGRESS: at 62.04% examples, 388181 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:29,457 : INFO : EPOCH 9 - PROGRESS: at 63.60% examples, 387870 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:30,467 : INFO : EPOCH 9 - PROGRESS: at 65.17% examples, 387495 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:31,473 : INFO : EPOCH 9 - PROGRESS: at 66.76% examples, 387382 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:32,485 : INFO : EPOCH 9 - PROGRESS: at 68.29% examples, 386829 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:33,497 : INFO : EPOCH 9 - PROGRESS: at 69.88% examples, 386666 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:34,512 : INFO : EPOCH 9 - PROGRESS: at 71.47% examples, 386498 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:35,521 : INFO : EPOCH 9 - PROGRESS: at 73.06% examples, 386392 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:36,528 : INFO : EPOCH 9 - PROGRESS: at 74.56% examples, 385792 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:37,539 : INFO : EPOCH 9 - PROGRESS: at 76.14% examples, 385700 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:38,543 : INFO : EPOCH 9 - PROGRESS: at 77.71% examples, 385483 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:39,544 : INFO : EPOCH 9 - PROGRESS: at 79.27% examples, 385322 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:20:40,554 : INFO : EPOCH 9 - PROGRESS: at 80.86% examples, 385240 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:41,573 : INFO : EPOCH 9 - PROGRESS: at 82.45% examples, 385107 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:42,593 : INFO : EPOCH 9 - PROGRESS: at 84.04% examples, 384958 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:43,607 : INFO : EPOCH 9 - PROGRESS: at 85.64% examples, 384857 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:20:44,620 : INFO : EPOCH 9 - PROGRESS: at 87.23% examples, 384774 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:45,630 : INFO : EPOCH 9 - PROGRESS: at 88.82% examples, 384710 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:46,637 : INFO : EPOCH 9 - PROGRESS: at 90.39% examples, 384521 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:47,654 : INFO : EPOCH 9 - PROGRESS: at 91.97% examples, 384418 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:48,662 : INFO : EPOCH 9 - PROGRESS: at 93.56% examples, 384385 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:49,678 : INFO : EPOCH 9 - PROGRESS: at 95.15% examples, 384303 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:50,696 : INFO : EPOCH 9 - PROGRESS: at 96.73% examples, 384192 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:51,707 : INFO : EPOCH 9 - PROGRESS: at 98.29% examples, 384000 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:52,716 : INFO : EPOCH 9 - PROGRESS: at 99.85% examples, 383831 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:52,806 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 15:20:52,807 : INFO : EPOCH - 9 : training on 30133876 raw words (22519081 effective words) took 58.7s, 383849 effective words/s\n",
      "2019-12-11 15:20:53,822 : INFO : EPOCH 10 - PROGRESS: at 1.96% examples, 400119 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:54,823 : INFO : EPOCH 10 - PROGRESS: at 3.87% examples, 398585 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:55,824 : INFO : EPOCH 10 - PROGRESS: at 5.80% examples, 398027 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:20:56,841 : INFO : EPOCH 10 - PROGRESS: at 7.67% examples, 394488 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:57,853 : INFO : EPOCH 10 - PROGRESS: at 9.59% examples, 394076 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:58,855 : INFO : EPOCH 10 - PROGRESS: at 11.51% examples, 394457 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:20:59,858 : INFO : EPOCH 10 - PROGRESS: at 13.43% examples, 394693 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:00,863 : INFO : EPOCH 10 - PROGRESS: at 15.34% examples, 394825 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:21:01,875 : INFO : EPOCH 10 - PROGRESS: at 17.25% examples, 394564 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:02,877 : INFO : EPOCH 10 - PROGRESS: at 19.17% examples, 394749 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:03,880 : INFO : EPOCH 10 - PROGRESS: at 21.08% examples, 394902 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:04,883 : INFO : EPOCH 10 - PROGRESS: at 22.99% examples, 395029 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:05,901 : INFO : EPOCH 10 - PROGRESS: at 24.90% examples, 394707 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:06,917 : INFO : EPOCH 10 - PROGRESS: at 26.81% examples, 394449 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:07,921 : INFO : EPOCH 10 - PROGRESS: at 28.71% examples, 394534 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:08,926 : INFO : EPOCH 10 - PROGRESS: at 30.62% examples, 394572 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:09,937 : INFO : EPOCH 10 - PROGRESS: at 32.52% examples, 394459 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:10,944 : INFO : EPOCH 10 - PROGRESS: at 34.43% examples, 394469 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:11,947 : INFO : EPOCH 10 - PROGRESS: at 36.33% examples, 394540 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:12,949 : INFO : EPOCH 10 - PROGRESS: at 37.99% examples, 392066 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:13,951 : INFO : EPOCH 10 - PROGRESS: at 39.89% examples, 392293 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:14,956 : INFO : EPOCH 10 - PROGRESS: at 41.79% examples, 392437 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:15,965 : INFO : EPOCH 10 - PROGRESS: at 43.69% examples, 392506 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:16,977 : INFO : EPOCH 10 - PROGRESS: at 45.59% examples, 392532 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:17,982 : INFO : EPOCH 10 - PROGRESS: at 47.49% examples, 392640 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:18,984 : INFO : EPOCH 10 - PROGRESS: at 49.39% examples, 392791 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:19,988 : INFO : EPOCH 10 - PROGRESS: at 51.09% examples, 392620 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:20,999 : INFO : EPOCH 10 - PROGRESS: at 52.69% examples, 392244 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:22,007 : INFO : EPOCH 10 - PROGRESS: at 54.26% examples, 391665 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:23,013 : INFO : EPOCH 10 - PROGRESS: at 55.82% examples, 391155 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:24,023 : INFO : EPOCH 10 - PROGRESS: at 57.41% examples, 390864 words/s, in_qsize 1, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-11 15:21:25,023 : INFO : EPOCH 10 - PROGRESS: at 58.97% examples, 390475 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:26,028 : INFO : EPOCH 10 - PROGRESS: at 60.53% examples, 390074 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:27,035 : INFO : EPOCH 10 - PROGRESS: at 62.07% examples, 389431 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:28,052 : INFO : EPOCH 10 - PROGRESS: at 63.63% examples, 388929 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:29,069 : INFO : EPOCH 10 - PROGRESS: at 65.23% examples, 388658 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:30,087 : INFO : EPOCH 10 - PROGRESS: at 66.82% examples, 388387 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:31,106 : INFO : EPOCH 10 - PROGRESS: at 68.41% examples, 388134 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:21:32,114 : INFO : EPOCH 10 - PROGRESS: at 69.97% examples, 387799 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:21:33,120 : INFO : EPOCH 10 - PROGRESS: at 71.53% examples, 387501 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:21:34,127 : INFO : EPOCH 10 - PROGRESS: at 73.12% examples, 387395 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:35,145 : INFO : EPOCH 10 - PROGRESS: at 74.71% examples, 387197 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:36,161 : INFO : EPOCH 10 - PROGRESS: at 76.27% examples, 386865 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:37,164 : INFO : EPOCH 10 - PROGRESS: at 77.83% examples, 386634 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:38,180 : INFO : EPOCH 10 - PROGRESS: at 79.42% examples, 386479 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:39,192 : INFO : EPOCH 10 - PROGRESS: at 81.02% examples, 386351 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:40,211 : INFO : EPOCH 10 - PROGRESS: at 82.61% examples, 386195 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:41,225 : INFO : EPOCH 10 - PROGRESS: at 84.20% examples, 386069 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:42,230 : INFO : EPOCH 10 - PROGRESS: at 85.80% examples, 386010 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:43,238 : INFO : EPOCH 10 - PROGRESS: at 87.39% examples, 385936 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:44,250 : INFO : EPOCH 10 - PROGRESS: at 88.98% examples, 385826 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:45,262 : INFO : EPOCH 10 - PROGRESS: at 90.57% examples, 385739 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:46,279 : INFO : EPOCH 10 - PROGRESS: at 92.15% examples, 385616 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:47,290 : INFO : EPOCH 10 - PROGRESS: at 93.74% examples, 385529 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:21:48,304 : INFO : EPOCH 10 - PROGRESS: at 95.33% examples, 385445 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:49,309 : INFO : EPOCH 10 - PROGRESS: at 96.92% examples, 385407 words/s, in_qsize 1, out_qsize 0\n",
      "2019-12-11 15:21:50,321 : INFO : EPOCH 10 - PROGRESS: at 98.51% examples, 385323 words/s, in_qsize 2, out_qsize 0\n",
      "2019-12-11 15:21:51,267 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-12-11 15:21:51,268 : INFO : EPOCH - 10 : training on 30133876 raw words (22520504 effective words) took 58.5s, 385262 effective words/s\n",
      "2019-12-11 15:21:51,269 : INFO : training on a 301338760 raw words (225202072 effective words) took 665.4s, 338426 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(sentences=tweets_pos + tweets_neg, corpus_file=None, size=size, alpha=0.025, window=5,\n",
    "                          min_count=min_count, max_vocab_size=None, sample=0.001, seed=1, workers=1, min_alpha=0.0001, sg=0,\n",
    "                          hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, iter=epoch, null_word=0, trim_rule=None,\n",
    "                          sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeeding\n",
    "### Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = np.zeros((len(tweets_pos),size))\n",
    "for index, tokens in enumerate(tweets_pos):\n",
    "    vect = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    train_pos[index] = np.mean(vect, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove_pos = np.unique([x for x,y in np.argwhere(np.isnan(train_pos))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_pos_2 = np.delete(train_pos,index_to_remove_pos,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = np.zeros((len(tweets_neg),size))\n",
    "for index, tokens in enumerate(tweets_neg):\n",
    "    vect = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    train_neg[index] = np.mean(vect, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove_neg = np.unique([x for x,y in np.argwhere(np.isnan(train_neg))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_neg_2 = np.delete(train_neg,index_to_remove_neg,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Datasets/twitter-datasets/test_data.txt\")\n",
    "tweets_test = [line.split() for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "test = np.zeros((len(tweets_test),size))\n",
    "for index, tokens in enumerate(tweets_test):\n",
    "    vect = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    test[index] = np.mean(vect, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove_test = np.unique([x for x,y in np.argwhere(np.isnan(test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = np.delete(test,index_to_remove_test,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine\n",
    "Combine pos and neg to have full training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((train_pos_2,train_neg_2))\n",
    "y_pos = np.ones(train_pos_2.shape[0])\n",
    "y_neg = np.repeat(-1,train_neg_2.shape[0])\n",
    "Y = np.hstack((y_pos,y_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Word2vec_X',X)\n",
    "np.save('Word2vec_Y',Y)\n",
    "np.save('Word2vec_test',test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('Word2vec_X.npy')\n",
    "Y = np.load('Word2vec_Y.npy')\n",
    "test_2 = np.load('Word2vec_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Logistic Regression with Cross-validation so don't need to split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10e2, fit_intercept=True, intercept_scaling=1,\n",
    "                         class_weight=None, random_state=None, solver='sag', max_iter=100000, multi_class='ovr',\n",
    "                         verbose=0, warm_start=False, n_jobs=-1, l1_ratio=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logiCV = LogisticRegressionCV(Cs=5, fit_intercept=True, cv=4, dual=False, penalty='l2', scoring=None,\n",
    "                     solver='sag', tol=0.0001, max_iter=10000, class_weight=None, n_jobs=-1, verbose=0,\n",
    "                     refit=True, intercept_scaling=1.0, multi_class='ovr', random_state=None, l1_ratios=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logiCV.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Datasets/twitter-datasets/test_data.txt\")\n",
    "tweets = [line for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\\n',\n",
       " \"because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>\\n\",\n",
       " '\" just put casper in a box ! \" looved the battle ! #crakkbitch\\n',\n",
       " \"thanks sir > > don't trip lil mama ... just keep doin ya thang !\\n\",\n",
       " 'visiting my brother tmr is the bestest birthday gift eveerrr ! ! !\\n',\n",
       " 'yay ! ! #lifecompleted . tweet / facebook me to let me know please\\n',\n",
       " '#1dnextalbumtitle : feel for you / rollercoaster of life . song cocept : life , #yolo , becoming famous ? <3 14 #followmeplz ! <3 x15\\n',\n",
       " \"workin hard or hardly workin rt at hardee's with my future coworker <user>\\n\",\n",
       " \"i saw . i'll be replying in a bit .\\n\",\n",
       " 'this is were i belong\\n',\n",
       " 'anddd to cheer #nationals2013 ?\\n',\n",
       " 'we send an invitation to shop on-line ! here you will find everything you need - without leaving home ... <url>\\n',\n",
       " 'just woke up , finna go to church\\n',\n",
       " 'agreed ! 12 more days left tho\\n',\n",
       " 'monet with katemelo\\n',\n",
       " 'like dammm lexis u got a lot to say when ur on twitter lol\\n',\n",
       " 'grateful today for a dream fulfilled ! ! my heart is so full - first 3 completed tracks have arrived back from new york ! #yeslord !\\n',\n",
       " 'at home affairs shall do it later\\n',\n",
       " 'barca bout to beat real madrid on saturday doe\\n',\n",
       " 'a lot of parts of asia . especially rats that live in the country and live on grains . supposed to be quite tasty .\\n',\n",
       " \"i wasn't even sleeping . so shut cho ole go back to sleep lookin ass\\n\",\n",
       " 'i have the worlds best dad . <3\\n',\n",
       " 'ab jaeyay werna meeting khatam hi hojaeygi baaqi ki buttering baad may karaygay hum sab\\n',\n",
       " 'no one doubts that ability !\\n',\n",
       " 'check my tweet pic out . that was the outfit before . this is it after\\n',\n",
       " \"just got my mid-term and i'm impressed ! ! #happy\\n\",\n",
       " 'my summer days : 1 . ) work from 10:30- 2:30 ish .. 2 . ) home , shower , & eat ... 3 . ) go out till whenever and do it all over again\\n',\n",
       " 'lol nooo .. food is ur friend\\n',\n",
       " '#16millionbritneyfan rt and tweet\\n',\n",
       " \"but seriously though .. it's called vanity fairest\\n\",\n",
       " \"lol chloe :') ill teach yous to cook , you'll be pros by the end of it !\\n\",\n",
       " 'lol , im finna eat something ... chicken\\n',\n",
       " \"any questions for me ? just put preston down for a short nap and mikes not home for another hour , feel like answer q's ! ! make ' em original\\n\",\n",
       " 'friday is payday & 4/20 ? ? hell yeah #reasons2dothebirdmanhandrub\\n',\n",
       " 'thank you again love & & i def will get that done ) i have plenty i want framed lols\\n',\n",
       " \"i'm very week thanksss ! what you up to ? xxx\\n\",\n",
       " \"#ff to my good , hilarious , sweet and kind friend , who loves toilet paper , he's always made me laugh when i needed to ! <3\\n\",\n",
       " 'apparently not )\\n',\n",
       " 'i live my life on the quote \" live life for the moment cos everything else is uncertain \" - louis tomlinson\\n',\n",
       " \"i can't wait to see it can you please notice or follow me ?\\n\",\n",
       " \"but they couldn't be bigger than ' s ones\\n\",\n",
       " '#yougetmajorpointsif you are olivia brown\\n',\n",
       " \"ty as well ! i don't know what i would do without you too my sister in christ ! you are truly a blessing to the kingdom !\\n\",\n",
       " 'i love southall travel , and before i visit southall i visit <url> first\\n',\n",
       " 'we care a lot for the ones you love ... good morning <url> share it with your friends . <url>\\n',\n",
       " 'i love you ! if you love me to please rt\\n',\n",
       " 'are you a blonde yet ?\\n',\n",
       " \"wait my brother has one ! let's get a picture\\n\",\n",
       " \"if i dont do it first thing i won't do it ! plus i beat the bridge traffic\\n\",\n",
       " 'japan has some of the nicest rivers ! <url>\\n',\n",
       " 'oh hi please follow me\\n',\n",
       " 'my new drawing based on just waiting for knowing her fav color <url>\\n',\n",
       " \"i haven't seen myself though and they didn't ask which is a bit annoying :p haha , ooo really ? why ?\\n\",\n",
       " 'i just need 17 more followers ! please follow to get me to 500 followers before saturday and to support ben kelly #teamjessie #thevoiceuk\\n',\n",
       " 'i want a dream like thatd ; hahah yolo . xx\\n',\n",
       " \"well mr . schmidt / knight , aren't you the charmer ? you just made my day five gazillion times better . love you !\\n\",\n",
       " \"no probs hope it's all going well !\\n\",\n",
       " 'watching now hehe ) )\\n',\n",
       " 'off to shower then bed , night night bbs\\n',\n",
       " \"be happy in front of people who don't like you , it kills them .\\n\",\n",
       " 'me and martina are gonna make a video to call me maybe with the boys\\n',\n",
       " 'laying with my little cuddle buddy tonight <url>\\n',\n",
       " \"aahh , sorry love ... but i know you'll ace them . )\\n\",\n",
       " 'how are #zuers holding up in the midst of all the midterms ? remember the secret to success is a good night sleep\\n',\n",
       " 'watching the first 48 , so is half my followers lol\\n',\n",
       " 'can i get a follow back from a fellow #twfanmily member please\\n',\n",
       " 'okay but next me you me are definitely hanging out\\n',\n",
       " 'i just figured out i have the same birthday as mike the situation from jersey shore .. but july 4th is a good day because i get fireworks\\n',\n",
       " 'dude i dident say one thing to her today ! ? ! and she starts trying to say shit ? k .\\n',\n",
       " \"can i get 3 more followers tonight :d i'm nice . you'll like my tweets . i'll try my best\\n\",\n",
       " 'yeah i was quite surprised too . even called the score .\\n',\n",
       " 'after school with my girls <user>\\n',\n",
       " 'hehe <3 well hello there\\n',\n",
       " \"i'll be sleeping there\\n\",\n",
       " 'i love that he thought of me and his sweet proud face when he handed them to me made me tear up ! xoxoxo\\n',\n",
       " '#smackkk like that\\n',\n",
       " 'when people judge & criticize you , just remember it says nothing about you & everything about them\\n',\n",
       " 'umm duh ! i told you ! u need to come to cracow\\n',\n",
       " 'aww , you are a very sweet kind . smurf . i love you too & talking with u lifts my spirits as well . <3\\n',\n",
       " 'give her that dope dick , till she can feel it in her stomach , eyes rolling back & toes curling\\n',\n",
       " \"happy 4.20 for reals . i'm sparking in the name of .\\n\",\n",
       " \"i know ! can't believe it myself ! i'll send you a pic of the hall !\\n\",\n",
       " 'rt help us trend jaomikay and kathrynbernardoasmikay thankyou ! <user>\\n',\n",
       " 'i got 2 cold 40s in the fridge\\n',\n",
       " 'heres to the kids who hate giving speeches\\n',\n",
       " 'gunun sorusu where is my mind <url> via <user>\\n',\n",
       " \"when i'm old enough i'm going on long lost family\\n\",\n",
       " '.. glad ur ok and it sure is can almost c it nite nite\\n',\n",
       " 'meal + cinema with katie\\n',\n",
       " 'so i thought since your a bieber fan and there arnt many at our school i assumed that was your team\\n',\n",
       " \"smiles what's up chikaroo ? xx\\n\",\n",
       " 'i know i was in spanish\\n',\n",
       " \"it's once every 24 hours\\n\",\n",
       " \"follow cos she's a fully sick dog\\n\",\n",
       " 'thanks for the super sexy gold show ladies ! have fun shopping !\\n',\n",
       " 'always a pleasure marie\\n',\n",
       " \"i'm not sure if they are really aware of twitter . a hyena on nitrous oxide might confuse them .\\n\",\n",
       " \"i dm'd you what to do get your followers to follow you on my your new twitter . where is this shindig ? x\\n\",\n",
       " \"i can't wait til she moves back to san antonio\\n\",\n",
       " 'so hate the weather . pwede bang summer na malamig ! ? haist \" i so love summer ! \"\\n',\n",
       " '9:19 my time and <user>\\n',\n",
       " 'we need to book our t4 on the beach tickets ! ! ! #excited\\n',\n",
       " 'oh wow ! ! is it like saturday over there ! ? sorry forgot about the time difference and omfg if 1d came in i would die ... xx\\n',\n",
       " 'black with a touch of blue <url>\\n',\n",
       " \"i wouldn't mind drinking these cause they're cute ! <url>\\n\",\n",
       " 'aight thanks sweetz rt yeah that 1\\n',\n",
       " 'adam west ? ? think we are forgetting\\n',\n",
       " 'this describes me this morning\\n',\n",
       " \"jessica makes me feel #foreveralone . but it's all good cause i know she'll be right next to me ... forever ... alone ;d aha\\n\",\n",
       " 'oh hahahah thanks\\n',\n",
       " 'try to find out where it is !\\n',\n",
       " 'u r supposed to be awake at this hour ! hahaha come and join me ! im on my #insommia & u have a #lifeofcallcenter )\\n',\n",
       " 'so ... happy birthday how old are you , swiftie ?\\n',\n",
       " \"can't wait for this trip to la\\n\",\n",
       " 'whatever peasant x\\n',\n",
       " \"that's embarrassing\\n\",\n",
       " 'i bet #dadforgottogetkerosine is trending worldwide after that tweet #stupidhashtag\\n',\n",
       " 'fckk both of yall okay , my titties big ! #dd <--- #wishfulthinking\\n',\n",
       " 'may all angels guard & guide ur way night & day\\n',\n",
       " 'lol jk jk kimmy ill try to get you some food wen i see you\\n',\n",
       " 'i am a muslim and i am proud to be one ! ! !\\n',\n",
       " 'rouninblogs : photo : stickers came in ! and doin work on a new shirt comin up soon #rouninapparel #rounin #design ... ...\\n',\n",
       " \"follow she's simply amazing and deserves more followers\\n\",\n",
       " 'kiss mhee real slow & get down & blow mhy mind\\n',\n",
       " 'lmao weird but true its crazy how i can always relate to the weird stuff tweet about ... smh lol\\n',\n",
       " 'zed ! msh aja ! ( rt this is lipbalm , not salep kutu air ... ... <url>\\n',\n",
       " 'i miss you nia ! and yes , hopefully ill see you at my grad love you !\\n',\n",
       " 'i wanna buy that \" stop wars \" t-shirt . bought one . we were supposed to wear it psychology together\\n',\n",
       " 'no problem babe :d thanks dear !\\n',\n",
       " 'subway date with #workflow\\n',\n",
       " 'omg that\\'s my shit ! .. rt \" making good love , , avant .. > most def is on the baby making playlist lol \"\\n',\n",
       " 'when i think of my future while closing my eyes , i can see you and me . together\\n',\n",
       " \"you're welcome ! and thanks for sharing ! looking forward to that . have a great day and stay fit !\\n\",\n",
       " '\" i am now a member of global 14 good \" .. ill b a member soon\\n',\n",
       " 'its all good blacky !\\n',\n",
       " \"thank you alison . i hope you're well + happy #ff to you too x\\n\",\n",
       " \"ooo this story is getting good can't wait for more hee hee xoxox\\n\",\n",
       " 'really ? wow thats good .. its a really nice article x\\n',\n",
       " 'writing poems about pycc #toolive chad saying \" too live \" > > > #realthug\\n',\n",
       " 'be a lady in the streets , but a freak in the sheets\\n',\n",
       " 'are you getting that tattoo ? ? <3\\n',\n",
       " 'talks with emmanuel / mayo whewww !\\n',\n",
       " '#thingsrkellysingsabout get ready fa that real gold shower\\n',\n",
       " \"under my covers there's a new adventure every time\\n\",\n",
       " 'laynn in the briefs\\n',\n",
       " 'follow me on instagram : <user>\\n',\n",
       " 'all the above !\\n',\n",
       " \"i'm proud to say that i never smoked in my life and i never will .\\n\",\n",
       " 'that i can bring home to my mom\\n',\n",
       " 'got my magazines in the mail today ! girls , you can relate . <url>\\n',\n",
       " 'when i think of his name all i can do is smile\\n',\n",
       " 'i see imma have to get a bitch prom taken away ...\\n',\n",
       " 'either way you win . get it girl\\n',\n",
       " \"are you all righthaha ! you're so excited , right of course , we can't wait , too . ) )\\n\",\n",
       " \"lol i'm still reassuringly trotskyist 95 % of the time\\n\",\n",
       " 'treats from an ed client who has been baking ! woohoo <url>\\n',\n",
       " 'no she will cuz its me\\n',\n",
       " '#waystobeginsex talking hella shit guy tell girl to shut up she says make me he says i will with this d * ck she talks more shit ...\\n',\n",
       " \"thanks so much bay , i love you too ! ! can't wait to see you ! !\\n\",\n",
       " 'want to text\\n',\n",
       " 'could you please follow me ? ? if you do i will dance around like an idiot , like i did when luke did ! please <3 #28\\n',\n",
       " \"please follow you'll love her ! !\\n\",\n",
       " '\" no one does me better than me lool \" is this banter ?\\n',\n",
       " 'lmao ! that made my day ! rt i got board so i thought i would practice my awesome dance moves lol <url>\\n',\n",
       " '... #bap site is sick . slide thru & check it out ! ! 1love\\n',\n",
       " 'is having lyrics jus need to think if i deffo want these certain lyrics\\n',\n",
       " ': haha .. its ok . i was just kidding . btw , am too a banglorean\\n',\n",
       " \"many thanks t ! i am very appreciative sorry it woke you ! how's eddy ? ?\\n\",\n",
       " \"i am hold your excitement i know there's a lot haha\\n\",\n",
       " 'when a girl can take jokes , make you laugh , and make you feel like a king ; then shes meant to be your queen ! <3\\n',\n",
       " 'well , thank you ! ! !\\n',\n",
       " '- smh .. ima still eat this icecream . double strawberry .\\n',\n",
       " 'well it happens alot , so alot of picture opportunity\\n',\n",
       " 'true , but bacon infused bourbon and a little maple syrup make it real tasty .\\n',\n",
       " 'how many mexicans does it take to change a lightbulb ? just juan #joketweet\\n',\n",
       " 'anyways , hope everybody passed their grad exams\\n',\n",
       " 'about to be home alonee ! #yess\\n',\n",
       " '#yes i guess it is #patient that pays off in the long run #imintoyou\\n',\n",
       " 'lol coz the songs called \" love love love \"\\n',\n",
       " 'haha it hasnt blown up like this in a while , so thanks for that lol\\n',\n",
       " \"happy birthday i love you with all my heart and stay army strong ! ! ! can't wait to see you again\\n\",\n",
       " 'watching freedom writers in advisory\\n',\n",
       " 'heres the official video for our song when the lights die ! we hope you like it <url>\\n',\n",
       " 'can u please ask btr if they already knew their albanian fans ?\\n',\n",
       " 'love you both , but you guys are real jackasses sometimes\\n',\n",
       " 'lmao ! idk fave but i got three shades of red for ya if you wanna freak it out\\n',\n",
       " \"lol he's inlove . ahaha . and you know you like it .\\n\",\n",
       " \"babe is home from work ! time to cuddle & watch ' 30 rock '\\n\",\n",
       " \":p i don't call anybody . some kinda bug with me . btw ritika called today . <user>\\n\",\n",
       " \"don't listen to haters they just bring you down ! pay no mind to the wannabeees i got your back girl ! ! ! snooks for life\\n\",\n",
       " \"doesn't mean you're allowed to use it\\n\",\n",
       " 'i promise with you and god\\n',\n",
       " 'the host of american band stand . he started a lot of careers on his show .\\n',\n",
       " \"yep got them here , cant wait now awww im sure they'll come soon i dont get why they are here already , its far too early x\\n\",\n",
       " 'nicki minaj , starships love that song <3 ( live on <url>\\n',\n",
       " 'i agree with everything you said about sam i just wanted in on the joke daily * hugs * coming your way time to dm ?\\n',\n",
       " \"hell yeah . ! ! ! wish we could hotbox together that'd be cool haha take a coupla hits for me'll do u !\\n\",\n",
       " 'goood keep it free for paddy please xx\\n',\n",
       " \"oh shit that just fucking sucks ! at least we're watching despicable me\\n\",\n",
       " \"i'll be there soon . at least i'll see you at reunion .\\n\",\n",
       " '#1dwebstersurfboards ill join in ! i just need to learn to surf .. xx .\\n',\n",
       " 'now that you left the band , you will continue playing music ? who knows ? ?\\n',\n",
       " '\" avi > > > you look like an anime character or something \" thank you big brother <3 luh yewww hoe !\\n',\n",
       " 'off to seaworld , queensland\\n',\n",
       " 'might get my belly button pierced soon on my birthday yeahhh .\\n',\n",
       " 'it was indeed and hahaha thanks you are joking right ? :p and yeah i should to match ;d\\n',\n",
       " '\" you\\'re beyond that boo . [: \" you are gorgeous\\n',\n",
       " 'night cutie im with you on the head part i never get any after my girl turns in so i feel ya later\\n',\n",
       " 'could i be one of those girls you notice ? been trying ages !\\n',\n",
       " \"with that gorgeous face , i don't know your name , but it ain't important babe cause i'll just call you mine #wale\\n\",\n",
       " 'okay sounds good ! ! text me and let me know ! even if you could come for an hour that would be great to ! just wanna see yah ! !\\n',\n",
       " 'said this song reminds her of me . long time ago , but i still love the song <3 <url>\\n',\n",
       " 'working term time means you get to see trashy daytime tele\\n',\n",
       " \"hahahaha best practice today hahaha love youu you had me literally on the floor laughing today ! i'll see you tomorrow ! !\\n\",\n",
       " 'me too , should be a great day\\n',\n",
       " 'karma will soon find u . play your game well love . good luck .\\n',\n",
       " \"yah ! going to watch tonight ! can't wait\\n\",\n",
       " 'thanks for whut ? ? nuthin , just a little many more inspiration\\n',\n",
       " \"follow me , it'll make me smile\\n\",\n",
       " 'ur not the only 1 dat dey head hangin get it ?\\n',\n",
       " 'haha you should take me with u\\n',\n",
       " 'thank you , i better be on best behavior then look forward to feeding you on friday !\\n',\n",
       " 'new to this follow meee\\n',\n",
       " 'lol u the fam pet\\n',\n",
       " 'it sure is ... however , no , i cannot take credit for that garret , but maybe i will start using some of my own !\\n',\n",
       " \"would i be able to have my ban lifted b / c i didn't do anything wrong ?\\n\",\n",
       " 'time for ... then sleeps ! ! ! #hittheroadjack\\n',\n",
       " 'my head aches when i get hungry . . . )\\n',\n",
       " 'it appears moot as u clearly have started drinking .\\n',\n",
       " \"it's not too late , but thank you\\n\",\n",
       " \"these new songs are sounding killer can't wait for you to all hear them ! awesome stuff coming up ...\\n\",\n",
       " 'i wanna dance on your body the way i shake it on staage\\n',\n",
       " 'right around the corner .. bring me a sammich\\n',\n",
       " 'only 3 weeks left ...\\n',\n",
       " \"oh here it's gonna be a good night yeah gonna go tomorrow and see if i can get one somewhere ..\\n\",\n",
       " 'ether way did u get yo ass beat ? lol bc u my prom date so i might have to hire a hitman hahah\\n',\n",
       " 'rest in peace dick clark , you always made new years a little bit better <3\\n',\n",
       " \"don't underestimate life but take it simply\\n\",\n",
       " 'thanks , but im about to sleep now . kidding ! ) you game for 5am ?\\n',\n",
       " \"film it across canada ! :d especially ottawa with lots of fans ! - p . s . i'd like to audition for the film\\n\",\n",
       " '#mybrother #playipad <url>\\n',\n",
       " \"i'm going to learn a deaf havana song\\n\",\n",
       " \"welp , im not doin ' it . and yes ; we're huge failures #wahh . lol\\n\",\n",
       " 'haha twitter fights are for pussies ! handle that shit ! ! lol\\n',\n",
       " 'me and singing in the car\\n',\n",
       " \"stop perving on their facebook then ! :p haha , jokes ! nooo , she was , she's a mess ! next you'll be saying you love glee ! god\\n\",\n",
       " \"hey gok . what's your fashion inspiration ?\\n\",\n",
       " 'which 1d guy is your perfect match ? mine is liam .. tweet me yours rt please <url>\\n',\n",
       " 'one class today , and spending the morning with my boyfriend > > > #winning .\\n',\n",
       " \"that's the only reason i got in\\n\",\n",
       " \"aw happy birthday lew ! hope you've had a fantastic day , time to grow up now methinks xxx\\n\",\n",
       " 'will be fun\\n',\n",
       " \"what up rage tweet gingey . don't worry be happy / turn that frown upside down\\n\",\n",
       " 'in a user conference ... at work ... all day = boooring ! !\\n',\n",
       " 'ima slide prolly later tonight ...\\n',\n",
       " \"half 5 i will colour it again for you when it's in better condition ! ! ! xx\\n\",\n",
       " 'ion mind tho cuz i live here otl lmao\\n',\n",
       " \"after a tiring day shopping with mom and i'm now preparing my things for the swimming tomorrow .\\n\",\n",
       " 'well i hope you do good on it\\n',\n",
       " 'about to make me something to eat btw the track meet was popping\\n',\n",
       " \"watching tv until i fall asleep , goodnight y'all\\n\",\n",
       " 'good morning ninie . have a nice day . .. segyero !\\n',\n",
       " 'oh i may bring holly up to the game then x\\n',\n",
       " 'i\\'m going to class ! rt \" free headphones outside the commons ! \" . get me some !\\n',\n",
       " \"nah , we're good rt banged out almost 3hours of revision with #kindalethal\\n\",\n",
       " 'thanks love !\\n',\n",
       " 'i wish had a tv show i could watch it all day long ! !\\n',\n",
       " \"i know we do ! ! i love your hair by the way , couldn't stay a normal colour for long could you hehe xxx\\n\",\n",
       " 'is a weirdo\\n',\n",
       " 'yaaa they had boothes down here on \" regents \" day . i have a purple pin on my jacket from them\\n',\n",
       " 'birthday night out / last night with the girls until september at <user>\\n',\n",
       " 'deff agree\\n',\n",
       " 'i love please follow me ?\\n',\n",
       " 'awesome ! thanks for the chegg love have a great day from all of us here <user>\\n',\n",
       " 'got my schedule all figured out .\\n',\n",
       " \"omg i'm so proud of this guy > > > ! ! ! congrats on becoming a theta nu nupe i'm sure you did your thang ! ! !\\n\",\n",
       " '\" \" long day , off to bed . \" night morning \" morning sharon\\n',\n",
       " 'ooo i would get people to follow you but cba right now\\n',\n",
       " 'pandora + pinterest my way to relax !\\n',\n",
       " \"lol . seriously . it's nothing\\n\",\n",
       " 'hello ? ive waited here for you\\n',\n",
       " 'like up b1tch3zz if you want a contest / challenge series ( <url>\\n',\n",
       " 'oh yeah you still talking that shxt ? ? ? hoe\\n',\n",
       " 'the boys are on their way to nz\\n',\n",
       " 'your two biggest fans are and jessica travaglini , you mean the world to them , please show your appreciation x\\n',\n",
       " \"* giggles * you know you done good when you get the block . i'm the owner of a jim utter block\\n\",\n",
       " 'needs to learn how to breath when she sings . i go with <user>\\n',\n",
       " 'my office any time charley\\n',\n",
       " 'tell her i said happy birthday as well ) )\\n',\n",
       " \"lmao , yea ! that's my babyyy what period y'all got together ?\\n\",\n",
       " 'oh yeaah , smell like what ? the dankitydank i just shmoked youu out with ? #fuckswithme\\n',\n",
       " 'lol what you doin for 4/20 ?\\n',\n",
       " 'omg ! thats going to be so cute !\\n',\n",
       " \"the text just sent me ! made me smile she's so sweet ! #loveeeyou\\n\",\n",
       " 'this makes me so happy #ziall <url>\\n',\n",
       " \"rt 9 days left ! tixbox at rkm fe unpad dipatiukur from 11 am - 5 pm and at oz radio bandung 10 am - 4 pm . don't miss it guys\\n\",\n",
       " \"don't forget to send the pictures to me , rachel & madison u\\n\",\n",
       " 'done cut off my nails .. thanks mama for cut it ..\\n',\n",
       " \"not all the year 9 ' s , i've been following you for ages\\n\",\n",
       " 'bby kiss me througt the phone\\n',\n",
       " 'so what we get druuunk .. so what we smoke weeed .. were just having fun .. its .. 444 ... tweeenty ! !\\n',\n",
       " \"niall got braces , can't wait to see him cause he's sucha wee cutey anyway aww one direction ... what am i like . . .\\n\",\n",
       " 'feel free to join hicks , you my nigga too )\\n',\n",
       " '#50liesiwastold - eating crust on bread will make your hair curl ! anyone else heard this ? ridiculous !\\n',\n",
       " 'hope my sister has a wonderful day\\n',\n",
       " 'haha hey there josh could you get that t-shirt down for me ? #no\\n',\n",
       " 'hey i would love to win signed copy of one direction dare to dream #iwantthe1dbook .\\n',\n",
       " \"jaden needs to tweet more i'm desperate ! ! !\\n\",\n",
       " 'i dont live in the uk , the usa , aus or nz . i live in samoa #hamoswant1dtour\\n',\n",
       " \"rt she's got great eyes plus great wardrobe\\n\",\n",
       " \"please follow me and my best friend it's means a lot for us xxx 19\\n\",\n",
       " \"it's about ed drewett and well he still hasn't got the popularity he deserves #asked\\n\",\n",
       " \"that word looks very close to andy biersack's name <url>\\n\",\n",
       " \"hahahah ! ) i-set naten please ! ) let's make paalam to coach marin mahehehehhe ) ) >:d <\\n\",\n",
       " 'why is it i knew you were asian oh yes the pic of you holding your adorable nephew was my clue )\\n',\n",
       " \"awe i'm starting to miss oomf\\n\",\n",
       " \"sam's love for mercedes is so cute ... and creepy . #glee\\n\",\n",
       " 'when boys are special enough to be your first time . ahahahha . on may te ) ) )\\n',\n",
       " 'girls with lip rings , tongue rings , nipple piercings , belly rings > > > #ifindthatattractive\\n',\n",
       " 'you keep them laying around or up your ass ? !\\n',\n",
       " 'congrats : claire , yves and alec are trending now .. ) #pbbteens4\\n',\n",
       " \"' oomf with braces look too good though , . . <user>\\n\",\n",
       " 'i avoid online dating sites . they match you up with people who share your interests and i dont want to go out with a weirdo\\n',\n",
       " 'yaya got the movie tickets for tomorrow night ) ) )\\n',\n",
       " 'are you working today , or are you just responding to tweets ? ? lol ! !\\n',\n",
       " 'okay really tho ... goodnight lol\\n',\n",
       " 'yeah , i should really consider that , probably life would be more interesting .\\n',\n",
       " 'white girlll wasttteddd\\n',\n",
       " 'hehe sorry lauren ! will buy something on ur behalf too x\\n',\n",
       " 'ooo i swear if i get the chance ima )\\n',\n",
       " 'i found the part he sings ! ! ! shoutout to me ? ( live on <url>\\n',\n",
       " 'what a beautiful day for 5 beautiful boys x\\n',\n",
       " \"thanks man i'm actually travelling to exeter to take part at the met office . always wanted to go there !\\n\",\n",
       " \"that'd be too cute to be on the kissing camera at one of the mavs or ranger games\\n\",\n",
       " 'my boo told me he loves me a lil bit today\\n',\n",
       " 'everything will be okay .\\n',\n",
       " \"you're all amazing , too amazing to write a song about\\n\",\n",
       " 'bitches never directly tried me . they know better haha\\n',\n",
       " 'so & i just got finished perfecting our prom moves ! haha #scotlandaintready\\n',\n",
       " 'oh yeah . try not to be jealous of that wild grandpa sweater .\\n',\n",
       " 'rt hei morning be nice day for me today .. have nice day today ... bismillah\\n',\n",
       " \"i know but id much rather save up and go see olly next year than go t4otb there's like noone that interests me atm x x\\n\",\n",
       " \"really tht's nice\\n\",\n",
       " \"they're in 4th & 5th grade & just finished 2 weeks of state testing .. they're pretty much done for the year .\\n\",\n",
       " \"make it through wednesday , then you're halfway !\\n\",\n",
       " 's / o to !\\n',\n",
       " 'you already know doeee\\n',\n",
       " \"sancha's mom wants the deee\\n\",\n",
       " \"only if you'll take my kindle wireless in part exchange ! !\\n\",\n",
       " \"training's done wooo adv . b's so sabaw )\\n\",\n",
       " \"i'm canadian , i always say please and thank you\\n\",\n",
       " 'that was joke btw , clealy loads of people own footballs ha\\n',\n",
       " 'before and after shots maybe\\n',\n",
       " 'urghh , delete your old account then\\n',\n",
       " 'thank fuck for that ! thought i was a good judge ! xx lol\\n',\n",
       " \"#unattractivethingsaboutme that i am a lazy texter but i will only reply to you if u're special to me\\n\",\n",
       " \"this is random , but we haven't talked in forever and i miss youuu\\n\",\n",
       " 'i really wish you looked like that\\n',\n",
       " \"rt if you can't wait for may 6 #goodluckcharlieseason3premiere\\n\",\n",
       " 'that steak - & baked potato is in there calling my name )\\n',\n",
       " \"- . - rt the reactions i'm getting from my last tweet about haitians >\\n\",\n",
       " 'anybody in my followers no anybody that needs a weekend #nanny or an evening #babysitter . #chiswick but willing to travel ? x\\n',\n",
       " 'cancer always have good sex horoscopes\\n',\n",
       " 'you ladies should come work up an appetite with a class before brunch #justsayin\\n',\n",
       " 'made me feel like a punk today lol he pushed me all in my head & face , bet that wont happen again lol\\n',\n",
       " \"first break finally - yosii time ! let's wow our customer\\n\",\n",
       " 'about to upload a picture\\n',\n",
       " \"nah , it's all about cornetto raspberry enigmas <3\\n\",\n",
       " 'uhm no lorans fake noone of the cyrus know her\\n',\n",
       " 'can us have a follow from you ? ! ? ! ) <3\\n',\n",
       " '\" no likes me , no one likes talking to me \" so not true ! ! i lovee you\\n',\n",
       " \"going to bed been a long day ! need my rest o zzz zzz night ' xo\\n\",\n",
       " 'as we go on , we remember . all the times we had together . and as our lives change , come whatever . we will still be , friends forever\\n',\n",
       " 'actually \" catch up lunch \" sounds great .. i hope we catch up some mutual feelings over that tho .. looking straight forward to it ! !\\n',\n",
       " \"what is liam payne's middlename ? xx\\n\",\n",
       " 'has no friends , he reads tho .. <url>\\n',\n",
       " 'try not to do anything to crazy\\n',\n",
       " 'im on my way well in the next hour\\n',\n",
       " 'hon , it sure will \" hehe , we nicheke tu ! ur day will come \" lol ... \" mulika mwizi \" style ... ehe ? ? \" <user>\\n',\n",
       " 'check out the latest photos from our visit to northern england . <url> york is our favorite city so far !\\n',\n",
       " 'mission #getthegoodguy , is now in progress\\n',\n",
       " 'hey have a nice day\\n',\n",
       " 'get the peanut butter or dark choc ones from an american food store , you wont regret it\\n',\n",
       " 'my daughter and team mates are getting ready for a badminton game ! competition is exciting\\n',\n",
       " \"ok , leto - i've been w / u 18yrs ! i've bought your movies / cd's / vyrted my ass off ! think i've earned a call ? do u want blood ? !\\n\",\n",
       " 'no school for me tomorrow or friday yummmp !\\n',\n",
       " '. she think its real this time\\n',\n",
       " '\" oh okay good . u too fly to be depressed \" thanks boo : - *\\n',\n",
       " \"whoa , that's a fine looking pussy !\\n\",\n",
       " \"haha i know but it's the best way to answer the question !\\n\",\n",
       " \"awww don't make me blush x x x it's alright ! keep the icon x x x\\n\",\n",
       " \"greeting from fashionista love , here's the update for today ! enjoy your shopping xoxo , nicole <url>\\n\",\n",
       " \"we'll be up all night !\\n\",\n",
       " 'daddys home its time to play\\n',\n",
       " \"jaja thanks for worry ... u'r a good friend\\n\",\n",
       " 'hey justin bieber can i have follow from you ? love you xoxo\\n',\n",
       " \"you have a good point , thank you you're a monster ?\\n\",\n",
       " 'you can look after it too\\n',\n",
       " 'he is a very big fa kath rt can you follow me pls pls pls , , , imm a bigg fan ! ! ! :d\\n',\n",
       " 'hahahah how ironic . thank you very much\\n',\n",
       " \"heard you're a player ? nice to meet you i'm the coach .\\n\",\n",
       " \"i'll make you a dinner that'll make you stay in the bathroom for a while\\n\",\n",
       " \"- ; bitches ain't shit & they ain't saying nothin : * a ` hunnit mothafuckazz can't tell me nothing \\\\ \\\\ * i beez in trap (\\n\",\n",
       " 'bigger & better things to come\\n',\n",
       " \"and i'm just gonna say , this is great <url>\\n\",\n",
       " '3 of me <url> i love the crosseyed one .\\n',\n",
       " 'nothing i changed my twitcon tho after your constructive criticism , lol\\n',\n",
       " \"rt goodnight to ... i'm so proud of you , you have no cluuue , on one deserves it more then you\\n\",\n",
       " \"ast , come back for more bubblin i'm dublin during the summer ?\\n\",\n",
       " 'do you want to make two #familymenders happy ? then press mine and follow button . we ask very nicely\\n',\n",
       " \"he's worth the wait love propa great shagger\\n\",\n",
       " 'thanks for following your online shopping partner .\\n',\n",
       " 'everyone is entitled to an opinion and i respect that . but , those of you still trashing jeff long can die in a fire . have a good night .\\n',\n",
       " \"wooo 4.20 real pot heads don't wait for a holiday , just saying\\n\",\n",
       " \"i'll be your drunk on a friday night representative\\n\",\n",
       " 'thank you ! ! ! same here !\\n',\n",
       " 'got the new today , love the cover guys x\\n',\n",
       " 'things that people say\\n',\n",
       " ':d :d :d :d :d wish jocelyn had a twitter . #kudos for her too .\\n',\n",
       " 'i think you should tweet this fab doll she honestly loves you sooo much ! please koko ! 1 .\\n',\n",
       " \"hey , can u help my group ? we're doing a video campaign .. we need you to view this link -> <url>\\n\",\n",
       " \"it's so fun ! ! ! go with us soon okay !\\n\",\n",
       " 'yep gonna watch it now\\n',\n",
       " '\" teeth and smile > > #beautiful \" aww your so sweet\\n',\n",
       " '@ thedrybar follow me , please\\n',\n",
       " 'me , liam and harry are following #everyone who follows rt if u did it\\n',\n",
       " \"#aquarians usually isn't tired or lazy . they just like to sleep this describes me perfectly <user>\\n\",\n",
       " 'my favorite thing in the world , is cracking my ankles\\n',\n",
       " 'eating that breakfast , starting a new diet that i learned yesterday !\\n',\n",
       " 'oops i read that wrong )\\n',\n",
       " \"ever looked at your friends & thought , why the heck aren't we comedians yet ? you knowww\\n\",\n",
       " \"cooled it with my bestfriend , i ain't see her in a grip . she been in the cut\\n\",\n",
       " 'lol neither on mine\\n',\n",
       " 'us . look at her lips they blue <url>\\n',\n",
       " 'normal is one of the few terms never used to describe me #imlimitededition\\n',\n",
       " 'really ? i thought you were already in the kitchen ...\\n',\n",
       " \"ok everyone's talking about pbb and we're about fangirling . how's that for making a change ? )\\n\",\n",
       " 'do you enjoy the white small car \" buy by \" albertooo for last chrimas <url> i know i\\'m a believer\\n',\n",
       " \"retweet if you was born in the 90 ' s ! #90's babies\\n\",\n",
       " 'when ike something i remember everything\\n',\n",
       " '#thoughtsduringschool my teacher is so hot ... <user>\\n',\n",
       " 'playing with bella through the door . <url>\\n',\n",
       " 'why fm ? mm diba ?\\n',\n",
       " \"i'm a tshirt hoarder . i love them . but the i won't turn down occasional sweats\\n\",\n",
       " 'she needs to borrow cobeys movies his favorite is depicable me ! i like it too and a part 2 is coming out this summer too !\\n',\n",
       " 'my dinner tho >\\n',\n",
       " 'rt humor : estimating the project size <url> #projectmanagement #joke #consultant\\n',\n",
       " 'play football tonight , up early take my u9 to their match back home out for run , take it from there , maybe sweat a bet in\\n',\n",
       " 'haha hell yeah when ?\\n',\n",
       " 'i have to study now , but please follow me . so i can concentrate ! <3\\n',\n",
       " 'day is going good less than an hr of wk to go\\n',\n",
       " 'listening to a7x and looking at athames online . : <url>\\n',\n",
       " 'of course ! do you have the coffee packed ? <user>\\n',\n",
       " \"ditto . safe travels , love ! rt how are you guys ? i'm thinking about you . hope you all got to smile at least once today\\n\",\n",
       " 'deep rt trying to come down to ur level rt why do u have to be an asshole\\n',\n",
       " \"what's the source of happiness ? or who ... , ?\\n\",\n",
       " 'awhh sucks ! ! ! wanna come home with me and go with me to get my nails done ? ?\\n',\n",
       " '. a . p ni oka oopu opesthunayi gs songs ! epudu review yenti malli.hehehe pepople will laugh at u ! ! hahaha rofl\\n',\n",
       " '23 minute more to fall in love #fridayiminlove <user>\\n',\n",
       " 'haha , he just called me attractive\\n',\n",
       " 'retweet this if you like it , please <url>\\n',\n",
       " 'good for you river , start dropping gold coins in there , dude ! one of your many gay fans <url>\\n',\n",
       " 'i wanna do it in the backseat , you on top of me grinding slowly too the rhythm of the beat\\n',\n",
       " '\" happy #thongthursday tweetybirds <url> yummmy\\n',\n",
       " \"salamat sa updates sec . , you're the best )\\n\",\n",
       " \"who are these new people following me who i don't know .. ?\\n\",\n",
       " 'the next time the boys are in australia niall will be celebrating his 20th birthday ! ( sept . 13th )\\n',\n",
       " \"have an amazing show tonight ! ! i'll be in the audience . hopefully i'll see you afterward sgtc until i die )\\n\",\n",
       " \"i'm glad this day is ending\\n\",\n",
       " \"i swear i'm always retweeting , her tweets are juss too funny or too real\\n\",\n",
       " '- - - the type of girl to change your life\\n',\n",
       " 'queven has the cutest butt\\n',\n",
       " 'amari and ma lil brotha ! <url>\\n',\n",
       " \"beautiful jordan flip flops . they're so small and cute <url>\\n\",\n",
       " 'you can have the one my dog uses .. hahah <url>\\n',\n",
       " 'bwhahahahha you know that was funny\\n',\n",
       " 'job interview though , wish me luck * fingerscrossed * good luck lauri\\n',\n",
       " 'getting a \" hey beautiful \" > > >\\n',\n",
       " \"hi it's my b-day 2day & all i want is 2 get followed by u . x 142\\n\",\n",
       " 'buying a half today ! yay me ! ! !\\n',\n",
       " \"better not make the wrong decision today , or i'll be very upset #reallythough\\n\",\n",
       " \"like what ? rt lmao ! oh don't worry i have ideas ...\\n\",\n",
       " 'everyone follow <user>\\n',\n",
       " \"blerg . 358 this time . i think i need a longer break now . it's hard to get back into the swing of things .\\n\",\n",
       " 'lmmfaoo , i aint think yu was ready for that . bt now that i kno i got chu\\n',\n",
       " 'made thus for you guys and early birthday wishes ! ! )\\n',\n",
       " 'watching never back down in classs\\n',\n",
       " 'you would like it ! the students choose a focal question and research the topic throughout the semester .\\n',\n",
       " \"' turned on webbie radio & & ' whats happenin came on . ayye . ! !\\n\",\n",
       " 'text i just got lol : today is the 7,665 th day of your life , in other words your 21st birthday i ( cont ) <url>\\n',\n",
       " 'he say he will kill somebody ova me !\\n',\n",
       " 'kicks of the day . <url>\\n',\n",
       " 'its the weekend ! ! s coming home anddd s baby shower , exciteddd\\n',\n",
       " 'chelsea played a blinder last night . fingers crossed for the second leg\\n',\n",
       " 'rt shout out to <user>\\n',\n",
       " 'get back up : <url> are you trapped by past failures ? get free with this one #aheartforgod\\n',\n",
       " \"i'm trying to take a nap but i can't while spongbob is on .\\n\",\n",
       " 'have a good time - i know you will\\n',\n",
       " 'until i am in melbourne and available for coffee dates ( or sharing a coke at the mall\\n',\n",
       " 'lol , ima ass whole i like fucking with her scary ass !\\n',\n",
       " \"#youaintnogoodif all you want is to get in my pants . sorrry , but it ain't workin '\\n\",\n",
       " 'i had a boy tell me that the fact that i bump rap & shit all the time was the \" biggest turn on \" he\\'s ever had #rns\\n',\n",
       " 'come to my bed , my house ? xx\\n',\n",
       " 'hey , thanx for following me .\\n',\n",
       " 'when texted me but i was so high and drugged up i spelled shit wrong . she got what i was saying tho lmao\\n',\n",
       " 'hi love your songs good luck with all your future albums .\\n',\n",
       " \"don't you like your kraken cake ? ! this is to celebrate your homecoming ! <url>\\n\",\n",
       " \"she isn't , i am ! she's been served with a workplace asbo ...\\n\",\n",
       " 'ferrari-only highway in malaysia ! thanks to : antype photography : for making us smile <url>\\n',\n",
       " 'sleep well phil rt had a blast tonight with all the musicians on stage with me , hope ( cont ) <url>\\n',\n",
       " 'only 4 more followers until 830 ! please help and spread the love rt\\n',\n",
       " 'thx and * bow * to mrs . australia #ff\\n',\n",
       " '#oomf is actually pretty funny made my night alot better\\n',\n",
       " \"that's exactly y all u guys / girls miss out on sum 1 u could have been with in actually enjoyed be-n with . ... . #advicetewwhoevaneedsit . .\\n\",\n",
       " 'oh , dont get me started trying really hard not to encounter spoilers , felt bad watching the trailer\\n',\n",
       " \"ahh yes why can't the song just bloody happen aha x\\n\",\n",
       " 'nice ! i should prepare my #readathon post hopefully later between dinner and book group\\n',\n",
       " 'sa alesg private or public . ) di ko rin alam bakit 24 instead of 22 eh wala namang event dun ng 22 .\\n',\n",
       " \"anywho morning y'all\\n\",\n",
       " 'girl turn the lights out , i bet i make you tap out !\\n',\n",
       " 'if you hurt me , please know that i always got someone lined up ready to catch me when i fall . im too bad for that #obb\\n',\n",
       " 'if you can ? we have 11 atm so everyone will play hole game x\\n',\n",
       " 'i like him in any hairstyle\\n',\n",
       " '( for women ) ... you know what they say abt big shoes ... big uterus .. uhhh 0_o ? ? ... #somemovie\\n',\n",
       " '#fb girl ! ! hope all is well\\n',\n",
       " 'i love lookn the mirror nd looking good but i neva brag about how i look or what i got #conceited but not to the max like most\\n',\n",
       " 'rt nobody is greater than god ! ! #blessed ) ! ! !\\n',\n",
       " 'tell me why ! hahah .\\n',\n",
       " \"seee i got a problem & i don't know what to do cause ' ...\\n\",\n",
       " 'sooo rt on twitter from my laptop lol\\n',\n",
       " 'it seems to me , or never answered ukrainian ruschers\\n',\n",
       " 'can u please give me the link to 1.2 & 3 ?\\n',\n",
       " 'aww thanks hun . wayyy too nice\\n',\n",
       " 'someone pick me up , and take me to norwich on the 29th , so i can watch lost ! live ?\\n',\n",
       " 'that sucks ! & & good goods just living life day by day ... hby ? ? ?\\n',\n",
       " 'before i met you i never knew what it was like ; to look at someone and smile for no reason\\n',\n",
       " 'everytime i go to the hair salon bat 2aked eny i have a really weird taste in hair oh well\\n',\n",
       " \"yeah for sure man well i think we'll be there a day extra so maybe can sort it then we'll check it sooner to the time\\n\",\n",
       " 'selling a coffee table.tv.couch.bed . need $ 195 by monday lol . #noitsnotfordrugsoranythingbad\\n',\n",
       " \"glad i don't go to freeman , just in case that nerd does blow it up\\n\",\n",
       " 'missing hurry up & get off already ! ! ! lolz\\n',\n",
       " \"it's gonna be a goood weekend\\n\",\n",
       " \"i'm leaving early\\n\",\n",
       " 'better than yesterday ! i can eat and drink now squirrel will be up and well in no time !\\n',\n",
       " 'just tryna text , who can i text on this lovely day\\n',\n",
       " 'nice use of word \" wise act \" ..\\n',\n",
       " 'ah glad to hear it . we must do something soon ! x\\n',\n",
       " 'yeah you should and aha yeahs she was like mmm no not really ahaa :p\\n',\n",
       " \"when you know the width and length and depth and height of jesus ' love , you would not have to worry anymore ! seriously ! it's that great !\\n\",\n",
       " 'hey nialler , can you follow back me ? im so feel lucky if you followed me , please nialler thanks xx\\n',\n",
       " 'i was too high ) ) )\\n',\n",
       " 'is a hot teacher (\\n',\n",
       " '* grinds against you , slowly stripping down * hey baby .\\n',\n",
       " \"i heard you doin you , and you heard i'm doin better\\n\",\n",
       " \"it's actually on itunes now ! u can download it ! ! ask vern or alex alll about it lol = = = > <user>\\n\",\n",
       " 'wearing a dress tomorrow even though i only have 2 classes with him lol\\n',\n",
       " \"guess who's birthday is ... now ? ? ! yours ! #happybirthday #airbag #applebutt\\n\",\n",
       " '#yougetmajorpointsif you rub my back without me asking\\n',\n",
       " 'finishing work early to get my hair done\\n',\n",
       " \"haha okay , that's good\\n\",\n",
       " 'hey there krisbian , follow back ?\\n',\n",
       " 'i hope likes the picture i took on her phone\\n',\n",
       " 'thanks , gary , will follow right back . )\\n',\n",
       " 'what made you wana be in a band ?\\n',\n",
       " 'pretty good , how about you ? xx\\n',\n",
       " 'haha okay what tanning lotion do you use ? i want to get it lol\\n',\n",
       " \"can't wait to go to brick lane next weekend ! spend the rest of my birthday money\\n\",\n",
       " 'tampa bound with da girls\\n',\n",
       " \"i'm gald we dont have school friday now i could sleep longer thursday night or not\\n\",\n",
       " 'thanks and only getting my maths and physics results\\n',\n",
       " 'girl you working wit some ass yea you bea yea make a nigga spend his cash yea his last yea )\\n',\n",
       " 'follow though * desirre * cdfu\\n',\n",
       " \"really ! did not know , cool & i'm watching a movie , eating dinner .\\n\",\n",
       " \"smh not down ! lol and yay inshallah ! that's awesome\\n\",\n",
       " 'late start tomorrow . oh thank god\\n',\n",
       " '#thoughtsduringschool my bed . and a cuddle buddy\\n',\n",
       " 'we sure do ... chicken caesar , feta or meditteranean ?\\n',\n",
       " 'oke thaankss\\n',\n",
       " 'just be safe\\n',\n",
       " ', thinking about cutting everybody off & starting fresh & i know exactly who i want\\n',\n",
       " \"ok so if you're mad at me , please take it out on me rather than on my car #thanks\\n\",\n",
       " 'my subtweets are for whoever they apply to . if you do what i subtweet about then subconsciously im talking to you\\n',\n",
       " \"haha ahww ) good ! i'm excited to see you too ! you know we act like we can only hang at watermelon bust ? #whatupwiththat\\n\",\n",
       " \"blessings from yorkshire pal '\\n\",\n",
       " 'hello i need help in something .. can u follow me back and then unfollow if u want ^ - ^\\n',\n",
       " 'ohhh ohh see if i get a chance to buy a drink later . heh\\n',\n",
       " \"no we'll train him and make his teeth blunt ...\\n\",\n",
       " 'hehe .. yes .. thats true thankyou cc\\n',\n",
       " 'check it out this is me riding the megaramp open last year new stuff for 2012 coming soon <url>\\n',\n",
       " \"i've been alright , hbu ? you go to hueneme right ? : o\\n\",\n",
       " \"we are excited for tonight's press launch for ! ! !\\n\",\n",
       " \"i showed it to mama and she was like oh my gah i'm gonna throw up haha but i think u look pretty sexy haha\\n\",\n",
       " 'obama is a #leo #teamleo\\n',\n",
       " 'good morning how was your show ?\\n',\n",
       " \"lmaoo rt i'll give you greens every time , #soyouknowitsreal\\n\",\n",
       " 'when you gonna go on that diet with me ! follow me ! gracias #newtothis\\n',\n",
       " \"me but i'm going to go due my hair\\n\",\n",
       " 'hi ! thanks for following ! )\\n',\n",
       " 'i love you so much ! can you please follow me ? <3\\n',\n",
       " 'i crackyou up rt lmafoo yoo said the blacker the berry the sweeter the juice & & you know #oomf juice sweet as a bitch ctfuu ! !\\n',\n",
       " 'me and ) ) #statestreetdiner <url>\\n',\n",
       " 'haha thanks gor following ive been wanting to say that .\\n',\n",
       " 'okay , then promote us please\\n',\n",
       " 'made the high school volleyball team\\n',\n",
       " 'thank you bestfriend <3 lol\\n',\n",
       " 'getting sexyy for tonight going out with my girls\\n',\n",
       " 'forever young i want to be , do you really want to live forever ... forever young\\n',\n",
       " 'hell yea can i after movies\\n',\n",
       " 'lol whats good tho\\n',\n",
       " '#iloveitwhen my mom takes me to get a frappe just for no reason\\n',\n",
       " \"they're probably already off .\\n\",\n",
       " 'american pie reunion , gone and cabin in the woods - i wna see them all\\n',\n",
       " 'so right after leaves the first thing says is why am i crying .. haha ( <url>\\n',\n",
       " 'i love being within your presence )\\n',\n",
       " 'i will though i have to so you have nothing to blackmail me with haha\\n',\n",
       " 'i hope blair didnt give you a full blown migraine ... i suggest a bout of dar williams to ease the pain #cantbearblair\\n',\n",
       " 'staying after school with thursday tho\\n',\n",
       " 'woke up to some reggae , classical rock and some new wave ( ahhh ) relaxation #thepolice #bobmarley #thedoors #rollingstones #jimmy cliff\\n',\n",
       " 'i laugh cause idont care anymore\\n',\n",
       " \"i'm about to go to sleep\\n\",\n",
       " 'ga mudheng , just tell u the truth\\n',\n",
       " \"haha , everythime when it's 11:11 i'm wishing that i'll meet one direction someday\\n\",\n",
       " 'i feel more confident with myself\\n',\n",
       " 'uwi ba tayo sheyver ? do come .. kayo ni mama love <user>\\n',\n",
       " 'i have to get tickets this time ! wish me luck ! love youu ! <user>\\n',\n",
       " 'early morning blowing\\n',\n",
       " 'someone tell me why has gone beyond beautiful : s ... ? & can you take ah trip to london soon ?\\n',\n",
       " 'happy 1th monthsary and me love you guys especially anna my babyyy : * *\\n',\n",
       " 'what a beauuutiful day for nyc\\n',\n",
       " 'may napanuodan na po ba kayo ng purple house with eng subs ?\\n',\n",
       " \"would you believe i did eli's pigeon face ? haha . it looks the same , tbh . > ) ) )\\n\",\n",
       " 'ur welcome and no\\n',\n",
       " \"if i'm louder would you see me ? xx\\n\",\n",
       " 'ohhh going to the movies with my big bro tomorrow though\\n',\n",
       " 'maybe after i get back from china !\\n',\n",
       " \"both of y'all can kiss it ! y'all love me , & all the extra ! lolll\\n\",\n",
       " 'i think these pictures i drew deserve a follow <url>\\n',\n",
       " 'shower then text till my eyes close\\n',\n",
       " 'sleepover at mansion with rebecca , and maica\\n',\n",
       " 'mind to follback , tybfr\\n',\n",
       " \"i mean i'm about to go to the store .\\n\",\n",
       " \"everything i look at i see gods loving hands at work . the trees , the stars , all of it man ! it's like the classic tootsie roll commercial\\n\",\n",
       " \"just heard your new single on ! ! sounds fab , really catchy ! it's trending too #callmyname\\n\",\n",
       " \"thank god i don't have any homework ) )\\n\",\n",
       " \"i'm gonna have to try it one day sweet chilli sauce is one of the best !\\n\",\n",
       " 'was it scandalous ?\\n',\n",
       " 'i had just gotten home . tuesday night debauchery with the gays .\\n',\n",
       " 'wow twitter better watch out for people like you hehe\\n',\n",
       " 'lml that too !\\n',\n",
       " 'havin such a good night with the girls ! ! #hannahwaite #chanicekett #anniemurdoch #lucyyoung\\n',\n",
       " 'cause you were flllyyinnnggg ... and it was bumpy so i was like k .. lmfao but whatever i got over it !\\n',\n",
       " 'do it ! ! will be so so good ! once in a lifetime\\n',\n",
       " \"hmm that's cute .. my watch is being passed around des moines and posted on facebook\\n\",\n",
       " \"it's #boythudervisitsgyss visits ! ! trend it now !\\n\",\n",
       " 'thanks ! what do you use snagit for ?\\n',\n",
       " 'i love the video #boyfriend , i love , always stay strong and never say never justin te amo 28\\n',\n",
       " \"i suck at all the languages except for my own ! ahahaha i hope i'll learn some more one day . be a little more like you ...\\n\",\n",
       " 'rt \" thanx a lot \"\\n',\n",
       " 'ok gonna play a few throw backs while i have my midnight snack slave - just a touch of love : <url> via <user>\\n',\n",
       " 'you should do another twitcam soon !\\n',\n",
       " 'keep that hashtag together #rookiemistakes\\n',\n",
       " \"you can go volunteer as tribute , i'll stay home with gale\\n\",\n",
       " 'haha i know ! ! ! someone showed me that too\\n',\n",
       " \"just read something , and i've realised .. you know nothing ;] if confused text me baaahaaa ! i love you xxx\\n\",\n",
       " 'you are off to a roaring start !\\n',\n",
       " \"i'm sorely tempted to add owls in my room too\\n\",\n",
       " \"he kno it be a wrap when i'm riding it from the back\\n\",\n",
       " 'no fcat for me today thank god\\n',\n",
       " 'spent time with arlene\\n',\n",
       " 'omg ! ! ! just did mi hair ! ! ! im amazing ! ! : oxoxox to mi followers ! ! ! muaaa\\n',\n",
       " \"twanna hear a poem ? okay .. roses are red , violets are blue , you don't suck dick .. then bitch fuck you\\n\",\n",
       " 'hahaa thought youd be pleased . xxx\\n',\n",
       " 'remember to have your taxes in by midnight tonight ! ! !\\n',\n",
       " 'is one of the reasons i go on youtube and then to watch <user>\\n',\n",
       " \"rt good morning i don't wanna forget the presence is a gift so i say thank you\\n\",\n",
       " \"whatever time , i'm not bothered . i should be up\\n\",\n",
       " 'lol #knowthat\\n',\n",
       " 'spent #earthday in the mountains smoking my favorite plant with my bros . happy #420 , #stonernation .\\n',\n",
       " 'where can i sign up to be in your music videos .. i will do it for free haha xx\\n',\n",
       " \"ekk i'm excited too ! your going up saturday right ? !\\n\",\n",
       " 'how about them braves tonight ! 9-3 over the mets ! haaa !\\n',\n",
       " \"can't wait till one direction's concert next year ! bring on 2013 !\\n\",\n",
       " 'i love so much ! <3 i wanna start talking to her a lot more like i used too )\\n',\n",
       " '+ + breakfast = great start to the day\\n',\n",
       " 'taco bell for the second time today\\n',\n",
       " 'at least i got to skip my ugly classes today\\n',\n",
       " \"i'm good what about you ? ? ... omg i know . great to have on twitter as well xoxo\\n\",\n",
       " \"it's official ! ms . jean garcia promotes this twitter page . pls help us too to promote this . thanks ! <url>\\n\",\n",
       " 'oh whoops . flight is here . sorry thanks for being so cheap . you do that well\\n',\n",
       " '. follow them and gain\\n',\n",
       " 'anytime . us girls have to stick together\\n',\n",
       " \"- thx , and want you to know , i've bought your tunes on itunes .\\n\",\n",
       " 'oh well , everyone has different opinions .\\n',\n",
       " 'today when i am back home i went to see a year one direction in the making harry dont listen it . you have a fantastic voice\\n',\n",
       " 'getting our nails did with <user>\\n',\n",
       " \"i want guy that's from philly ha !\\n\",\n",
       " 'made me laugh . i must be a teacher .\\n',\n",
       " \"ohhh right right ! hahaha yes i can't wait for this xmas ! i know what to send everyone now )\\n\",\n",
       " \"5 ' 5 with brown eyes , a smile like the sunshineee\\n\",\n",
       " 'hmm i have to say i miss you bunches ! #iloveyouu and def cant wait to see you tonight #mytime\\n',\n",
       " \"you'll get the hang of it ! !\\n\",\n",
       " 'buat cewe 2 .. find the love who would fight for you and ur relationship . and when u find that person , fight for him . never give up\\n',\n",
       " '<url> even though justin has been to wales once before ( he was in bangor ) can you please help and sign this <3 thanks\\n',\n",
       " 'rt the heat game is on and you lookin for a twitter wifey ? rt who wants to be my twitter wifey \"\\n',\n",
       " 'thats crazy ! and yes we do and ill see if i cab find anything .\\n',\n",
       " 'i want to curl up in a ball and sleep forever .\\n',\n",
       " \"been writing songs like it's my job , but i have to admit , i don't think i can wait anymore . i want people to hear them .\\n\",\n",
       " 'cool , hopefully before my hol late august ... some new material to relax to on the beach\\n',\n",
       " '\" smh up nepa ! ! ! * looks around * erm ... oh ! electricity is bak :d \"\\n',\n",
       " 'thankyou whats a reserve and b reserve ?\\n',\n",
       " 'so nice walking into fye yesterday an asking for the cd\\n',\n",
       " \"you're a thuggg ! why you playin ? lol & i believe it , you seem too mean to approach .\\n\",\n",
       " \"hello miss cheryl come and get me . . . oh , sorry jen it's you . . . #tweetinglyricslykagangsta xxx\\n\",\n",
       " 'getting my first voxbox ! ! ! ( its a box of free make-up and goodies you are sent to review lol ) yayyy !\\n',\n",
       " 'these girls get mad cause i never want something serious i just think about you cause you made me not want to go back to that . heartless\\n',\n",
       " 'shut up & just make sure my bae be careful\\n',\n",
       " 'a couple more hours and we willl be workin on our seeexxy ass tan lines #cantwait oh yeah #its420 #yay\\n',\n",
       " 'this is a great day ! #finallyyouadmitit\\n',\n",
       " \"tml going ikea shop for racks plus i'm going to look for some decor or furnitures\\n\",\n",
       " 'kiss you on your neck and tell you everything is great\\n',\n",
       " 'no bother , youll get the grades x\\n',\n",
       " 'haha ... mornin though rt bana ! sijazoea android bado #teamdabr\\n',\n",
       " 'please follow me ! i have been trying for so long\\n',\n",
       " 'continue going with him , being his little dog and following him everywhere yeah ?\\n',\n",
       " 'yes im abt to b a mommy ...\\n',\n",
       " 'lol how many tennis courts do you need ?\\n',\n",
       " 'i was so close to tweeting that ... glad i didnt now\\n',\n",
       " \"i'd go to the doctor for that cough , mate\\n\",\n",
       " \"you are strong and you don't let other people influence you or get you down\\n\",\n",
       " 'i forgive you this time but never again and ye\\n',\n",
       " 'anytimeee #bfftweet\\n',\n",
       " \"download #hookt it's a better way for us to message one another <url> my hookt id is dmy 3bp3\\n\",\n",
       " 'oh 2nd semester . ^ ^ first year in the university .\\n',\n",
       " \"oh good cos i've run out of bread ! !\\n\",\n",
       " \"he's a father-like hyung .. i like being around him .. i learn many things from him\\n\",\n",
       " \"dont complain , only action you'll get for a while\\n\",\n",
       " 'i love to see you jealous\\n',\n",
       " 'one of our teams got 2nd place in mud tug ! ! congrats ladies !\\n',\n",
       " 'hey ! stranger over there im really liking the way , you whip it whip it ! i really want you now your so fine so fine\\n',\n",
       " 'definitely i love going to the echo to see shows !\\n',\n",
       " 'playing with my new watercolor paint pens <url>\\n',\n",
       " 'when we all know that misses mr w\\n',\n",
       " 'her daily sentence : \" i hate my french teacher . \" lmao <3\\n',\n",
       " 'congrats on the prevailing of common sense\\n',\n",
       " 'thanks & i tried my hardest . but she pushed me .. lol thts old shit tho , im over it\\n',\n",
       " \"ok ! i'm starting to love one direction .\\n\",\n",
       " 'i cant sleep much cuz i must get up at 6.30 am everyday when i have school : l ... have fun ther and say them hi from germany\\n',\n",
       " 'it will grow one day\\n',\n",
       " \"ik u wont see this tweet but i always tweet you everyday hoping you'll replay ! my bday is in a week ! happy bday to me ?\\n\",\n",
       " '#youcangetmajorpointsif you send me random cute texts that will keep me smiling all day long\\n',\n",
       " 'tried to do the #philippines #flag . #nails #yellow #red #blue #hard #candy #filipino #pinay <url>\\n',\n",
       " 'had a good nap probably finna go bk to sleep\\n',\n",
       " 'you should watch this korean pop group ! they are really something ! <url>\\n',\n",
       " 'i think all of them have boyfriends too ... buhahaha . true that though\\n',\n",
       " 'now nah udh liat kan aku senyum :d rt \" when i see you smileee ... ~ ~ \\\\ m / \"\\n',\n",
       " \"maybe . but you just see 1 of the ' dot ' and you see 1000 of bp .\\n\",\n",
       " \"for sure ! can't wait for tomorrow !\\n\",\n",
       " 'nice haha ! have fun ! and i did and thank you ! !\\n',\n",
       " 'it did as it goes !\\n',\n",
       " \"let's just hope they taste good\\n\",\n",
       " 'awww ! so jealous right now ! haha ! what was it / she like ? ? hehe ! xxx\\n',\n",
       " 'i want to come back alreadyyy\\n',\n",
       " \"tom has blue eyes , katie has brown eyes , and suri was lucky enough to get her dad's same amazing eyes\\n\",\n",
       " 'we have a brother sister gang , and we intend on bullying our mom xd\\n',\n",
       " 'pshh u better and aw man i want my clam cakes\\n',\n",
       " \"and they've got cocktails to start they know us well #nabshow <user>\\n\",\n",
       " 'lol i was too ! shakinq , heart beatinq fast and all ! but it ainn baddd\\n',\n",
       " \"yes that's right so that's not too long to wait !\\n\",\n",
       " 'hey ! take me and steph out for a raspado ! ! ! haha\\n',\n",
       " \"i wouldn't hate luc if he leaked a sex tape xp ) imagine ... me too : 333\\n\",\n",
       " 'tweetiung from the harmonys ( lol\\n',\n",
       " '\" #mybiggestfearis losing my bestfriend we\\'ve lasted this long . this is forever\\n',\n",
       " 'was amaziiing ! his arms corrr sexaaay\\n',\n",
       " 'naaah , \" je vous aime \" is . ich habe euch lieb . is that right ?\\n',\n",
       " 'well jst the tweetn n the dark part\\n',\n",
       " \"dear anyone who's reading this . i hope you have a reason to smile today . #behappy\\n\",\n",
       " '10 minutes left to join the networking cruise ... its rockin\\n',\n",
       " 'good yeah all good thanks darling . yeah definitely , lemme know when your free and we will all have a catch up\\n',\n",
       " 'i have to many socks to chose from for wacky tack day ! which one should i wear ? #coolsocks <url>\\n',\n",
       " '#startyourdaywithasmile ) ) )\\n',\n",
       " 'me , diamond , michala , nd raced to 3rd hr we wass fooling !\\n',\n",
       " 'niggas be thinking they doing something by unfollowing you .. you just doing me a favor thanks ! !\\n',\n",
       " 'when people swear they know you . no uu know what ii want uu to know\\n',\n",
       " '#thoughtsduringschool i wish a was a hamster so i could just chill all day .. i wonder what its like to be a hamster ... must be cool\\n',\n",
       " 'i heard in the streets im the one you wantin\\n',\n",
       " 'when you just sit there and randomly laugh about something that happened\\n',\n",
       " \"wha ? ? ? pshhh pls you ain't bout that life\\n\",\n",
       " '\" would you mind \" hmmm\\n',\n",
       " 'ahhh , this is gunna be a good one\\n',\n",
       " 'oh thank youuu ! it took awhile to figure it out .\\n',\n",
       " 'thank you , i was very happy !\\n',\n",
       " 'it would be the proudest moment in my life\\n',\n",
       " 'pmsl na u go girl was a good game hun stop with them rolly pollys init lool x x\\n',\n",
       " 'got my call today need to finish this letter ! !\\n',\n",
       " \"i think you're both probably right , but i had to point out the counter-example because i'm a dick like that\\n\",\n",
       " \"nice one mate , i'll make sure to pop in for a pint when i'm in town then\\n\",\n",
       " 'maybe a hot shower will get me tired ?\\n',\n",
       " 'rt <url> the review check it out - - - jheeeze bars cuzzy * salute * ima send you that track on sunday\\n',\n",
       " 'louis bunjee jumping in nz ! ( credits to the owner ) - marie xx <url>\\n',\n",
       " 'it looks so good ! and josh is in it which automatically makes it amazing x\\n',\n",
       " '#primaryschoolmemories you cutting your ear open with a pair of scissors\\n',\n",
       " 'save a horse ride my scooter .. ladies any takers ?\\n',\n",
       " \"liam got rt'ed from some eggs hi to you eggs . <url>\\n\",\n",
       " \"ok ! so i started thinkin ' about what to getcha for ur birthday & i have come to the conclusion that i am a fucking genius ! !\\n\",\n",
       " 'sooner or later things will be the way you imagine them .\\n',\n",
       " 'k , thats better ! yah i will\\n',\n",
       " 'hi chachi ! keep safe . please follow back\\n',\n",
       " \"y'all should make a road trip & come get me on friday so i can join !\\n\",\n",
       " \"you'll be on the train home ... remember !\\n\",\n",
       " 'forever with you no matter what\\n',\n",
       " 'thank you boo , and love you too ... )\\n',\n",
       " 'thankyooou tooony i loooveyoou soosooo much !\\n',\n",
       " 'uhmm i guess ill go out for a drive #letshopeforthebest\\n',\n",
       " 'anything for my zebby poo ! ! !\\n',\n",
       " 'being drunk is so much better than not being drunk\\n',\n",
       " 'goodnight to my baby hope she had a wonderful day ! <3\\n',\n",
       " 'i just got home . goodnyt . opening naman bi tom so i need to rest na .\\n',\n",
       " 'lol yeah my old ish was getting old and this how i be feeling so why not\\n',\n",
       " \"s / o to girls who don't allow their hearts to be broken\\n\",\n",
       " 'sending this out 2morrow <url>\\n',\n",
       " 'hahahah you should have heard her yesterday ! she told me to sign up for christian mingle ...\\n',\n",
       " 'baby i got nothing to hide <user>\\n',\n",
       " 'just out enjoying some sun <url>\\n',\n",
       " 'looking at old pics of me & all i see is fayfe #cray <url>\\n',\n",
       " 'hey can you do me a huge favor and check out my friends song . maybe subscribe ? ) <url>\\n',\n",
       " 'thanks for the mention ! prolificwriters <url> much appreciated ! #jenny_meszaros\\n',\n",
       " 'man , it was a looonnng night ! ! part 2 next weekend\\n',\n",
       " 'still so fucking orange from last night . i look ridiculous < max faggot warehouse tomo ? poom poom flex\\n',\n",
       " 'hey gorgeous ! lol\\n',\n",
       " 'in thee bed looking at keepers creepers , with a bra & & leggings on eating animal crackers\\n',\n",
       " \"sure ! ! ! omg ! ! i can't wait ! ! so excitated ! ! #boyfriend ! when my idol follow me ? 18\\n\",\n",
       " \"rt rt rt rt #openfollow for kpopers ' all fandom ' flwrs kece * b :d openfollow\\n\",\n",
       " 'i could understand your comment with google translate , thanks a lot and it was a very happy time for me to see you all in seoul\\n',\n",
       " 'i\\'ll just have to wait for \" the payback years \"\\n',\n",
       " \"any advice ? it's my first time going !\\n\",\n",
       " 'now im on the phone with makeda i missed them !\\n',\n",
       " 'alright alright . time to close the laptop . goodnight nigguhs <3\\n',\n",
       " \"it's happy hour ... all day ... at sonic . i'm not complaining .. best day ever ? yeah . best start to my week of finals ? yesss ! ! !\\n\",\n",
       " 'but then robinho took his spot & doubled that with an amazing 40 + goal tally\\n',\n",
       " 'got some more snacks for my baby today . love you <3\\n',\n",
       " 'good morning ! ! thanks have a blessed weekend !\\n',\n",
       " \"i'm notorious for doing shit like this for trending topics :d #noproblem\\n\",\n",
       " '\" babe came over to comfort me she cute huh <url> ctfuuu\\n',\n",
       " 'thanks for the info\\n',\n",
       " 'just told my mommy how much i love and care for her\\n',\n",
       " \"even tho u wont see this .. im home sick , but other than that i'm great ! how are you today ? ur the sweetest btw ! much love !\\n\",\n",
       " 'my dad is always right #heknowsbest ! ! #ilovemydaddy #ourrelationship ! !\\n',\n",
       " 'nothing wrong . entirely a personal choice\\n',\n",
       " 'everyone follow she is really nice and has good chat\\n',\n",
       " 'hii ed come to australia <3 xx\\n',\n",
       " \"gratz's powerpoints + the stuff i actually understood in hilliard's class = an outstanding essay\\n\",\n",
       " \"#yougetmajorpointsif you're not too sappy ... ew ... even though says i'll die alone hahah\\n\",\n",
       " \"who's this hottie ? ? ) <url>\\n\",\n",
       " 'awwwe okay babyy byeee :\\') goodnight , sleep well , and sweet dreams \" dream about me and juju \" gonna miss yew more <3\\n',\n",
       " \"it seems too good to be true , but i don't want to wake up out of it .\\n\",\n",
       " 'echo cd is on repeat right now ! i love every song & voices of & <user>\\n',\n",
       " 'yeah just days before confi and you decide to cut ..\\n',\n",
       " 'you can get like norwich-london on the train for 8.50 depending on the date !\\n',\n",
       " 'we on the phone now\\n',\n",
       " 'lol okay thanks tyson\\n',\n",
       " 'well , thank you for the visit , glad you enjoyed\\n',\n",
       " 'size small please\\n',\n",
       " \"lovin ' my pics a little too much ! hahaha oh well ! <url>\\n\",\n",
       " 'waking up to the sound of birds chirping\\n',\n",
       " 'today was fkin dope . i was chef for the whole class leading niggas & telling em what to do .\\n',\n",
       " 'rt words of an adulterer ! lolrt baby u re finer than ur fine cousin ...\\n',\n",
       " '.. well , the sky is blue today ! .\\n',\n",
       " 'caught me chilln on skype this morning . good morning world <url>\\n',\n",
       " 'hi thank you very much for the rt !\\n',\n",
       " 'seriously what time though ? haha love you dude . turn that frown upside down x\\n',\n",
       " 'i saw your cute little face in the stands\\n',\n",
       " 'believe it boo ! what happened today was amazing ! and now we are seeing them in a few short hours xoxox\\n',\n",
       " \"lool dw they're nothing ignore my twitter complaints haha if they get worse i'll go doctors\\n\",\n",
       " 'yap , team from indonesia\\n',\n",
       " 'morning charlotte , you are up early . have a great day i am off playing golf later !\\n',\n",
       " \"i don't mind who's getting it\\n\",\n",
       " \"all the recent photo shoots i have had are now online at <url> - check ' em out ! <url>\\n\",\n",
       " '\" what\\'s up with you \" i really been busy , just trying to do the right thing\\n',\n",
       " 'do you like harry potter ? and hunger games ?\\n',\n",
       " 'thanks for the history thingy\\n',\n",
       " 'i should be mad , but for some reason im not\\n',\n",
       " 'a bit of both an erstwhile sports journalist !\\n',\n",
       " \"haha not for me ! and yeaaa ! ! ! and meh it's going good i guess getting amazing marks and shiz lol\\n\",\n",
       " \"im not proud to say this but please stop poisoning other people's mind . stop feeding them with your insecurities . stop it . stop it . night !\\n\",\n",
       " 'looking mighty fine in your new twitter icon ! x\\n',\n",
       " \"most economists agree that higher vat's r the way to go ( hst / gst 4 us ) , but try selling that to the double-double masses !\\n\",\n",
       " 'i love the bethlehem store\\n',\n",
       " 'i have to be honest , i could listen to them\\n',\n",
       " 'they will trust xxx\\n',\n",
       " 'good to know <3\\n',\n",
       " \"i know ! :/ hey did you know that demi is coming here ! she'll play in argentina !\\n\",\n",
       " \"it's my birthday in 14 days could i please get that follow from you as a gift ? <3 <3\\n\",\n",
       " 'did the bookbagboys win ?\\n',\n",
       " 'hi , can you follow me on please please , . kisses from france x3 2032112 2\\n',\n",
       " 'i finally get to see poetic justice , all at one time not just parts of it\\n',\n",
       " 'i can be the sweetest girl you have ever met , but when you get on my bad side all hell breaks loose\\n',\n",
       " 'good night to our lovliest hero xx\\n',\n",
       " \"i'm staying single for a while i need to focus on me instead of girls ! ! but i'm still gonna have fun #singlelife going ok for now\\n\",\n",
       " 'thanks for the rt entrenovias\\n',\n",
       " 'happy happy birthday to yer lovely brotha !\\n',\n",
       " 'rt getting gassed to in my tech class , my teacher is like wtf lool posh prick ! !\\n',\n",
       " \"wow , i actually think it's awesome !\\n\",\n",
       " 'hey there ! ! !\\n',\n",
       " \"shoutout to she's a directioner and follows back\\n\",\n",
       " 'lol . do you work tomorrow ? better get to bed woman !\\n',\n",
       " 'hahah ! only chayton would\\n',\n",
       " 'really ? lolol sounds like a top lad\\n',\n",
       " 'courteous of my lovely friend knows what to do when a girl feels down <url>\\n',\n",
       " \"mexico is now very late , so i'll go to sleep , i hope to hear from you when you wake up , wish me good night\\n\",\n",
       " 'the biggest day hye sweetheart <user>\\n',\n",
       " 'dont worry , we wont tell anyone ! our little secret ( live on <url>\\n',\n",
       " 'oh hahha yes must bond my classmates were bonded through orientation so we got very close hahha somehow jiayou ! ^ ^\\n',\n",
       " 'thanks #twitneighbours #ff\\n',\n",
       " 'to right mark well said dude\\n',\n",
       " 'i think those of us that work there would beg to differ hahah .\\n',\n",
       " \"god is good all the time . he's there for you in good times and he also stays with you during your bad times . ) )\\n\",\n",
       " 'its gonna be a good weekend\\n',\n",
       " ', now following back bestfriend ! )\\n',\n",
       " \"alrighttt whale wanker :p glad we didn't go jogging haha ! xx\\n\",\n",
       " 'lolol when people sub-tweet me youre cute af\\n',\n",
       " 'and star power rt kush & orange juice and burn after rolling are my favorite wiz khalifa mixtapes\\n',\n",
       " 'thank you ! i love you more <3 and my ppl love u too\\n',\n",
       " 'do i want it in the kitchen ? isaid hell yeah\\n',\n",
       " 'wanna meet me half way later bitch so i can have my jeans back ? ) )\\n',\n",
       " \"#bbcfootball feelings are chelsea are going through but don't know how ! ! !\\n\",\n",
       " 'thankss for the shoutout\\n',\n",
       " 'the scent of freshly cut grass\\n',\n",
       " \"never slow cooked pork chops before . i have done them over low heat on the skillet . crockpot i've done shoulder and butt\\n\",\n",
       " 'look up quotes on google\\n',\n",
       " 'the older i get , the more i see the power of that young woman , my mother\\n',\n",
       " 'aw the smiths on at work . very unexpected surprise\\n',\n",
       " \"oh right i see :') what time will it be ? like english time ? xo\\n\",\n",
       " 'that a boy . next time you come we will plan something excellent and will hold you to that .\\n',\n",
       " 'shhhshhsshhhshut the fuck up\\n',\n",
       " 'thanks for the follow you guys sound amazing\\n',\n",
       " 'when my headphones are on , i am in my own dream world .\\n',\n",
       " 'it just makes me feel like my efforts to overcome difficult situations arent wasted when people can see the difference .\\n',\n",
       " 'oh of course lol #ourwholeuniversewasinahotdensestate\\n',\n",
       " 'ctfuu , you coulda said that :p & tell her i said hi\\n',\n",
       " 'my phone never notified me of your reply ! aw thank you i can defo come now , woop , see you at 8 x\\n',\n",
       " 'lantaran to si kar eh . di na kita gusto eh . actually , mahal na kita eh : \" > ) ) )\\n',\n",
       " 'totally agreed viv\\n',\n",
       " 'yahhh thank you ) night , see in the am .\\n',\n",
       " \"you're just as in love with the hunger games as i am at the moment , it seems . love that .\\n\",\n",
       " \"could i ask that you follow me ? i'd like to send you a couple of dm's . many thanks\\n\",\n",
       " 'chillin with matt and brittany stay strong <3\\n',\n",
       " 'either is soo cute\\n',\n",
       " 'yay my bestie i havent seen in 2 years coming to see me today\\n',\n",
       " \"when will ftsk be announced for warped tour 2012 ? lol i'm seriously hoping you guys will be there xxx\\n\",\n",
       " 'nf my brother gf follow back\\n',\n",
       " \"g'night then\\n\",\n",
       " 'ready to start cosmo !\\n',\n",
       " \"i told my mum it's a pug or a baby - shes warming to the idea\\n\",\n",
       " \"no i haven't spent it , still got 120 but i'm planning on spending it hehe\\n\",\n",
       " 'i actually like being single , it means that #ridingsolo can be my theme song ) #myjam\\n',\n",
       " 'but then again , its unique\\n',\n",
       " 'these are on sale for $ 10.00 ! amazing ! <url> seriously go look at their sale section . #loveshoes <url>\\n',\n",
       " '16 days and no selfharm\\n',\n",
       " 'we have the same birthday only that i was born a few years after you ! sorry for that\\n',\n",
       " 'mrs yoder told my mom after skool was too late to go ? so i have to go in the morning which is stupiddd but i am gonna pass\\n',\n",
       " \"guess i'm watching movies tonight . anyone wanna join ? haha\\n\",\n",
       " 'defiantly love waking up and reimi being here\\n',\n",
       " 'if you looking for me you know where to find me ,\\n',\n",
       " 'small big boobed blonde seeks danny ( the script ) look alike lmao xx\\n',\n",
       " 'lol ! and when we decided to sing somebody that i used to know in the middle of adventure island like cool kids\\n',\n",
       " 'people gonna judge im sure but it me and you babe .\\n',\n",
       " 'i wish you would ! lol , then i could return the favor ! )\\n',\n",
       " 'challenge accepted ! ! lol\\n',\n",
       " '\" that\\'s good ( keep ya head up & stay beautiful ! \" : )\\n',\n",
       " 'you guys did an amazing job ! really really great .\\n',\n",
       " 'the mood is set . my body is screaming out for ya . i gotta secret i wanna show ya #noholdingbackk dont stop , dont you dare .\\n',\n",
       " 'whoaaa whoa whoaaa sweet child of mineee yeahhh yeah yeahhh #stepbrothers\\n',\n",
       " 'what an amazing performance to end you 18th year with , you rocked\\n',\n",
       " 'good luck kak \" believe in allah that everything would be okay \"\\n',\n",
       " \"it's nice to be creeped upon ! !\\n\",\n",
       " 'if u me follow me and i follow back that doesnt mean unfollow me so you have 1 more follower . i will just find u and unfollow right back\\n',\n",
       " 'when im home bored i watch cooking channels . wishing that can be me cooking all that food .\\n',\n",
       " 'just rescued a fox with a broken leg\\n',\n",
       " \"lol u know u can kick that person off of chat on blogtv right ? lol i'm watching ur blogtv right now . u should follow me (\\n\",\n",
       " \"i've got presents for everyone from my event ( beware they are shit ) haha #loveyou\\n\",\n",
       " 'so its working out fine ! nice so much wish i was there , but i do believe it will be more opportunities ! good luck\\n',\n",
       " 'im proud ta say that my big susta gone make it in this world .. a nigga bouta graduate ... i love her\\n',\n",
       " 'they are sold out , but if u have extra then feel free to send 2 this way .. you might know people that can make it happen\\n',\n",
       " 'i wanna love u , every day and every night !\\n',\n",
       " \"relax ... rt i just don't want to start naming names , unless provoked .\\n\",\n",
       " 'thanks to my pin up studio team yesterday , we did some great pictures ! <url>\\n',\n",
       " 'bitch ishould of had myguards up like you did . but itsss cooo . illl comeback\\n',\n",
       " 'awww #goodnightbabylux and #harrylovesbabylux are trending\\n',\n",
       " 'my booo > rt you my everything\\n',\n",
       " 'one more follower please ? #teamfollowback\\n',\n",
       " 'vas happenin boys ? rt if you remember this <url>\\n',\n",
       " \"i can not wait for esque june 19 i'm very anxious te amo 28\\n\",\n",
       " 'finished ! this pouch is on its way to new york today ! time to wrap it up ! <url>\\n',\n",
       " 'things that turn me on ... male gleeks * cough <3\\n',\n",
       " 'can you give me a shoutout i have followed\\n',\n",
       " '* talks in ear peice * kevae & toolay pat his ass down on da low\\n',\n",
       " \"aw , can you ask her to follow me ? haha , been trying for a long time ! she's an inspiration . have fun you two ! xx\\n\",\n",
       " 'happy birthday ... from all slater fans ... ) )\\n',\n",
       " 'all true except the nice , smart , handsome part .. ! thanks my love .. !\\n',\n",
       " 'ayeee so proud of you\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
      "\n",
      "dunno justin read mention . justin god knows , hope follow # believe 15\n"
     ]
    }
   ],
   "source": [
    "print(tt)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(tt)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "result = ' '.join(result)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.686, 'pos': 0.314, 'compound': 0.4939}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores('just woke up , finna go to church ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/etienne/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987aadacba074df29955d5d9e3a82f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_2 = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for tweet in tqdm(tweets):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    result = [i for i in tokens if not i in stop_words]\n",
    "    result = ' '.join(result)\n",
    "\n",
    "    ss = sid.polarity_scores(tweet)\n",
    "    if ss['neu'] == 1:\n",
    "        prediction_2.append(-1)\n",
    "    elif ss['neg'] > ss['pos']:\n",
    "        prediction_2.append(-1)\n",
    "    else:\n",
    "        prediction_2.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/72/6b3264aa2889b7dde7663464b99587d95cd6a5f3b9b30181f14d78a63e64/tensorflow-2.0.0-cp37-cp37m-macosx_10_11_x86_64.whl\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.2.2)\n",
      "Collecting wrapt>=1.11.1 (from tensorflow)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.17.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.33.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.25.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.11.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: h5py in /usr/local/anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (42.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.21.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.6.16)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/anaconda3/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n",
      "\u001b[31mthinc 6.12.1 has requirement msgpack<0.6.0,>=0.5.6, but you'll have msgpack 0.6.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mthinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mspacy 2.0.16 has requirement regex==2018.01.10, but you'll have regex 2019.4.14 which is incompatible.\u001b[0m\n",
      "Installing collected packages: wrapt, tensorflow\n",
      "  Found existing installation: wrapt 1.10.11\n",
      "\u001b[31mCannot uninstall 'wrapt'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-784aa54c851e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/keras/backend/load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Try and load external backend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "\n",
    "from keras_preprocessing import text\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "import logging\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.read_csv(\"Datasets/twitter-datasets/train_pos_cleaned.csv\", index_col=0)\n",
    "neg_df = pd.read_csv(\"Datasets/twitter-datasets/train_neg_cleaned.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([pos_df,neg_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49673</th>\n",
       "      <td>k fine lah if liddat i also sleep loh haiz nights twitter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71551</th>\n",
       "      <td>going to be told im blind tomorrow ok slight exaggeration but yeah having my eyes tested then a day of uni work just need it done</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5506</th>\n",
       "      <td>sometimes its nice to just buy a little bit of jewelry #liasophia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38370</th>\n",
       "      <td>this is my tweet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36930</th>\n",
       "      <td>im not always nice but i dont have a reason not to be</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                  tweets  \\\n",
       "49673  k fine lah if liddat i also sleep loh haiz nights twitter                                                                           \n",
       "71551  going to be told im blind tomorrow ok slight exaggeration but yeah having my eyes tested then a day of uni work just need it done   \n",
       "5506   sometimes its nice to just buy a little bit of jewelry #liasophia                                                                   \n",
       "38370  this is my tweet                                                                                                                    \n",
       "36930  im not always nice but i dont have a reason not to be                                                                               \n",
       "\n",
       "       label  \n",
       "49673  1      \n",
       "71551  0      \n",
       "5506   1      \n",
       "38370  1      \n",
       "36930  1      "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.sample(frac=1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"Datasets/twitter-datasets/test_data_cleaned.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100260 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 100000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 300\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(np.hstack((train.tweets.values,test.tweets.values)))\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (199976, 50)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(train.tweets.values)\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159980, 50) (159980,)\n",
      "(39996, 50) (39996,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 1)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/etienne/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159980 samples, validate on 39996 samples\n",
      "Epoch 1/3\n",
      "159980/159980 [==============================] - 555s 3ms/step - loss: 0.4655 - accuracy: 0.7685 - val_loss: 0.3971 - val_accuracy: 0.8163\n",
      "Epoch 2/3\n",
      " 99328/159980 [=================>............] - ETA: 3:11 - loss: 0.3412 - accuracy: 0.8485"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6a19b2aa3017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m           validation_data=(X_test, Y_test))\n\u001b[0m\u001b[1;32m     21\u001b[0m score, acc = model.evaluate(X_test, Y_test,\n\u001b[1;32m     22\u001b[0m                             batch_size=batch_size)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "#model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(Dense(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(test.tweets.values)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.ones_like(y_pred_prob)\n",
    "y_pred[y_pred_prob<0.5] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = log.predict(test_2)\n",
    "y_pred = np.insert(prediction, index_to_remove_test -1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(range(1,10001), y_pred, 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 75.95%\n"
     ]
    }
   ],
   "source": [
    "solution = pd.read_csv('derived_solution.csv').Prediction\n",
    "print(\"Accuracy : {:.02f}%\".format(100*np.mean(solution == y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation with solver :\n",
    "- lbfgs : 75.66\n",
    "- newton-cg : 75.60%\n",
    "- sag : 75.69%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best on Aicrowd: (76.90%)\n",
    "- not full tweets\n",
    "- sag with C = 1, tol = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
