{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try with given scripts (BAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = np.load('embeddings.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.pkl\", \"rb\") as f:\n",
    "        vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines_pos = sum(1 for line in open('Datasets/twitter-datasets/train_pos_full.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/etienne/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/etienne/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "train_pos = np.zeros((num_lines_pos,emb.shape[1]))\n",
    "with open('Datasets/twitter-datasets/train_pos_full.txt') as f:\n",
    "    for line_index, line in enumerate(f):\n",
    "        words = line.split()\n",
    "        index = [vocab[word] for word in words if word in vocab.keys()]\n",
    "        line_fet = np.mean(np.array([emb[i] for i in index]),axis = 0)\n",
    "        train_pos[line_index] = line_fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove_pos = np.unique([x for x,y in np.argwhere(np.isnan(train_pos))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_2 = np.delete(train_pos,index_to_remove_pos,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines_neg = sum(1 for line in open('Datasets/twitter-datasets/train_neg_full.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = np.zeros((num_lines_neg,emb.shape[1]))\n",
    "with open('Datasets/twitter-datasets/train_neg_full.txt') as f:\n",
    "    for line_index, line in enumerate(f):\n",
    "        words = line.split()\n",
    "        index = [vocab[word] for word in words if word in vocab.keys()]\n",
    "        line_fet = np.mean(np.array([emb[i] for i in index]),axis = 0)\n",
    "        train_neg[line_index] = line_fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove_neg = np.unique([x for x,y in np.argwhere(np.isnan(train_neg))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_2 = np.delete(train_neg,index_to_remove_neg,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get total training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((train_pos_2,train_neg_2))\n",
    "y_pos = np.ones(train_pos_2.shape[0])\n",
    "y_neg = np.repeat(-1,train_neg_2.shape[0])\n",
    "Y = np.hstack((y_pos,y_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X',X)\n",
    "np.save('Y',Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('X.npy')\n",
    "Y = np.load('Y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = build_poly(X,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=True, order='C')\n",
    "X = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  2.96126954e-01, -3.82799708e-02, ...,\n",
       "         1.45567212e-03, -6.46967335e-03,  2.87541904e-02],\n",
       "       [ 1.00000000e+00,  3.15962548e-01, -1.83284904e-02, ...,\n",
       "         3.12987105e-05, -6.29418917e-04,  1.26576516e-02],\n",
       "       [ 1.00000000e+00,  3.08120552e-01, -1.70786095e-01, ...,\n",
       "         1.47494328e-05,  2.48567688e-04,  4.18903535e-03],\n",
       "       ...,\n",
       "       [ 1.00000000e+00,  3.28799104e-01, -8.18543578e-02, ...,\n",
       "         3.01726021e-03, -8.18731286e-03,  2.22162118e-02],\n",
       "       [ 1.00000000e+00,  2.52777100e-01, -1.10337703e-01, ...,\n",
       "         1.83283686e-03,  1.72449017e-03,  1.62254830e-03],\n",
       "       [ 1.00000000e+00,  3.21354338e-01,  9.62912834e-02, ...,\n",
       "         1.86356312e-04,  2.05703618e-03,  2.27059539e-02]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = std.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train',X_train)\n",
    "np.save('X_test',X_test)\n",
    "np.save('Y_train',Y_train)\n",
    "np.save('Y_test',Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1.,  1., ..., -1.,  1.,  1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "Y_train = np.load('Y_train.npy')\n",
    "Y_test = np.load('Y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi = LogisticRegression(penalty='l2', dual=False, tol=10e-10, C=0.5, fit_intercept=True, intercept_scaling=1, \n",
    "                          class_weight=None, random_state=None, solver='warn', max_iter=100, multi_class='warn', \n",
    "                          verbose=1, warm_start=False, n_jobs=None, l1_ratio=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/etienne/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "logi.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5519145574367231"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, tol=10e-10, C=0.5, multi_class='ovr', \n",
    "                fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=1, random_state=None, \n",
    "                max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=1e-09,\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5521745652369571"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize test tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines_test = sum(1 for line in open('Datasets/twitter-datasets/test_data.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/etienne/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/etienne/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "test = np.zeros((num_lines_test,emb.shape[1]))\n",
    "with open('Datasets/twitter-datasets/test_data.txt') as f:\n",
    "    for line_index, line in enumerate(f):\n",
    "        line = line.split(',',1)[1]\n",
    "        words = line.split()\n",
    "        index = [vocab[word] for word in words if word in vocab.keys()]\n",
    "        line_fet = np.mean(np.array([emb[i] for i in index]),axis = 0)\n",
    "        test[line_index] = line_fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove_test = np.unique([x for x,y in np.argwhere(np.isnan(test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = np.delete(test,index_to_remove_test,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = std.fit_transform(test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = build_poly(test_2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(test_2)\n",
    "prediction_2 = np.insert(prediction, index_to_remove_test -1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "## Vocabulary vectorizing\n",
    "Read words in positive and neg tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Datasets/twitter-datasets/train_pos.txt\")\n",
    "tweets_pos = [line.split() for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Datasets/twitter-datasets/train_neg.txt\")\n",
    "tweets_neg = [line.split() for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-26 08:41:51,919 : INFO : collecting all words and their counts\n",
      "2019-11-26 08:41:51,920 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-26 08:41:51,952 : INFO : PROGRESS: at sentence #10000, processed 141898 words, keeping 11411 word types\n",
      "2019-11-26 08:41:51,978 : INFO : PROGRESS: at sentence #20000, processed 285174 words, keeping 18950 word types\n",
      "2019-11-26 08:41:52,003 : INFO : PROGRESS: at sentence #30000, processed 429701 words, keeping 25161 word types\n",
      "2019-11-26 08:41:52,035 : INFO : PROGRESS: at sentence #40000, processed 571007 words, keeping 30591 word types\n",
      "2019-11-26 08:41:52,063 : INFO : PROGRESS: at sentence #50000, processed 714851 words, keeping 35669 word types\n",
      "2019-11-26 08:41:52,091 : INFO : PROGRESS: at sentence #60000, processed 858157 words, keeping 40301 word types\n",
      "2019-11-26 08:41:52,121 : INFO : PROGRESS: at sentence #70000, processed 1003256 words, keeping 44631 word types\n",
      "2019-11-26 08:41:52,151 : INFO : PROGRESS: at sentence #80000, processed 1145914 words, keeping 48851 word types\n",
      "2019-11-26 08:41:52,175 : INFO : PROGRESS: at sentence #90000, processed 1290612 words, keeping 52921 word types\n",
      "2019-11-26 08:41:52,203 : INFO : PROGRESS: at sentence #100000, processed 1433854 words, keeping 56693 word types\n",
      "2019-11-26 08:41:52,239 : INFO : PROGRESS: at sentence #110000, processed 1605175 words, keeping 65238 word types\n",
      "2019-11-26 08:41:52,274 : INFO : PROGRESS: at sentence #120000, processed 1777636 words, keeping 72320 word types\n",
      "2019-11-26 08:41:52,314 : INFO : PROGRESS: at sentence #130000, processed 1952469 words, keeping 78473 word types\n",
      "2019-11-26 08:41:52,352 : INFO : PROGRESS: at sentence #140000, processed 2124084 words, keeping 84500 word types\n",
      "2019-11-26 08:41:52,398 : INFO : PROGRESS: at sentence #150000, processed 2294909 words, keeping 89941 word types\n",
      "2019-11-26 08:41:52,438 : INFO : PROGRESS: at sentence #160000, processed 2466888 words, keeping 95263 word types\n",
      "2019-11-26 08:41:52,477 : INFO : PROGRESS: at sentence #170000, processed 2636558 words, keeping 100257 word types\n",
      "2019-11-26 08:41:52,517 : INFO : PROGRESS: at sentence #180000, processed 2808064 words, keeping 105297 word types\n",
      "2019-11-26 08:41:52,558 : INFO : PROGRESS: at sentence #190000, processed 2976306 words, keeping 109865 word types\n",
      "2019-11-26 08:41:52,596 : INFO : collected 114427 word types from a corpus of 3147506 raw words and 200000 sentences\n",
      "2019-11-26 08:41:52,596 : INFO : Loading a fresh vocabulary\n",
      "2019-11-26 08:41:52,666 : INFO : effective_min_count=5 retains 21161 unique words (18% of original 114427, drops 93266)\n",
      "2019-11-26 08:41:52,667 : INFO : effective_min_count=5 leaves 3011656 word corpus (95% of original 3147506, drops 135850)\n",
      "2019-11-26 08:41:52,723 : INFO : deleting the raw counts dictionary of 114427 items\n",
      "2019-11-26 08:41:52,725 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2019-11-26 08:41:52,726 : INFO : downsampling leaves estimated 2229363 word corpus (74.0% of prior 3011656)\n",
      "2019-11-26 08:41:52,777 : INFO : estimated required memory for 21161 words and 200 dimensions: 44438100 bytes\n",
      "2019-11-26 08:41:52,778 : INFO : resetting layer weights\n",
      "2019-11-26 08:41:56,416 : INFO : training model with 3 workers on 21161 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-11-26 08:41:57,423 : INFO : EPOCH 1 - PROGRESS: at 44.26% examples, 891791 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-26 08:41:58,424 : INFO : EPOCH 1 - PROGRESS: at 89.70% examples, 989195 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-26 08:41:58,635 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-26 08:41:58,644 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 08:41:58,647 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 08:41:58,648 : INFO : EPOCH - 1 : training on 3147506 raw words (2230404 effective words) took 2.2s, 1002204 effective words/s\n",
      "2019-11-26 08:41:59,661 : INFO : EPOCH 2 - PROGRESS: at 50.14% examples, 1003959 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-26 08:42:00,666 : INFO : EPOCH 2 - PROGRESS: at 86.78% examples, 948112 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-26 08:42:00,963 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-26 08:42:00,966 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 08:42:00,971 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 08:42:00,972 : INFO : EPOCH - 2 : training on 3147506 raw words (2229449 effective words) took 2.3s, 961552 effective words/s\n",
      "2019-11-26 08:42:01,997 : INFO : EPOCH 3 - PROGRESS: at 45.29% examples, 908778 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-26 08:42:03,028 : INFO : EPOCH 3 - PROGRESS: at 87.07% examples, 941592 words/s, in_qsize 4, out_qsize 1\n",
      "2019-11-26 08:42:03,445 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-26 08:42:03,449 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 08:42:03,460 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 08:42:03,461 : INFO : EPOCH - 3 : training on 3147506 raw words (2230293 effective words) took 2.5s, 903489 effective words/s\n",
      "2019-11-26 08:42:04,472 : INFO : EPOCH 4 - PROGRESS: at 33.46% examples, 674273 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-26 08:42:05,484 : INFO : EPOCH 4 - PROGRESS: at 78.24% examples, 845989 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-26 08:42:06,084 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-26 08:42:06,085 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 08:42:06,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 08:42:06,101 : INFO : EPOCH - 4 : training on 3147506 raw words (2228843 effective words) took 2.6s, 847883 effective words/s\n",
      "2019-11-26 08:42:07,119 : INFO : EPOCH 5 - PROGRESS: at 49.47% examples, 997136 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-26 08:42:08,126 : INFO : EPOCH 5 - PROGRESS: at 95.92% examples, 1060651 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-26 08:42:08,205 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-26 08:42:08,213 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 08:42:08,217 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 08:42:08,218 : INFO : EPOCH - 5 : training on 3147506 raw words (2228950 effective words) took 2.1s, 1061767 effective words/s\n",
      "2019-11-26 08:42:08,220 : INFO : training on a 15737530 raw words (11147939 effective words) took 11.8s, 944519 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(sentences=tweets_pos + tweets_neg,size = size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeeding\n",
    "### Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/etienne/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/etienne/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "train_pos = np.zeros((len(tweets_pos),size))\n",
    "for index, tokens in enumerate(tweets_pos):\n",
    "    vect = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    train_pos[index] = np.mean(vect, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove_pos = np.unique([x for x,y in np.argwhere(np.isnan(train_pos))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_2 = np.delete(train_pos,index_to_remove_pos,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = np.zeros((len(tweets_neg),size))\n",
    "for index, tokens in enumerate(tweets_neg):\n",
    "    vect = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    train_neg[index] = np.mean(vect, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove_neg = np.unique([x for x,y in np.argwhere(np.isnan(train_neg))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_2 = np.delete(train_neg,index_to_remove_neg,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Datasets/twitter-datasets/test_data.txt\")\n",
    "tweets_test = [line.split() for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.zeros((len(tweets_test),size))\n",
    "for index, tokens in enumerate(tweets_test):\n",
    "    vect = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    test[index] = np.mean(vect, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_remove_test = np.unique([x for x,y in np.argwhere(np.isnan(test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = np.delete(test,index_to_remove_test,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine\n",
    "Combine pos and neg to have full training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((train_pos_2,train_neg_2))\n",
    "y_pos = np.ones(train_pos_2.shape[0])\n",
    "y_neg = np.repeat(-1,train_neg_2.shape[0])\n",
    "Y = np.hstack((y_pos,y_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Logistic Regression with Cross-validation so don't need to split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "logiCV = LogisticRegressionCV(Cs=10, fit_intercept=True, cv=5, dual=False, penalty='l2', scoring=None,\n",
    "                     solver='newton-cg', tol=0.0001, max_iter=1000, class_weight=None, n_jobs=-1, verbose=1,\n",
    "                     refit=True, intercept_scaling=1.0, multi_class='ovr', random_state=None, l1_ratios=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "logiCV.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = logiCV.predict(test_2)\n",
    "prediction_2 = np.insert(prediction, index_to_remove_test -1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(range(1,10001), prediction_2, 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 75.66%\n"
     ]
    }
   ],
   "source": [
    "solution = pd.read_csv('derived_solution.csv').Prediction\n",
    "print(\"Accuracy : {:.02f}%\".format(100*np.mean(solution == prediction_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation with solver :\n",
    "- lbfgs : 75.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
